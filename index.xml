<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Brock's Blog on Data|DevOps|Cloud</title><link>https://be-rock.github.io/blog/</link><description>Recent content on Brock's Blog on Data|DevOps|Cloud</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Â© 2025 Brock</copyright><lastBuildDate>Sat, 15 Nov 2025 22:36:28 +0000</lastBuildDate><atom:link href="https://be-rock.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Useful Stuff</title><link>https://be-rock.github.io/blog/posts/useful-stuff/</link><pubDate>Sat, 15 Nov 2025 22:36:28 +0000</pubDate><guid>https://be-rock.github.io/blog/posts/useful-stuff/</guid><description>Miscellaneous useful stuff that I pickup over time</description></item><item><title>The Role of plaintext and Markdown Files in AI-driven development</title><link>https://be-rock.github.io/blog/posts/2025-10_the-role-of-plaintext-and-markdown-files-in-ai/</link><pubDate>Sat, 01 Nov 2025 22:02:06 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/2025-10_the-role-of-plaintext-and-markdown-files-in-ai/</guid><description>Using plaintext and Markdown Files in AI-driven development</description></item><item><title>Change Data Capture via Spark Streaming and Delta Lake CDF</title><link>https://be-rock.github.io/blog/posts/spark-streaming-delta-lake-cdf/</link><pubDate>Mon, 27 Oct 2025 10:55:10 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/spark-streaming-delta-lake-cdf/</guid><description>&lt;h2 class="relative group"&gt;Summary
&lt;div id="summary" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#summary" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;a
href="https://delta.io/"
target="_blank"
&gt;Delta Lake&lt;/a&gt; provides support for CDC (Change Data Capture) through its internal &lt;a
href="https://docs.delta.io/delta-change-data-feed/"
target="_blank"
&gt;Change Data Feed&lt;/a&gt; (CDF).&lt;/p&gt;</description></item><item><title>Using the StreamingQueryListener with Spark Streaming</title><link>https://be-rock.github.io/blog/posts/spark-streaming-streamingquerylistener/</link><pubDate>Mon, 13 Oct 2025 20:08:27 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/spark-streaming-streamingquerylistener/</guid><description>&lt;h2 class="relative group"&gt;Summary
&lt;div id="summary" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#summary" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This is an example of how to setup a Spark &lt;a
href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQueryListener.html"
target="_blank"
&gt;&lt;code&gt;StreamingQueryListener&lt;/code&gt;&lt;/a&gt; that writes to some sample targets such as &lt;code&gt;json&lt;/code&gt; and &lt;code&gt;Postgres&lt;/code&gt;.&lt;/p&gt;</description></item><item><title>Apache Spark Streaming - templatized</title><link>https://be-rock.github.io/blog/posts/apache-spark-streaming-templatized/</link><pubDate>Sun, 12 Oct 2025 20:21:51 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/apache-spark-streaming-templatized/</guid><description>&lt;h2 class="relative group"&gt;Summary
&lt;div id="summary" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#summary" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a &lt;code&gt;foreachBatch&lt;/code&gt; sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like &lt;code&gt;yaml&lt;/code&gt;, if so desired.&lt;/p&gt;</description></item><item><title>My Recent Python Toolkit</title><link>https://be-rock.github.io/blog/posts/my-recent-python-toolkit/</link><pubDate>Mon, 29 Sep 2025 21:00:24 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/my-recent-python-toolkit/</guid><description>&lt;h2 class="relative group"&gt;Introduction
&lt;div id="introduction" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#introduction" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The Python development ecosystem has changed pretty dramatically over the recent years.&lt;/p&gt;
&lt;p&gt;This post provides a practical introduction to the toolkit that I&amp;rsquo;ve been using, explaining why I chose each tool and how they work together.&lt;/p&gt;</description></item><item><title>Evaluating V (Language)</title><link>https://be-rock.github.io/blog/posts/evaluating-vlang/</link><pubDate>Mon, 25 Aug 2025 19:39:16 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/evaluating-vlang/</guid><description>&lt;h2 class="relative group"&gt;Summary
&lt;div id="summary" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#summary" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I enjoy learning new languages, and decided it would be fun to do some learning the &amp;lsquo;old-school&amp;rsquo; way, meaning no AI-assisted coding, trial-and-error, and using the docs. Note - LLMs will be used merely as a search-engine equivalent to aid with solutions and resolve issues, but &lt;em&gt;not&lt;/em&gt; to build a solution.&lt;/li&gt;
&lt;li&gt;I have worked with numerous languages in the past, but 2 that have been on my radar are &lt;a
href="https://go.dev/"
target="_blank"
&gt;go&lt;/a&gt; and &lt;a
href="https://vlang.io/"
target="_blank"
&gt;V&lt;/a&gt;. This post will be about &lt;code&gt;V&lt;/code&gt; but I hope to do something similar for &lt;code&gt;go&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;V&lt;/code&gt; promises to be stable (despite not yet having reached 1.0 release), easy to learn (&amp;ldquo;can be learned over the course of a weekend&amp;rdquo;), fast, and is statically typed.&lt;/p&gt;</description></item><item><title>Streaming with Bufstream, Protobuf, and Spark</title><link>https://be-rock.github.io/blog/posts/kafka-protobuf-and-bufstream/</link><pubDate>Sat, 09 Aug 2025 22:29:16 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/kafka-protobuf-and-bufstream/</guid><description>&lt;h2 class="relative group"&gt;Summary
&lt;div id="summary" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#summary" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary&lt;/li&gt;
&lt;li&gt;Bufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group"&gt;What is the point?
&lt;div id="what-is-the-point" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#what-is-the-point" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing.&lt;/li&gt;
&lt;li&gt;This blog attempts to test the claims of Buf - the company behind:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Bufstream&lt;/code&gt; - the &amp;ldquo;drop-in replacement for Kafka&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BSR&lt;/code&gt; - the Buf Schema Registry which implements the Confluent Schema Registry API&lt;/li&gt;
&lt;li&gt;&lt;code&gt;buf&lt;/code&gt; CLI - a simple way to develop and manage Protobuf&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the &lt;code&gt;buf&lt;/code&gt; CLI will be referenced in this blog, but less of a focus.&lt;/p&gt;</description></item><item><title>Using a Container to run PySpark Unit Tests</title><link>https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/</link><pubDate>Sat, 12 Jul 2025 12:58:15 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/</guid><description>&lt;h2 class="relative group"&gt;Summary
&lt;div id="summary" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#summary" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code&lt;/li&gt;
&lt;li&gt;This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group"&gt;Why, just why?
&lt;div id="why-just-why" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#why-just-why" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.&lt;/p&gt;</description></item><item><title>Building a Pyspark Custom Data Sources for DuckDB</title><link>https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/</link><pubDate>Sat, 14 Jun 2025 23:51:42 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/</guid><description>&lt;h2 class="relative group"&gt;Summary
&lt;div id="summary" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#summary" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources&lt;/li&gt;
&lt;li&gt;A custom data source allows you to connect to a source system that that Spark may not currently have support for&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group"&gt;PySpark Custom Data Sources - an Overview
&lt;div id="pyspark-custom-data-sources---an-overview" class="anchor"&gt;&lt;/div&gt;
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
&lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#pyspark-custom-data-sources---an-overview" aria-label="Anchor"&gt;#&lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Starting with &lt;a
href="https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html"
target="_blank"
&gt;Apache Spark 4.0&lt;/a&gt; and &lt;a
href="https://docs.databricks.com/aws/en/pyspark/datasources"
target="_blank"
&gt;Databricks 15.2&lt;/a&gt;, PySpark supports custom data sources.&lt;/p&gt;</description></item><item><title>Search</title><link>https://be-rock.github.io/blog/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://be-rock.github.io/blog/search/</guid><description>search</description></item></channel></rss>