<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building a Pyspark Custom Data Sources for DuckDB | Brock B's Blog</title><meta name=keywords content="duckdb,spark"><meta name=description content="Summary

Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources
A custom data source allows you to connect to a source system that that Spark may not currently have support for

PySpark Custom Data Sources - an Overview
Starting with Apache Spark 4.0 and Databricks 15.2, PySpark supports custom data sources.
So what are PySpark Custom Data Sources?
Custom data sources allow you to define a source format other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint."><meta name=author content><link rel=canonical href=https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://be-rock.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://be-rock.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://be-rock.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://be-rock.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://be-rock.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/"><meta property="og:site_name" content="Brock B's Blog"><meta property="og:title" content="Building a Pyspark Custom Data Sources for DuckDB"><meta property="og:description" content="Summary Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources A custom data source allows you to connect to a source system that that Spark may not currently have support for PySpark Custom Data Sources - an Overview Starting with Apache Spark 4.0 and Databricks 15.2, PySpark supports custom data sources.
So what are PySpark Custom Data Sources?
Custom data sources allow you to define a source format other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-14T23:51:42-05:00"><meta property="article:modified_time" content="2025-06-14T23:51:42-05:00"><meta property="article:tag" content="Duckdb"><meta property="article:tag" content="Spark"><meta name=twitter:card content="summary"><meta name=twitter:title content="Building a Pyspark Custom Data Sources for DuckDB"><meta name=twitter:description content="Summary

Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources
A custom data source allows you to connect to a source system that that Spark may not currently have support for

PySpark Custom Data Sources - an Overview
Starting with Apache Spark 4.0 and Databricks 15.2, PySpark supports custom data sources.
So what are PySpark Custom Data Sources?
Custom data sources allow you to define a source format other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://be-rock.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Building a Pyspark Custom Data Sources for DuckDB","item":"https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building a Pyspark Custom Data Sources for DuckDB","name":"Building a Pyspark Custom Data Sources for DuckDB","description":"Summary Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources A custom data source allows you to connect to a source system that that Spark may not currently have support for PySpark Custom Data Sources - an Overview Starting with Apache Spark 4.0 and Databricks 15.2, PySpark supports custom data sources.\nSo what are PySpark Custom Data Sources?\nCustom data sources allow you to define a source format other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint.\n","keywords":["duckdb","spark"],"articleBody":"Summary Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources A custom data source allows you to connect to a source system that that Spark may not currently have support for PySpark Custom Data Sources - an Overview Starting with Apache Spark 4.0 and Databricks 15.2, PySpark supports custom data sources.\nSo what are PySpark Custom Data Sources?\nCustom data sources allow you to define a source format other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint.\nDuckDB and DuckLake are being discussed a lot lately so I thought it would be fun to see how easy it would be to interact with it from Spark. DuckDB does have an experimental Spark API but that would just be too easy, so let’s try to roll our own for educational purposes.\nHow does it work? To use Custom Data Source, we define a class that inherits from DataSource and then a Reader or Writer class inherits from DataSourceReader or DataSourceWriter respectively. These classes all reside in the pyspark.sql.datasource module.\npyspark.sql.datasource ├── DataSource ├── DataSourceReader └── DataSourceWriter There is also a DataSourceStreamReader and DataSourceStreamWriter but we wont touch on them in this blog.\nSetup Setup instructions follow but can also be setup using targets in a sample Makefile shown below in References/Makefile\nInstall the DuckDB CLI\nNote - This is a platform-dependent step so I’ll omit this setup instruction here and refer you to the docs instead. Setup the Python environment. Also can be run with make python-setup\nuv venv --python 3.12 source .venv/bin/activate uv pip install duckdb ipython pyspark==4.0.0 Create a quick test table in DuckDb. Can also be run with make duckdb-setup duckdb dev.duckdb create table t1 (c1 int); insert into t1 values (1); Start PySpark use the make target: make start-pyspark source .venv/bin/activate \u0026\u0026 \\ PYSPARK_DRIVER_PYTHON_OPTS=\"--TerminalInteractiveShell.editing_mode=vi --colors=Linux\" PYSPARK_DRIVER_PYTHON=ipython pyspark Define a Data Source Your custom data source must inherit from DataSource and will then be referenced by the custom reader class that will ultimately inherit from DataSourceReader.\nfrom pyspark.sql.datasource import DataSource, DataSourceReader from pyspark.sql.types import StructType class DuckDBDataSource(DataSource): @classmethod def name(cls): return \"duckdb\" def schema(self): ... def reader(self, schema: str): return DuckDBDataSourceReader(schema, self.options) Define a Data Source Reader This is where much of the magic resides in terms of how the Reader will interact with the DataSource. class DuckDBDataSourceReader(DataSourceReader): def __init__(self, schema, options): self.schema = schema self.options = options def read(self, partition): import duckdb db_path = self.options[\"db_path\"] query = self.options[\"query\"] with duckdb.connect(db_path) as conn: cursor = conn.execute(query) for row in cursor.fetchall(): yield tuple(row) Read from a DuckDB source from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() spark.dataSource.register(DuckDBDataSource) ( spark.read .format(\"duckdb\") .option(\"db_path\", \"dev.duckdb\") .option(\"query\", \"SELECT * FROM t1\") .schema(\"c1 int\") .load() ).show() +---+ | c1| +---+ | 1| +---+ Takeaway This blog just scratches the surface of what’s possible with Pyspark Custom Data Sources and would need numerous enhancements (obviously) to be used in any serious manner but hopefully gets across the point of the Data Source’s capabilities.\nReferences Makefile A Makefile to help simplify the setup # Makefile .DEFAULT_GOAL := help SHELL := /bin/bash help: ## Show this help message @echo -e 'Usage: make [target] ...\\n' @echo 'targets:' @egrep '^(.+)\\:\\ ##\\ (.+)' ${MAKEFILE_LIST} | column -t -c 2 -s ':#' .PHONY: setup-python setup-python: ## setup python env and dependencies uv venv --python 3.12 .venv source .venv/bin/activate \u0026\u0026 uv pip install duckdb ipython pyarrow pyspark==4.0.0 .PHONY: setup-duckdb setup-duckdb: ## setup duckdb database and test table with data duckdb dev.duckdb -c \"create table t1 (c1 int); insert into t1 values (1);\" .PHONY: setup setup: ## setup python and duckdb setup-python setup-duckdb .PHONY: start-pyspark start-pyspark: ## start-pyspark source .venv/bin/activate \u0026\u0026 \\ PYSPARK_DRIVER_PYTHON_OPTS=\"--TerminalInteractiveShell.editing_mode=vi --colors=Linux\" PYSPARK_DRIVER_PYTHON=ipython pyspark ","wordCount":"658","inLanguage":"en","datePublished":"2025-06-14T23:51:42-05:00","dateModified":"2025-06-14T23:51:42-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/"},"publisher":{"@type":"Organization","name":"Brock B's Blog","logo":{"@type":"ImageObject","url":"https://be-rock.github.io/blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://be-rock.github.io/blog/ accesskey=h title="Brock B's Blog (Alt + H)">Brock B's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://be-rock.github.io/blog/ title=Home><span>Home</span></a></li><li><a href=https://be-rock.github.io/blog/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://be-rock.github.io/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://be-rock.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Building a Pyspark Custom Data Sources for DuckDB</h1><div class=post-meta><span title='2025-06-14 23:51:42 -0500 -0500'>June 14, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;658 words</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#pyspark-custom-data-sources---an-overview aria-label="PySpark Custom Data Sources - an Overview">PySpark Custom Data Sources - an Overview</a><ul><li><a href=#how-does-it-work aria-label="How does it work?">How does it work?</a></li></ul></li><li><a href=#setup aria-label=Setup>Setup</a><ul><li><a href=#define-a-data-source aria-label="Define a Data Source">Define a Data Source</a></li><li><a href=#define-a-data-source-reader aria-label="Define a Data Source Reader">Define a Data Source Reader</a></li></ul></li><li><a href=#read-from-a-duckdb-source aria-label="Read from a DuckDB source">Read from a DuckDB source</a></li><li><a href=#takeaway aria-label=Takeaway>Takeaway</a></li><li><a href=#references aria-label=References>References</a><ul><li><a href=#makefile aria-label=Makefile>Makefile</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><ul><li>Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources</li><li>A custom data source allows you to connect to a source system that that Spark may not currently have support for</li></ul><h2 id=pyspark-custom-data-sources---an-overview>PySpark Custom Data Sources - an Overview<a hidden class=anchor aria-hidden=true href=#pyspark-custom-data-sources---an-overview>#</a></h2><p>Starting with <a href=https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html>Apache Spark 4.0</a> and <a href=https://docs.databricks.com/aws/en/pyspark/datasources>Databricks 15.2</a>, PySpark supports custom data sources.</p><p>So what <em>are</em> PySpark Custom Data Sources?</p><p>Custom data sources allow you to define a source <code>format</code> other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint.</p><p><a href=https://duckdb.org/>DuckDB</a> and <a href=https://ducklake.select/>DuckLake</a> are being discussed a lot lately so I thought it would be fun to see how easy it would be to interact with it from Spark. DuckDB does have an <a href=https://duckdb.org/docs/stable/clients/python/spark_api>experimental Spark API</a> but that would just be too easy, so let&rsquo;s try to roll our own for educational purposes.</p><h3 id=how-does-it-work>How does it work?<a hidden class=anchor aria-hidden=true href=#how-does-it-work>#</a></h3><p>To use Custom Data Source, we define a class that inherits from <code>DataSource</code> and then a Reader or Writer class inherits from <code>DataSourceReader</code> or <code>DataSourceWriter</code> respectively. These classes all reside in the <code>pyspark.sql.datasource</code> module.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pyspark.sql.datasource
</span></span><span style=display:flex><span>├── DataSource
</span></span><span style=display:flex><span>├── DataSourceReader
</span></span><span style=display:flex><span>└── DataSourceWriter
</span></span></code></pre></div><p>There is also a <code>DataSourceStreamReader</code> and <code>DataSourceStreamWriter</code> but we wont touch on them in this blog.</p><h2 id=setup>Setup<a hidden class=anchor aria-hidden=true href=#setup>#</a></h2><p>Setup instructions follow but can also be setup using targets in a sample <code>Makefile</code> shown below in <a href=#makefile>References/Makefile</a></p><ul><li><p><a href=https://duckdb.org/docs/installation>Install the DuckDB CLI</a></p><ul><li><em>Note</em> - This is a platform-dependent step so I&rsquo;ll omit this setup instruction here and refer you to the docs instead.</li></ul></li><li><p>Setup the Python environment. Also can be run with <code>make python-setup</code></p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>uv venv --python 3.12
</span></span><span style=display:flex><span>source .venv/bin/activate
</span></span><span style=display:flex><span>uv pip install duckdb ipython pyspark<span style=color:#f92672>==</span>4.0.0
</span></span></code></pre></div><ul><li>Create a quick test table in DuckDb. Can also be run with <code>make duckdb-setup</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>duckdb dev.duckdb
</span></span><span style=display:flex><span>create table t1 <span style=color:#f92672>(</span>c1 int<span style=color:#f92672>)</span>;
</span></span><span style=display:flex><span>insert into t1 values <span style=color:#f92672>(</span>1<span style=color:#f92672>)</span>;
</span></span></code></pre></div><ul><li>Start PySpark use the make target: <code>make start-pyspark</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>source .venv/bin/activate <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>      PYSPARK_DRIVER_PYTHON_OPTS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;--TerminalInteractiveShell.editing_mode=vi --colors=Linux&#34;</span> PYSPARK_DRIVER_PYTHON<span style=color:#f92672>=</span>ipython pyspark
</span></span></code></pre></div><h3 id=define-a-data-source>Define a Data Source<a hidden class=anchor aria-hidden=true href=#define-a-data-source>#</a></h3><p>Your custom data source must inherit from <code>DataSource</code> and will then be referenced by the custom reader class that will ultimately inherit from <code>DataSourceReader</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.datasource <span style=color:#f92672>import</span> DataSource, DataSourceReader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.types <span style=color:#f92672>import</span> StructType
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DuckDBDataSource</span>(DataSource):
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@classmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>name</span>(cls):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;duckdb&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>schema</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>reader</span>(self, schema: str):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> DuckDBDataSourceReader(schema, self<span style=color:#f92672>.</span>options)
</span></span></code></pre></div><h3 id=define-a-data-source-reader>Define a Data Source Reader<a hidden class=anchor aria-hidden=true href=#define-a-data-source-reader>#</a></h3><ul><li>This is where much of the magic resides in terms of how the Reader will interact with the DataSource.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DuckDBDataSourceReader</span>(DataSourceReader):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, schema, options):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>schema <span style=color:#f92672>=</span> schema
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>options <span style=color:#f92672>=</span> options
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>read</span>(self, partition):
</span></span><span style=display:flex><span>        <span style=color:#f92672>import</span> duckdb
</span></span><span style=display:flex><span>        db_path <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>options[<span style=color:#e6db74>&#34;db_path&#34;</span>]
</span></span><span style=display:flex><span>        query <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>options[<span style=color:#e6db74>&#34;query&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> duckdb<span style=color:#f92672>.</span>connect(db_path) <span style=color:#66d9ef>as</span> conn:
</span></span><span style=display:flex><span>            cursor <span style=color:#f92672>=</span> conn<span style=color:#f92672>.</span>execute(query)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> row <span style=color:#f92672>in</span> cursor<span style=color:#f92672>.</span>fetchall():
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>yield</span> tuple(row)
</span></span></code></pre></div><h2 id=read-from-a-duckdb-source>Read from a DuckDB source<a hidden class=anchor aria-hidden=true href=#read-from-a-duckdb-source>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> SparkSession
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>spark <span style=color:#f92672>=</span> SparkSession<span style=color:#f92672>.</span>builder<span style=color:#f92672>.</span>getOrCreate()
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>dataSource<span style=color:#f92672>.</span>register(DuckDBDataSource)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(
</span></span><span style=display:flex><span>    spark<span style=color:#f92672>.</span>read
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;duckdb&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;db_path&#34;</span>, <span style=color:#e6db74>&#34;dev.duckdb&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;query&#34;</span>, <span style=color:#e6db74>&#34;SELECT * FROM t1&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>schema(<span style=color:#e6db74>&#34;c1 int&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span><span style=color:#f92672>+---+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span> c1<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+---+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  <span style=color:#ae81ff>1</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+---+</span>
</span></span></code></pre></div><h2 id=takeaway>Takeaway<a hidden class=anchor aria-hidden=true href=#takeaway>#</a></h2><p>This blog just scratches the surface of what&rsquo;s possible with Pyspark Custom Data Sources and would need numerous enhancements (obviously) to be used in any serious manner but hopefully gets across the point of the Data Source&rsquo;s capabilities.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><h3 id=makefile>Makefile<a hidden class=anchor aria-hidden=true href=#makefile>#</a></h3><ul><li>A <code>Makefile</code> to help simplify the setup</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Makefile data-lang=Makefile><span style=display:flex><span><span style=color:#75715e># Makefile
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>.DEFAULT_GOAL <span style=color:#f92672>:=</span> help
</span></span><span style=display:flex><span>SHELL <span style=color:#f92672>:=</span> /bin/bash
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>help</span><span style=color:#f92672>:</span> <span style=color:#75715e>## Show this help message
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	@echo -e <span style=color:#e6db74>&#39;Usage: make [target] ...\n&#39;</span>
</span></span><span style=display:flex><span>	@echo <span style=color:#e6db74>&#39;targets:&#39;</span>
</span></span><span style=display:flex><span>	@egrep <span style=color:#e6db74>&#39;^(.+)\:\ ##\ (.+)&#39;</span> <span style=color:#e6db74>${</span>MAKEFILE_LIST<span style=color:#e6db74>}</span> | column -t -c <span style=color:#ae81ff>2</span> -s <span style=color:#e6db74>&#39;:#&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>.PHONY</span><span style=color:#f92672>:</span> setup-python
</span></span><span style=display:flex><span><span style=color:#a6e22e>setup-python</span><span style=color:#f92672>:</span> <span style=color:#75715e>## setup python env and dependencies
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	uv venv --python 3.12 .venv
</span></span><span style=display:flex><span>	source .venv/bin/activate <span style=color:#f92672>&amp;&amp;</span> uv pip install duckdb ipython pyarrow pyspark<span style=color:#f92672>==</span>4.0.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>.PHONY</span><span style=color:#f92672>:</span> setup-duckdb
</span></span><span style=display:flex><span><span style=color:#a6e22e>setup-duckdb</span><span style=color:#f92672>:</span> <span style=color:#75715e>## setup duckdb database and test table with data
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	duckdb dev.duckdb -c <span style=color:#e6db74>&#34;create table t1 (c1 int); insert into t1 values (1);&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>.PHONY</span><span style=color:#f92672>:</span> setup
</span></span><span style=display:flex><span><span style=color:#a6e22e>setup</span><span style=color:#f92672>:</span> <span style=color:#75715e>## setup python and duckdb
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	setup-python setup-duckdb
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>.PHONY</span><span style=color:#f92672>:</span> start-pyspark
</span></span><span style=display:flex><span><span style=color:#a6e22e>start-pyspark</span><span style=color:#f92672>:</span> <span style=color:#75715e>## start-pyspark
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	source .venv/bin/activate <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>      PYSPARK_DRIVER_PYTHON_OPTS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;--TerminalInteractiveShell.editing_mode=vi --colors=Linux&#34;</span> PYSPARK_DRIVER_PYTHON<span style=color:#f92672>=</span>ipython pyspark
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://be-rock.github.io/blog/tags/duckdb/>Duckdb</a></li><li><a href=https://be-rock.github.io/blog/tags/spark/>Spark</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://be-rock.github.io/blog/>Brock B's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>