<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Streaming with Bufstream, Protobuf, and Spark | Brock B's Blog</title><meta name=keywords content="bufstream,kafka,protobuf,spark,streaming"><meta name=description content="Summary

Bufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary
Bufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink

What is the point?

I often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing.
This blog attempts to test the claims of Buf - the company behind:

Bufstream - the &ldquo;drop-in replacement for Kafka&rdquo;
BSR - the Buf Schema Registry which implements the Confluent Schema Registry API
buf CLI - a simple way to develop and manage Protobuf



Note that the buf CLI will be referenced in this blog, but less of a focus."><meta name=author content><link rel=canonical href=https://be-rock.github.io/blog/posts/kafka-protobuf-and-bufstream/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://be-rock.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://be-rock.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://be-rock.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://be-rock.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://be-rock.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://be-rock.github.io/blog/posts/kafka-protobuf-and-bufstream/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://be-rock.github.io/blog/posts/kafka-protobuf-and-bufstream/"><meta property="og:site_name" content="Brock B's Blog"><meta property="og:title" content="Streaming with Bufstream, Protobuf, and Spark"><meta property="og:description" content="Summary Bufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary Bufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink What is the point? I often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing. This blog attempts to test the claims of Buf - the company behind: Bufstream - the “drop-in replacement for Kafka” BSR - the Buf Schema Registry which implements the Confluent Schema Registry API buf CLI - a simple way to develop and manage Protobuf Note that the buf CLI will be referenced in this blog, but less of a focus."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-09T22:29:16-05:00"><meta property="article:modified_time" content="2025-08-09T22:29:16-05:00"><meta property="article:tag" content="Bufstream"><meta property="article:tag" content="Kafka"><meta property="article:tag" content="Protobuf"><meta property="article:tag" content="Spark"><meta property="article:tag" content="Streaming"><meta name=twitter:card content="summary"><meta name=twitter:title content="Streaming with Bufstream, Protobuf, and Spark"><meta name=twitter:description content="Summary

Bufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary
Bufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink

What is the point?

I often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing.
This blog attempts to test the claims of Buf - the company behind:

Bufstream - the &ldquo;drop-in replacement for Kafka&rdquo;
BSR - the Buf Schema Registry which implements the Confluent Schema Registry API
buf CLI - a simple way to develop and manage Protobuf



Note that the buf CLI will be referenced in this blog, but less of a focus."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://be-rock.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Streaming with Bufstream, Protobuf, and Spark","item":"https://be-rock.github.io/blog/posts/kafka-protobuf-and-bufstream/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Streaming with Bufstream, Protobuf, and Spark","name":"Streaming with Bufstream, Protobuf, and Spark","description":"Summary Bufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary Bufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink What is the point? I often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing. This blog attempts to test the claims of Buf - the company behind: Bufstream - the \u0026ldquo;drop-in replacement for Kafka\u0026rdquo; BSR - the Buf Schema Registry which implements the Confluent Schema Registry API buf CLI - a simple way to develop and manage Protobuf Note that the buf CLI will be referenced in this blog, but less of a focus.\n","keywords":["bufstream","kafka","protobuf","spark","streaming"],"articleBody":"Summary Bufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary Bufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink What is the point? I often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing. This blog attempts to test the claims of Buf - the company behind: Bufstream - the “drop-in replacement for Kafka” BSR - the Buf Schema Registry which implements the Confluent Schema Registry API buf CLI - a simple way to develop and manage Protobuf Note that the buf CLI will be referenced in this blog, but less of a focus.\nGetting Started On a surface-level, I have found the Buf docs to be nice and well written with 4 applicable quickstart guides that will be the basis for getting started https://buf.build/docs/bufstream/quickstart/ https://buf.build/docs/bsr/quickstart/ https://buf.build/docs/cli/quickstart/ https://buf.build/docs/bufstream/iceberg/quickstart/ Bufstream Installation Starting with this Bufstream quickstart guide, we are advised to download the bufstream CLI with curl and then simply ./bufstream serve. Let’s see if that’s as easy as it sounds.\ncurl -sSL -o bufstream \\ \"https://buf.build/dl/bufstream/latest/bufstream-$(uname -s)-$(uname -m)\" \u0026\u0026 \\ chmod +x bufstream ./bufstream serve ... ... time=2025-08-05T22:45:14.648-05:00 level=INFO msg=\"kafka server started\" host=localhost port=9092 tls=false public=true time=2025-08-05T22:45:14.648-05:00 level=INddFO msg=\"kafka server started\" host=127.0.0.1 port=9092 tls=false time=2025-08-05T22:45:14.648-05:00 level=INFO msg=\"kafka server started\" host=::1 port=9092 tls=false time=2025-08-05T22:45:14.664-05:00 level=INFO msg=\"updating ownership\" oldShardNum=0 oldShardCount=0 shardNum=0 shardCount=1 Ok, that really was quite easy.\nSetting up a web-based Kafka Administrative Console (optional) The quickstart then recommends installing either either AKHQ or Redpanda console for managing Kafka-compatible workloads. The Redpanda installation is a simple 4-liner docker run so I’ll give that one a try.\ndocker run -p 8080:8080 \\ -e KAFKA_BROKERS=host.docker.internal:9092 \\ -e KAFKA_CLIENTID=\"rpconsole;broker_count=1;host_override=host.docker.internal\" \\ docker.redpanda.com/redpandadata/console:v3.1.3 ... ... {\"level\":\"warn\",\"ts\":\"2025-08-06T03:56:19.199Z\",\"msg\":\"failed to test Kafka connection, going to retry in 1s\",\"remaining_retries\":5} ... {\"level\":\"fatal\",\"ts\":\"2025-08-06T03:56:51.007Z\",\"msg\":\"failed to start console service\",\"error\":\"failed to test kafka connectivity: failed to test kafka connection: failed to request metadata: unable to dial: dial tcp: lookup host.docker.internal on 10.0.2.3:53: no such host\"} Hmmm, that did not work as expected. I am testing on Linux and believe that the quickstart presumes we’re running MacOS. I have also seen subtle network differences in the past when working with Docker (ok, Podman) on Linux. This eventually worked:\ndocker run --network=host -p 8080:8080 \\ -e KAFKA_BROKERS=localhost:9092 \\ docker.redpanda.com/redpandadata/console:v3.1.3 ... {\"level\":\"info\",\"ts\":\"2025-08-06T04:08:00.900Z\",\"msg\":\"started Redpanda Console\",\"version\":\"v3.1.3\",\"built_at\":\"1753360440\"} {\"level\":\"info\",\"ts\":\"2025-08-06T04:08:00.903Z\",\"msg\":\"connecting to Kafka seed brokers, trying to fetch cluster metadata\",\"seed_brokers\":[\"localhost:9092\"]} {\"level\":\"info\",\"ts\":\"2025-08-06T04:08:00.910Z\",\"msg\":\"successfully connected to kafka cluster\",\"advertised_broker_count\":1,\"topic_count\":0,\"controller_id\":1234570725,\"kafka_version\":\"between v0.10.2 and v0.11.0\"} {\"level\":\"info\",\"ts\":\"2025-08-06T04:08:01.111Z\",\"msg\":\"Server listening on address\",\"address\":\"[::]:8080\",\"port\":8080} I was then able to view the Redpanda console in my browser at http://localhost:8080/overview.\nWorking with Protobuf and the BSR Continuing with the quickstart, I cloned the Github repo and moved the previously installed bufstream CLI to the cloned directory.\ngit clone https://github.com/bufbuild/bufstream-demo.git \u0026\u0026 \\ mv bufstream ./bufstream-demo \u0026\u0026 \\ cd ./bufstream-demo This repo contains pre-created Protobuf files that have integration and support for Confluent-compatabile schema registries (which BSR complies with).\nAn example proto file is in proto/bufstream/demo/v1/demo.proto of the cloned repo and defines an EmailUpdated message.\nThings get a little unclear here but from what I understand, this demo.proto file imports a reference to a confluent module with import \"buf/confluent/v1/extensions.proto\"; and this is what enables the BSR to be compatible with Confluent schema registry. The mapping between the proto file and the topic is done with name: \"email-updated-value\". So the topic name here becomes email-updated.\nProducing and Consuming data The quickstart references a go run ./cmd/bufstream-demo-produce ... and go run ./cmd/bufstream-demo-consume ... but I’ve noticed that the cloned repo comes with a Makefile. That would be simpler to use here but the produce/consume commands seem to be out of sync so we’ll stick with what the demo suggests to be safe.\ngo run ./cmd/bufstream-demo-produce \\ --topic email-updated \\ --group email-verifier \\ --csr-url \"https://demo.buf.dev/integrations/confluent/bufstream-demo\" go run ./cmd/bufstream-demo-produce --topic email-updated --group email-verifier go: downloading github.com/brianvoe/gofakeit/v7 v7.3.0 go: downloading github.com/google/uuid v1.6.0 ... ... time=2025-08-06T22:41:20.299-05:00 level=INFO msg=\"produced semantically invalid protobuf message\" id=072da6ca-8878-4c11-944b-5876a4fc4370 time=2025-08-06T22:41:20.450-05:00 level=INFO msg=\"produced invalid data\" id=32bc2119-f1bb-43f4-a9db-28fbb04ff7b5 time=2025-08-06T22:41:21.588-05:00 level=INFO msg=\"produced semantically valid protobuf message\" id=d6fc0bf3-e13b-421f-a696-212059d7961d ... So a lot of data is being generated, some of which is considered semantically invalid. Looking in the RedPanda console, I can see sample data such as foobar as well as other sample data such as $f4525add-da7d-4b2b-aae9-884e7bab535dgarnettwunsch@dickens.netllama, it’s clear which of these 2 do not conform to the proto schema.\nNow I’ll try the make target make consume-run and see what happens since this is consistent with the snippet in the blog (unlike the produce).\nmake consume-run go run ./cmd/bufstream-demo-consume --topic email-updated --group email-verifier \\ --csr-url \"https://demo.buf.dev/integrations/confluent/bufstream-demo\" make consume-run go run ./cmd/bufstream-demo-consume --topic email-updated --group email-verifier \\ --csr-url \"https://demo.buf.dev/integrations/confluent/bufstream-demo\" time=2025-08-06T22:48:44.981-05:00 level=INFO msg=\"starting consume\" time=2025-08-06T22:48:44.982-05:00 level=INFO msg=\"consumed message with new email sisterglover@labadie.biz and old email orvilledickinson@turcotte.biz\" time=2025-08-06T22:48:45.984-05:00 level=INFO msg=\"consumed message with new email hound and old email antoniomurphy@bradtke.info\" time=2025-08-06T22:48:45.984-05:00 level=INFO msg=\"consumed malformed data\" error=\"registration is missing for encode/decode\" length=7 ... The log message consumed malformed data makes it sound like the consumer is consuming the data even if it is malformed and does not conform to the schema. This seems unexpected but I may have a misunderstanding on how this works.\nThe quickstart then suggests to run:\n./bufstream serve --config config/bufstream.yaml …which now seems to tie it all together by ensuring that the consumer has a connection to the BSR and ensuring that the Consumer only sees records that comply with the registered schema. I verified that schema enforcement does seem to be happening here by reviewing the terminal output for the producer and consumer. The Producer shows invalid records (e.g. does not conform to the schema):\n# sample invalid log message from the producer time=2025-08-09T22:36:50.224-05:00 level=ERROR msg=\"error on produce of invalid data\" error=\"failed to produce: INVALID_RECORD: This record has failed the validation on the broker and hence been rejected.\" …but the Consumer logs, only showed valid messages such as:\n# sample valid log message from the consumer time=2025-08-09T22:36:45.222-05:00 level=INFO msg=\"consumed message with new email monkey and old email ernestlegros@parisian.name\" Bufstream and Apache Spark If Bufstream really is a drop-in replacement for Apache Kafka, reading from Bufstream with Spark should also be straight-forward. Let’s confirm this.\nfrom pyspark.sql import DataFrame from pyspark.sql import functions as sf from pyspark.sql.streaming.readwriter import DataStreamWriter from pyspark.sql.streaming.query import StreamingQuery KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\" TOPIC = \"email-updated\" df: DataFrame = ( spark .readStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) .option(\"subscribe\", TOPIC) .option(\"startingOffsets\", \"earliest\") .load() .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") ) datastream_writer: DataStreamWriter = ( df .select(\"value\") .writeStream .format(\"console\") .option(\"truncate\", \"false\") ) streaming_query: StreamingQuery = datastream_writer.start() ------------------------------------------- Batch: 0 ------------------------------------------- +---------------------------------------------------------------------------------------------+ |value | +---------------------------------------------------------------------------------------------+ |\\n$926b3e7f-0311-4b0e-8925-e49b653e3f95jacqueskoss@lehner.info�alexisgraham@raynor.com | |\\n$c0ec4ae6-d7c0-49d0-96f3-f45e5df45e78jewelrohan@strosin.org�salmon | |foobar | |\\n$a1d8b79c-87c5-4dde-9183-b702e5b5b334marshallreynolds@armstrong.io�ericklittle@lang.name| |\\n$ab20b911-7bee-4fe9-8366-6211766412dfgissellejohnston@runolfsson.org�\\vgrasshopper | |foobar | |\\n$f89d1a48-2d9b-4d77-a872-d39be4c61921rickybechtelar@stamm.net�raoulbeahan@rutherford.io | |\\n$8af0f411-c2ed-494d-ac82-e495b111045austinanitzsche@lebsack.net�\\bplatypus | |foobar | |\\n$a58f1996-c6b2-416b-8531-a603921dd828marisafunk@towne.net�blakerippin@kemmer.biz | |\\n$48fa5f74-5921-4061-ad25-559ce2cb6e7aeleonoreupton@rohan.net�wasp | |foobar ... So this appeared to work at a very basic level, but this is not how we should be performing reads via Spark given that we want our consumers to know and cross-reference the expected schema defined in the BSR. Given that we confirmed this worked as expected with the above Consumer, we will for now presume that this capability is possible with Spark as well and leave the full Spark –\u003e BSR integration for later research.\nWhat about Iceberg? I followed the Iceberg quickstart but it seems like the compose file is referencing a spark/ directory that doesn’t exist: OSError: Dockerfile not found in /home/username/tmp/buf-examples/bufstream/iceberg-quickstart/spark .So I couldn’t get this working but according to the guide, all you need is:\nBufstream broker object storage an Iceberg catalog implementation The details of your Iceberg catalog are defined in config/bufstream.yaml.\nWhat about Bufstream in Production? The initial impression I had of Bustream is that deployments are super simple. I suppose this is true, in comparison to a Kafka deployment, but it still requires:\nObject storage such as AWS S3, Google Cloud Storage, or Azure Blob Storage. A metadata storage service such as PostgreSQL. There are numerous different deployment options such as Docker, Helm, and others with a provided Terraform module. It’s unclear if BSR requires an additional set of deployments or if that is integrated into Postgres.\nRecap Buf’s vision seems well-thought and clearly-defined. A simplified Kafka-compatible streaming engine A single way to manage schemas with Protobuf Schema enforcement with the BSR Data Lakehouse integration with Apache Iceberg The streaming market has no shortage of vended options but Buf’s vision really makes it stand out. Protobuf has the reputation for being less user-friendly than other serialization formats like JSON but this may change if the buf CLI can keep it’s promises of simplifying the schema mangement experience. ","wordCount":"1435","inLanguage":"en","datePublished":"2025-08-09T22:29:16-05:00","dateModified":"2025-08-09T22:29:16-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://be-rock.github.io/blog/posts/kafka-protobuf-and-bufstream/"},"publisher":{"@type":"Organization","name":"Brock B's Blog","logo":{"@type":"ImageObject","url":"https://be-rock.github.io/blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://be-rock.github.io/blog/ accesskey=h title="Brock B's Blog (Alt + H)">Brock B's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://be-rock.github.io/blog/ title=Home><span>Home</span></a></li><li><a href=https://be-rock.github.io/blog/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://be-rock.github.io/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://be-rock.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Streaming with Bufstream, Protobuf, and Spark</h1><div class=post-meta><span title='2025-08-09 22:29:16 -0500 -0500'>August 9, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1435 words</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#what-is-the-point aria-label="What is the point?">What is the point?</a></li><li><a href=#getting-started aria-label="Getting Started">Getting Started</a></li><li><a href=#bufstream aria-label=Bufstream>Bufstream</a><ul><li><a href=#installation aria-label=Installation>Installation</a></li><li><a href=#setting-up-a-web-based-kafka-administrative-console-optional aria-label="Setting up a web-based Kafka Administrative Console (optional)">Setting up a web-based Kafka Administrative Console (optional)</a></li><li><a href=#working-with-protobuf-and-the-bsr aria-label="Working with Protobuf and the BSR">Working with Protobuf and the BSR</a></li><li><a href=#producing-and-consuming-data aria-label="Producing and Consuming data">Producing and Consuming data</a></li></ul></li><li><a href=#bufstream-and-apache-spark aria-label="Bufstream and Apache Spark">Bufstream and Apache Spark</a></li><li><a href=#what-about-iceberg aria-label="What about Iceberg?">What about Iceberg?</a></li><li><a href=#what-about-bufstream-in-production aria-label="What about Bufstream in Production?">What about Bufstream in Production?</a></li><li><a href=#recap aria-label=Recap>Recap</a></li></ul></div></details></div><div class=post-content><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><ul><li>Bufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary</li><li>Bufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink</li></ul><h2 id=what-is-the-point>What is the point?<a hidden class=anchor aria-hidden=true href=#what-is-the-point>#</a></h2><ul><li>I often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing.</li><li>This blog attempts to test the claims of Buf - the company behind:<ul><li><code>Bufstream</code> - the &ldquo;drop-in replacement for Kafka&rdquo;</li><li><code>BSR</code> - the Buf Schema Registry which implements the Confluent Schema Registry API</li><li><code>buf</code> CLI - a simple way to develop and manage Protobuf</li></ul></li></ul><p>Note that the <code>buf</code> CLI will be referenced in this blog, but less of a focus.</p><h2 id=getting-started>Getting Started<a hidden class=anchor aria-hidden=true href=#getting-started>#</a></h2><ul><li>On a surface-level, I have found the Buf docs to be nice and well written with 4 applicable quickstart guides that will be the basis for getting started<ul><li><a href=https://buf.build/docs/bufstream/quickstart/>https://buf.build/docs/bufstream/quickstart/</a></li><li><a href=https://buf.build/docs/bsr/quickstart/>https://buf.build/docs/bsr/quickstart/</a></li><li><a href=https://buf.build/docs/cli/quickstart/>https://buf.build/docs/cli/quickstart/</a></li><li><a href=https://buf.build/docs/bufstream/iceberg/quickstart/>https://buf.build/docs/bufstream/iceberg/quickstart/</a></li></ul></li></ul><h2 id=bufstream>Bufstream<a hidden class=anchor aria-hidden=true href=#bufstream>#</a></h2><h3 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h3><p>Starting with <a href=https://buf.build/docs/bufstream/quickstart/>this Bufstream quickstart guide</a>, we are advised to download the <code>bufstream</code> CLI with <code>curl</code> and then simply <code>./bufstream serve</code>. Let&rsquo;s see if that&rsquo;s as easy as it sounds.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -sSL -o bufstream <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    <span style=color:#e6db74>&#34;https://buf.build/dl/bufstream/latest/bufstream-</span><span style=color:#66d9ef>$(</span>uname -s<span style=color:#66d9ef>)</span><span style=color:#e6db74>-</span><span style=color:#66d9ef>$(</span>uname -m<span style=color:#66d9ef>)</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    chmod +x bufstream
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>./bufstream serve
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>&lt;snipped&gt;
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-05T22:45:14.648-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;kafka server started&#34;</span> host<span style=color:#f92672>=</span>localhost port<span style=color:#f92672>=</span><span style=color:#ae81ff>9092</span> tls<span style=color:#f92672>=</span>false public<span style=color:#f92672>=</span>true
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-05T22:45:14.648-05:00 level<span style=color:#f92672>=</span>INddFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;kafka server started&#34;</span> host<span style=color:#f92672>=</span>127.0.0.1 port<span style=color:#f92672>=</span><span style=color:#ae81ff>9092</span> tls<span style=color:#f92672>=</span>false
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-05T22:45:14.648-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;kafka server started&#34;</span> host<span style=color:#f92672>=</span>::1 port<span style=color:#f92672>=</span><span style=color:#ae81ff>9092</span> tls<span style=color:#f92672>=</span>false
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-05T22:45:14.664-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;updating ownership&#34;</span> oldShardNum<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> oldShardCount<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> shardNum<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> shardCount<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>Ok, that really was quite easy.</p><h3 id=setting-up-a-web-based-kafka-administrative-console-optional>Setting up a web-based Kafka Administrative Console (optional)<a hidden class=anchor aria-hidden=true href=#setting-up-a-web-based-kafka-administrative-console-optional>#</a></h3><p>The quickstart then recommends installing either either <a href=https://akhq.io/>AKHQ</a> or <a href=https://docs.redpanda.com/current/console>Redpanda console</a> for managing Kafka-compatible workloads. The Redpanda installation is a simple 4-liner <code>docker run</code> so I&rsquo;ll give that one a try.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker run -p 8080:8080 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -e KAFKA_BROKERS<span style=color:#f92672>=</span>host.docker.internal:9092 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -e KAFKA_CLIENTID<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;rpconsole;broker_count=1;host_override=host.docker.internal&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    docker.redpanda.com/redpandadata/console:v3.1.3
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>&lt;snipped&gt;
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span><span style=color:#f92672>{</span><span style=color:#e6db74>&#34;level&#34;</span>:<span style=color:#e6db74>&#34;warn&#34;</span>,<span style=color:#e6db74>&#34;ts&#34;</span>:<span style=color:#e6db74>&#34;2025-08-06T03:56:19.199Z&#34;</span>,<span style=color:#e6db74>&#34;msg&#34;</span>:<span style=color:#e6db74>&#34;failed to test Kafka connection, going to retry in 1s&#34;</span>,<span style=color:#e6db74>&#34;remaining_retries&#34;</span>:5<span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span><span style=color:#f92672>{</span><span style=color:#e6db74>&#34;level&#34;</span>:<span style=color:#e6db74>&#34;fatal&#34;</span>,<span style=color:#e6db74>&#34;ts&#34;</span>:<span style=color:#e6db74>&#34;2025-08-06T03:56:51.007Z&#34;</span>,<span style=color:#e6db74>&#34;msg&#34;</span>:<span style=color:#e6db74>&#34;failed to start console service&#34;</span>,<span style=color:#e6db74>&#34;error&#34;</span>:<span style=color:#e6db74>&#34;failed to test kafka connectivity: failed to test kafka connection: failed to request metadata: unable to dial: dial tcp: lookup host.docker.internal on 10.0.2.3:53: no such host&#34;</span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>Hmmm, that did not work as expected. I am testing on Linux and believe that the quickstart presumes we&rsquo;re running MacOS. I have also seen subtle network differences in the past when working with Docker (ok, Podman) on Linux. This eventually worked:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker run --network<span style=color:#f92672>=</span>host -p 8080:8080 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -e KAFKA_BROKERS<span style=color:#f92672>=</span>localhost:9092 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    docker.redpanda.com/redpandadata/console:v3.1.3
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span><span style=color:#f92672>{</span><span style=color:#e6db74>&#34;level&#34;</span>:<span style=color:#e6db74>&#34;info&#34;</span>,<span style=color:#e6db74>&#34;ts&#34;</span>:<span style=color:#e6db74>&#34;2025-08-06T04:08:00.900Z&#34;</span>,<span style=color:#e6db74>&#34;msg&#34;</span>:<span style=color:#e6db74>&#34;started Redpanda Console&#34;</span>,<span style=color:#e6db74>&#34;version&#34;</span>:<span style=color:#e6db74>&#34;v3.1.3&#34;</span>,<span style=color:#e6db74>&#34;built_at&#34;</span>:<span style=color:#e6db74>&#34;1753360440&#34;</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>{</span><span style=color:#e6db74>&#34;level&#34;</span>:<span style=color:#e6db74>&#34;info&#34;</span>,<span style=color:#e6db74>&#34;ts&#34;</span>:<span style=color:#e6db74>&#34;2025-08-06T04:08:00.903Z&#34;</span>,<span style=color:#e6db74>&#34;msg&#34;</span>:<span style=color:#e6db74>&#34;connecting to Kafka seed brokers, trying to fetch cluster metadata&#34;</span>,<span style=color:#e6db74>&#34;seed_brokers&#34;</span>:<span style=color:#f92672>[</span><span style=color:#e6db74>&#34;localhost:9092&#34;</span><span style=color:#f92672>]}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>{</span><span style=color:#e6db74>&#34;level&#34;</span>:<span style=color:#e6db74>&#34;info&#34;</span>,<span style=color:#e6db74>&#34;ts&#34;</span>:<span style=color:#e6db74>&#34;2025-08-06T04:08:00.910Z&#34;</span>,<span style=color:#e6db74>&#34;msg&#34;</span>:<span style=color:#e6db74>&#34;successfully connected to kafka cluster&#34;</span>,<span style=color:#e6db74>&#34;advertised_broker_count&#34;</span>:1,<span style=color:#e6db74>&#34;topic_count&#34;</span>:0,<span style=color:#e6db74>&#34;controller_id&#34;</span>:1234570725,<span style=color:#e6db74>&#34;kafka_version&#34;</span>:<span style=color:#e6db74>&#34;between v0.10.2 and v0.11.0&#34;</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#f92672>{</span><span style=color:#e6db74>&#34;level&#34;</span>:<span style=color:#e6db74>&#34;info&#34;</span>,<span style=color:#e6db74>&#34;ts&#34;</span>:<span style=color:#e6db74>&#34;2025-08-06T04:08:01.111Z&#34;</span>,<span style=color:#e6db74>&#34;msg&#34;</span>:<span style=color:#e6db74>&#34;Server listening on address&#34;</span>,<span style=color:#e6db74>&#34;address&#34;</span>:<span style=color:#e6db74>&#34;[::]:8080&#34;</span>,<span style=color:#e6db74>&#34;port&#34;</span>:8080<span style=color:#f92672>}</span>
</span></span></code></pre></div><p>I was then able to view the Redpanda console in my browser at http://localhost:8080/overview.</p><h3 id=working-with-protobuf-and-the-bsr>Working with Protobuf and the BSR<a hidden class=anchor aria-hidden=true href=#working-with-protobuf-and-the-bsr>#</a></h3><p>Continuing with the quickstart, I cloned the Github repo and moved the previously installed <code>bufstream</code> CLI to the cloned directory.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>git clone https://github.com/bufbuild/bufstream-demo.git <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    mv bufstream ./bufstream-demo <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    cd ./bufstream-demo
</span></span></code></pre></div><p>This repo contains pre-created Protobuf files that have integration and support for Confluent-compatabile schema registries (which BSR complies with).</p><p>An example proto file is in <code>proto/bufstream/demo/v1/demo.proto</code> of the cloned repo and defines an <code>EmailUpdated</code> message.</p><p>Things get a little unclear here but from what I understand, this <code>demo.proto</code> file imports a reference to a confluent module with <code>import "buf/confluent/v1/extensions.proto";</code> and this is what enables the BSR to be compatible with Confluent schema registry. The mapping between the proto file and the topic is done with <code>name: "email-updated-value"</code>. So the topic name here becomes <code>email-updated</code>.</p><h3 id=producing-and-consuming-data>Producing and Consuming data<a hidden class=anchor aria-hidden=true href=#producing-and-consuming-data>#</a></h3><p>The quickstart references a <code>go run ./cmd/bufstream-demo-produce ...</code> and <code>go run ./cmd/bufstream-demo-consume ...</code> but I&rsquo;ve noticed that the cloned repo comes with a <code>Makefile</code>. That would be simpler to use here but the produce/consume commands seem to be out of sync so we&rsquo;ll stick with what the demo suggests to be safe.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>go run ./cmd/bufstream-demo-produce <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --topic email-updated <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --group email-verifier <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --csr-url <span style=color:#e6db74>&#34;https://demo.buf.dev/integrations/confluent/bufstream-demo&#34;</span>
</span></span><span style=display:flex><span>go run ./cmd/bufstream-demo-produce --topic email-updated --group email-verifier
</span></span><span style=display:flex><span>go: downloading github.com/brianvoe/gofakeit/v7 v7.3.0
</span></span><span style=display:flex><span>go: downloading github.com/google/uuid v1.6.0
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>&lt;downloading lots of go libs here&gt;
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-06T22:41:20.299-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;produced semantically invalid protobuf message&#34;</span> id<span style=color:#f92672>=</span>072da6ca-8878-4c11-944b-5876a4fc4370
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-06T22:41:20.450-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;produced invalid data&#34;</span> id<span style=color:#f92672>=</span>32bc2119-f1bb-43f4-a9db-28fbb04ff7b5
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-06T22:41:21.588-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;produced semantically valid protobuf message&#34;</span> id<span style=color:#f92672>=</span>d6fc0bf3-e13b-421f-a696-212059d7961d
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>So a lot of data is being generated, some of which is considered semantically invalid. Looking in the RedPanda console, I can see sample data such as <code>foobar</code> as well as other sample data such as <code>$f4525add-da7d-4b2b-aae9-884e7bab535dgarnettwunsch@dickens.netllama</code>, it&rsquo;s clear which of these 2 do not conform to the proto schema.</p><p>Now I&rsquo;ll try the <code>make</code> target <code>make consume-run</code> and see what happens since this is consistent with the snippet in the blog (unlike the produce).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make consume-run
</span></span><span style=display:flex><span>go run ./cmd/bufstream-demo-consume --topic email-updated --group email-verifier <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--csr-url <span style=color:#e6db74>&#34;https://demo.buf.dev/integrations/confluent/bufstream-demo&#34;</span>
</span></span><span style=display:flex><span>make consume-run
</span></span><span style=display:flex><span>go run ./cmd/bufstream-demo-consume --topic email-updated --group email-verifier <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	--csr-url <span style=color:#e6db74>&#34;https://demo.buf.dev/integrations/confluent/bufstream-demo&#34;</span>
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-06T22:48:44.981-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;starting consume&#34;</span>
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-06T22:48:44.982-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;consumed message with new email sisterglover@labadie.biz and old email orvilledickinson@turcotte.biz&#34;</span>
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-06T22:48:45.984-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;consumed message with new email hound and old email antoniomurphy@bradtke.info&#34;</span>
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-06T22:48:45.984-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;consumed malformed data&#34;</span> error<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;registration is missing for encode/decode&#34;</span> length<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>The log message <code>consumed malformed data</code> makes it sound like the consumer is consuming the data even if it is malformed and does not conform to the schema. This seems unexpected but I may have a misunderstanding on how this works.</p><p>The quickstart then suggests to run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./bufstream serve --config config/bufstream.yaml
</span></span></code></pre></div><p>&mldr;which now seems to tie it all together by ensuring that the consumer has a connection to the BSR and ensuring that the Consumer only sees records that comply with the registered schema. I verified that schema enforcement does seem to be happening here by reviewing the terminal output for the producer and consumer. The Producer shows invalid records (e.g. does not conform to the schema):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># sample invalid log message from the producer</span>
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-09T22:36:50.224-05:00 level<span style=color:#f92672>=</span>ERROR msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;error on produce of invalid data&#34;</span> error<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;failed to produce: INVALID_RECORD: This record has failed the validation on the broker and hence been rejected.&#34;</span>
</span></span></code></pre></div><p>&mldr;but the Consumer logs, <em>only</em> showed valid messages such as:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># sample valid log message from the consumer</span>
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span>2025-08-09T22:36:45.222-05:00 level<span style=color:#f92672>=</span>INFO msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;consumed message with new email monkey and old email ernestlegros@parisian.name&#34;</span>
</span></span></code></pre></div><h2 id=bufstream-and-apache-spark>Bufstream and Apache Spark<a hidden class=anchor aria-hidden=true href=#bufstream-and-apache-spark>#</a></h2><p>If Bufstream really is a drop-in replacement for Apache Kafka, reading from Bufstream with Spark should also be straight-forward. Let&rsquo;s confirm this.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> DataFrame
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> functions <span style=color:#66d9ef>as</span> sf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.streaming.readwriter <span style=color:#f92672>import</span> DataStreamWriter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.streaming.query <span style=color:#f92672>import</span> StreamingQuery
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>KAFKA_BOOTSTRAP_SERVERS <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;localhost:9092&#34;</span>
</span></span><span style=display:flex><span>TOPIC <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;email-updated&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df: DataFrame <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    spark
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>readStream
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;kafka&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;kafka.bootstrap.servers&#34;</span>, KAFKA_BOOTSTRAP_SERVERS)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;subscribe&#34;</span>, TOPIC)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;startingOffsets&#34;</span>, <span style=color:#e6db74>&#34;earliest&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>selectExpr(<span style=color:#e6db74>&#34;CAST(key AS STRING)&#34;</span>, <span style=color:#e6db74>&#34;CAST(value AS STRING)&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>datastream_writer: DataStreamWriter <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    df
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>select(<span style=color:#e6db74>&#34;value&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>writeStream
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;console&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;truncate&#34;</span>, <span style=color:#e6db74>&#34;false&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>streaming_query: StreamingQuery <span style=color:#f92672>=</span> datastream_writer<span style=color:#f92672>.</span>start()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>-------------------------------------------</span>
</span></span><span style=display:flex><span>Batch: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-------------------------------------------</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+---------------------------------------------------------------------------------------------+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>value                                                                                        <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+---------------------------------------------------------------------------------------------+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>\n<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>926</span>b3e7f<span style=color:#f92672>-</span><span style=color:#ae81ff>0311</span><span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>b0e<span style=color:#f92672>-</span><span style=color:#ae81ff>8925</span><span style=color:#f92672>-</span>e49b653e3f95jacqueskoss<span style=color:#a6e22e>@lehner.info</span><span style=color:#960050;background-color:#1e0010>�</span>alexisgraham<span style=color:#a6e22e>@raynor.com</span>    <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>\n<span style=color:#960050;background-color:#1e0010>$</span>c0ec4ae6<span style=color:#f92672>-</span>d7c0<span style=color:#f92672>-</span><span style=color:#ae81ff>49</span>d0<span style=color:#f92672>-</span><span style=color:#ae81ff>96</span>f3<span style=color:#f92672>-</span>f45e5df45e78jewelrohan<span style=color:#a6e22e>@strosin.org</span><span style=color:#960050;background-color:#1e0010>�</span>salmon                      <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>foobar                                                                                      <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>\n<span style=color:#960050;background-color:#1e0010>$</span>a1d8b79c<span style=color:#f92672>-</span><span style=color:#ae81ff>87</span>c5<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>dde<span style=color:#f92672>-</span><span style=color:#ae81ff>9183</span><span style=color:#f92672>-</span>b702e5b5b334marshallreynolds<span style=color:#a6e22e>@armstrong.io</span><span style=color:#960050;background-color:#1e0010>�</span>ericklittle<span style=color:#a6e22e>@lang.name</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>\n<span style=color:#960050;background-color:#1e0010>$</span>ab20b911<span style=color:#f92672>-</span><span style=color:#ae81ff>7</span>bee<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>fe9<span style=color:#f92672>-</span><span style=color:#ae81ff>8366</span><span style=color:#f92672>-</span><span style=color:#ae81ff>6211766412</span>dfgissellejohnston<span style=color:#a6e22e>@runolfsson.org</span><span style=color:#960050;background-color:#1e0010>�</span>\vgrasshopper       <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>foobar                                                                                      <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>\n<span style=color:#960050;background-color:#1e0010>$</span>f89d1a48<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>d9b<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>d77<span style=color:#f92672>-</span>a872<span style=color:#f92672>-</span>d39be4c61921rickybechtelar<span style=color:#a6e22e>@stamm.net</span><span style=color:#960050;background-color:#1e0010>�</span>raoulbeahan<span style=color:#a6e22e>@rutherford.io</span> <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>\n<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>8</span>af0f411<span style=color:#f92672>-</span>c2ed<span style=color:#f92672>-</span><span style=color:#ae81ff>494</span>d<span style=color:#f92672>-</span>ac82<span style=color:#f92672>-</span>e495b111045austinanitzsche<span style=color:#a6e22e>@lebsack.net</span><span style=color:#960050;background-color:#1e0010>�</span>\bplatypus              <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>foobar                                                                                      <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>\n<span style=color:#960050;background-color:#1e0010>$</span>a58f1996<span style=color:#f92672>-</span>c6b2<span style=color:#f92672>-</span><span style=color:#ae81ff>416</span>b<span style=color:#f92672>-</span><span style=color:#ae81ff>8531</span><span style=color:#f92672>-</span>a603921dd828marisafunk<span style=color:#a6e22e>@towne.net</span><span style=color:#960050;background-color:#1e0010>�</span>blakerippin<span style=color:#a6e22e>@kemmer.biz</span>        <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>\n<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>48</span>fa5f74<span style=color:#f92672>-</span><span style=color:#ae81ff>5921</span><span style=color:#f92672>-</span><span style=color:#ae81ff>4061</span><span style=color:#f92672>-</span>ad25<span style=color:#f92672>-</span><span style=color:#ae81ff>559</span>ce2cb6e7aeleonoreupton<span style=color:#a6e22e>@rohan.net</span><span style=color:#960050;background-color:#1e0010>�</span>wasp                       <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>foobar
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span></code></pre></div><p>So this appeared to work at a very basic level, but this is not how we should be performing reads via Spark given that we want our consumers to know and cross-reference the expected schema defined in the BSR. Given that we confirmed this worked as expected with the above Consumer, we will for now presume that this capability is possible with Spark as well and leave the full Spark &ndash;> BSR integration for later research.</p><h2 id=what-about-iceberg>What about Iceberg?<a hidden class=anchor aria-hidden=true href=#what-about-iceberg>#</a></h2><p>I followed the <a href=https://buf.build/docs/bufstream/iceberg/quickstart>Iceberg quickstart</a> but it seems like the compose file is referencing a <code>spark/</code> directory that doesn&rsquo;t exist: <code>OSError: Dockerfile not found in /home/username/tmp/buf-examples/bufstream/iceberg-quickstart/spark</code> .So I couldn&rsquo;t get this working but according to the guide, all you need is:</p><ul><li>Bufstream broker</li><li>object storage</li><li>an Iceberg catalog implementation</li></ul><p>The details of your Iceberg catalog are defined in <code>config/bufstream.yaml</code>.</p><h2 id=what-about-bufstream-in-production>What about Bufstream in Production?<a hidden class=anchor aria-hidden=true href=#what-about-bufstream-in-production>#</a></h2><p>The initial impression I had of Bustream is that deployments are super simple. I suppose this is true, in comparison to a Kafka deployment, but it still requires:</p><ul><li>Object storage such as AWS S3, Google Cloud Storage, or Azure Blob Storage.</li><li>A metadata storage service such as PostgreSQL.</li></ul><p>There are numerous different deployment options such as Docker, Helm, and others with a provided Terraform module. It&rsquo;s unclear if BSR requires an additional set of deployments or if that is integrated into Postgres.</p><h2 id=recap>Recap<a hidden class=anchor aria-hidden=true href=#recap>#</a></h2><ul><li>Buf&rsquo;s vision seems well-thought and clearly-defined.<ul><li>A simplified Kafka-compatible streaming engine</li><li>A single way to manage schemas with Protobuf</li><li>Schema enforcement with the BSR</li><li>Data Lakehouse integration with Apache Iceberg</li></ul></li><li>The streaming market has no shortage of vended options but Buf&rsquo;s vision really makes it stand out.</li><li>Protobuf has the reputation for being less user-friendly than other serialization formats like JSON but this may change if the <code>buf</code> CLI can keep it&rsquo;s promises of simplifying the schema mangement experience.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://be-rock.github.io/blog/tags/bufstream/>Bufstream</a></li><li><a href=https://be-rock.github.io/blog/tags/kafka/>Kafka</a></li><li><a href=https://be-rock.github.io/blog/tags/protobuf/>Protobuf</a></li><li><a href=https://be-rock.github.io/blog/tags/spark/>Spark</a></li><li><a href=https://be-rock.github.io/blog/tags/streaming/>Streaming</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://be-rock.github.io/blog/>Brock B's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>