<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | Brock B's Blog</title><meta name=keywords content><meta name=description content="Posts - Brock B's Blog"><meta name=author content><link rel=canonical href=https://be-rock.github.io/blog/posts/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://be-rock.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://be-rock.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://be-rock.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://be-rock.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://be-rock.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://be-rock.github.io/blog/posts/index.xml><link rel=alternate hreflang=en href=https://be-rock.github.io/blog/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://be-rock.github.io/blog/posts/"><meta property="og:site_name" content="Brock B's Blog"><meta property="og:title" content="Posts"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://be-rock.github.io/blog/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://be-rock.github.io/blog/ accesskey=h title="Brock B's Blog (Alt + H)">Brock B's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://be-rock.github.io/blog/ title=Home><span>Home</span></a></li><li><a href=https://be-rock.github.io/blog/posts/ title=Posts><span class=active>Posts</span></a></li><li><a href=https://be-rock.github.io/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://be-rock.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Using the StreamingQueryListener with Spark Streaming</h2></header><div class=entry-content><p>Summary This is an example of how to setup a Spark StreamingQueryListener that writes to some sample targets such as json and Postgres.
Setup Streaming Query from pyspark.sql import DataFrame from pyspark.sql import functions as f from pyspark.sql.streaming import DataStreamWriter, StreamingQuery NUM = 1 SOURCE = "rate" TARGET = "noop" QUERY_NAME = f"{SOURCE}-{TARGET}-{NUM}" CHECKPOINT_LOCATION = f"file:/tmp/checkpoint/{QUERY_NAME}" readstream_options = { "rowsPerSecond": "1", } trigger_options = { "processingTime": "10 seconds", } writestream_options = { "checkpointLocation": CHECKPOINT_LOCATION, } readstream_df: DataFrame = ( spark .readStream .format(SOURCE) .options(**readstream_options) .load() ) datastream_writer: DataStreamWriter = ( readstream_df .writeStream .trigger(**trigger_options) .format(TARGET) .options(**writestream_options) .queryName(QUERY_NAME) ) streaming_query: StreamingQuery = datastream_writer.start() Streaming Query Listener - json to stdout import datetime import json from pyspark.sql.streaming.listener import ( QueryIdleEvent, QueryProgressEvent, QueryStartedEvent, QueryTerminatedEvent, StreamingQueryListener, ) class StreamingQueryListenerJson(StreamingQueryListener): def onQueryStarted(self, event: QueryStartedEvent) -> None: payload = { "time": datetime.datetime.now().isoformat(), "event": "onQueryStarted", "id": event.id, "name": event.name, "runId": event.runId, "message": "query started" } print(payload) def onQueryProgress(self, event: QueryProgressEvent) -> None: json_data = json.loads(event.progress.json) payload = { "time": datetime.datetime.now().isoformat(), "event": "onQueryProgress", "id": json_data.get("id"), "name": json_data.get("name"), "runId": json_data.get("runId"), "message": json.dumps(json_data) } print(payload) def onQueryIdle(self, event: QueryIdleEvent) -> None: payload = { "time": datetime.datetime.now().isoformat(), "event": "onQueryIdle", "id": event.id, "name": event.name, "runId": event.runId, "message": "query is idle" } print(payload) def onQueryTerminated(self, event: QueryTerminatedEvent) -> None: payload = { "time": datetime.datetime.now().isoformat(), "event": "onQueryIdle", "id": event.id, "name": event.name, "runId": event.runId, "message": "query is terminated" } print(payload) spark.streams.addListener(StreamingQueryListenerJson()) Streaming Query Listener - json to a file import datetime from pyspark.sql.streaming.listener import ( QueryIdleEvent, QueryProgressEvent, QueryStartedEvent, QueryTerminatedEvent, StreamingQueryListener, ) class StreamingQueryListenerJson(StreamingQueryListener): def __init__(self) -> None: self.f = open("/tmp/f.json", "w") def onQueryStarted(self, event: QueryStartedEvent) -> None: payload = { "time": datetime.datetime.now().isoformat(), "event": "onQueryStarted", "id": event.id, "name": event.name, "runId": event.runId, "message": "query started" } self.f.write(json.dumps(payload)) def onQueryProgress(self, event: QueryProgressEvent) -> None: json_data = json.loads(event.progress.json) payload = { "time": datetime.datetime.now().isoformat(), "event": "onQueryProgress", "id": json_data.get("id"), "name": json_data.get("name"), "runId": json_data.get("runId"), "message": json.dumps(json_data) } self.f.write(json.dumps(payload)) def onQueryIdle(self, event: QueryIdleEvent) -> None: payload = { "time": datetime.datetime.now().isoformat(), "event": "onQueryIdle", "id": event.id, "name": event.name, "runId": event.runId, "message": "query is idle" } self.f.write(json.dumps(payload)) def onQueryTerminated(self, event: QueryTerminatedEvent) -> None: payload = { "time": datetime.datetime.now().isoformat(), "event": "onQueryIdle", "id": event.id, "name": event.name, "runId": event.runId, "message": "query is terminated" } self.f.write(json.dumps(payload)) self.f.close() spark.streams.addListener(StreamingQueryListenerJson()) Streaming Query Listener - Sqlite Sqlite is not multi-threaded so we can experience failures with concurrent writes. One way to prevent this issue from happening, is to append the StreamingQueryListener events to append to a Python queue.Queue() instead of directly to the database, so that all writes can be managed by a single thread.
...</p></div><footer class=entry-footer><span title='2025-10-13 20:08:27 -0500 -0500'>October 13, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;804 words</footer><a class=entry-link aria-label="post link to Using the StreamingQueryListener with Spark Streaming" href=https://be-rock.github.io/blog/posts/spark-streaming-streamingquerylistener/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Apache Spark Streaming - templatized</h2></header><div class=entry-content><p>Summary This is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a foreachBatch sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like yaml, if so desired.
A Streaming Query template from pyspark.sql import DataFrame from pyspark.sql import functions as f from pyspark.sql.streaming import DataStreamWriter, StreamingQuery NUM = 1 SOURCE = "rate" # delta, kafka, rate TARGET = "noop" # delta, kafka, console, noop QUERY_NAME = f"{SOURCE}-{TARGET}-{NUM}" CHECKPOINT_LOCATION = f"file:/tmp/checkpoint/{QUERY_NAME}" readstream_options = { "rowsPerSecond": "1", # rate # "skipChangeCommits": "true" # delta # "kafka.bootstrap.servers": "localhost:9092", # kafka # "subscribe": "topic1", # kafka # "startingOffsets": "latest", # kafka } trigger_options = { "processingTime": "1 seconds", # "availableNow": "true", } writestream_options = { "checkpointLocation": CHECKPOINT_LOCATION, } def apply_transformations(df: DataFrame) -> DataFrame: return df.withColumn("current_timestamp", f.current_timestamp()) readstream_df: DataFrame = ( spark .readStream .format(SOURCE) .options(**readstream_options) .load() ) transformed_df: DataFrame = apply_transformations(df=readstream_df) datastream_writer: DataStreamWriter = ( transformed_df .writeStream .trigger(**trigger_options) .format(TARGET) .options(**writestream_options) .queryName(QUERY_NAME) ) streaming_query: StreamingQuery = datastream_writer.start() A summary of the above template Dictionaries to manage: stream source options stream sink options trigger options A function to manage all transformations using DataFrame.transform prior to writing to the sink, that can be expanded on A snippet for the DataStreamWriter And finally, a call to .start() to return a StreamingQuery Note that this would need to change to toTable() if writing to a table sink such as delta or iceberg. Further templatizing this code You may wnat to codify this further into a markup language like yaml and then make the configuration strongly-typed using pydantic. Here’s an example of what that might look like:
...</p></div><footer class=entry-footer><span title='2025-10-12 20:21:51 -0500 -0500'>October 12, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;363 words</footer><a class=entry-link aria-label="post link to Apache Spark Streaming - templatized" href=https://be-rock.github.io/blog/posts/apache-spark-streaming-templatized/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>My Recent Python Toolkit</h2></header><div class=entry-content><p>Introduction The Python development ecosystem has changed pretty dramatically over the recent years.
This post provides a practical introduction to the toolkit that I’ve been using, explaining why I chose each tool and how they work together.
The Toolkit Command Runner - make Configuration - pydantic Containerization - podman Continuous Integration - GitHub Actions IDE - cursor Linting & Formatting - ruff Load Testing - locust Logging - logging Pre-commit - prek Python project and environment management - uv Scheduling - cron Testing - pytest Type Checking - ty Command Runner - make While not initially designed for this purpose, make works very well as a command runner. A Makefile can be created in your project root to represent common ways for your project to be used. It is also self-documenting and a helpful way for newcomers to get started with your project.
...</p></div><footer class=entry-footer><span title='2025-09-29 21:00:24 -0500 -0500'>September 29, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1384 words</footer><a class=entry-link aria-label="post link to My Recent Python Toolkit" href=https://be-rock.github.io/blog/posts/my-recent-python-toolkit/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Evaluating V (Language)</h2></header><div class=entry-content><p>Summary I enjoy learning new languages, and decided it would be fun to do some learning the ‘old-school’ way, meaning no AI-assisted coding, trial-and-error, and using the docs. Note - LLMs will be used merely as a search-engine equivalent to aid with solutions and resolve issues, but not to build a solution. I have worked with numerous languages in the past, but 2 that have been on my radar are go and V. This post will be about V but I hope to do something similar for go. V promises to be stable (despite not yet having reached 1.0 release), easy to learn (“can be learned over the course of a weekend”), fast, and is statically typed.
...</p></div><footer class=entry-footer><span title='2025-08-25 19:39:16 -0500 -0500'>August 25, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1236 words</footer><a class=entry-link aria-label="post link to Evaluating V (Language)" href=https://be-rock.github.io/blog/posts/evaluating-vlang/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Streaming with Bufstream, Protobuf, and Spark</h2></header><div class=entry-content><p>Summary Bufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary Bufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink What is the point? I often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing. This blog attempts to test the claims of Buf - the company behind: Bufstream - the “drop-in replacement for Kafka” BSR - the Buf Schema Registry which implements the Confluent Schema Registry API buf CLI - a simple way to develop and manage Protobuf Note that the buf CLI will be referenced in this blog, but less of a focus.
...</p></div><footer class=entry-footer><span title='2025-08-09 22:29:16 -0500 -0500'>August 9, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1435 words</footer><a class=entry-link aria-label="post link to Streaming with Bufstream, Protobuf, and Spark" href=https://be-rock.github.io/blog/posts/kafka-protobuf-and-bufstream/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Using a Container to run PySpark Unit Tests</h2></header><div class=entry-content><p>Summary Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding Why, just why? Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.
...</p></div><footer class=entry-footer><span title='2025-07-12 12:58:15 -0500 -0500'>July 12, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;761 words</footer><a class=entry-link aria-label="post link to Using a Container to run PySpark Unit Tests" href=https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Building a Pyspark Custom Data Sources for DuckDB</h2></header><div class=entry-content><p>Summary Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources A custom data source allows you to connect to a source system that that Spark may not currently have support for PySpark Custom Data Sources - an Overview Starting with Apache Spark 4.0 and Databricks 15.2, PySpark supports custom data sources.
So what are PySpark Custom Data Sources?
Custom data sources allow you to define a source format other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint.
...</p></div><footer class=entry-footer><span title='2025-06-14 23:51:42 -0500 -0500'>June 14, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;658 words</footer><a class=entry-link aria-label="post link to Building a Pyspark Custom Data Sources for DuckDB" href=https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/></a></article></main><footer class=footer><span>&copy; 2025 <a href=https://be-rock.github.io/blog/>Brock B's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>