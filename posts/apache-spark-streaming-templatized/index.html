<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Apache Spark Streaming - templatized | Brock's Blog on Data|DevOps|Cloud</title><meta name=keywords content="spark,streaming,config,pydantic"><meta name=description content='Summary
This is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a foreachBatch sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like yaml, if so desired.
A Streaming Query template
from pyspark.sql import DataFrame
from pyspark.sql import functions as f
from pyspark.sql.streaming import DataStreamWriter, StreamingQuery

NUM = 1
SOURCE = "rate" # delta, kafka, rate
TARGET = "noop" # delta, kafka, console, noop
QUERY_NAME = f"{SOURCE}-{TARGET}-{NUM}"
CHECKPOINT_LOCATION = f"file:/tmp/checkpoint/{QUERY_NAME}"

readstream_options = {
    "rowsPerSecond": "1", # rate
    # "skipChangeCommits": "true" # delta
    # "kafka.bootstrap.servers": "localhost:9092", # kafka
    # "subscribe": "topic1", # kafka
    # "startingOffsets": "latest", # kafka
}

trigger_options = {
    "processingTime": "1 seconds",
    # "availableNow": "true",
}

writestream_options = {
    "checkpointLocation": CHECKPOINT_LOCATION,
}


def apply_transformations(df: DataFrame) -> DataFrame:
    return df.withColumn("current_timestamp", f.current_timestamp())


readstream_df: DataFrame = (
    spark
    .readStream
    .format(SOURCE)
    .options(**readstream_options)
    .load()
)

transformed_df: DataFrame = apply_transformations(df=readstream_df)

datastream_writer: DataStreamWriter = (
    transformed_df
    .writeStream
    .trigger(**trigger_options)
    .format(TARGET)
    .options(**writestream_options)
    .queryName(QUERY_NAME)
)

streaming_query: StreamingQuery = datastream_writer.start()
A summary of the above template

Dictionaries to manage:

stream source options
stream sink options
trigger options


A function to manage all transformations using DataFrame.transform prior to writing to the sink, that can be expanded on
A snippet for the DataStreamWriter
And finally, a call to .start() to return a StreamingQuery

Note that this would need to change to toTable() if writing to a table sink such as delta or iceberg.



Further templatizing this code
You may wnat to codify this further into a markup language like yaml and then make the configuration strongly-typed using pydantic. Here&rsquo;s an example of what that might look like:'><meta name=author content><link rel=canonical href=https://be-rock.github.io/blog/posts/apache-spark-streaming-templatized/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://be-rock.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://be-rock.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://be-rock.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://be-rock.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://be-rock.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://be-rock.github.io/blog/posts/apache-spark-streaming-templatized/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://be-rock.github.io/blog/posts/apache-spark-streaming-templatized/"><meta property="og:site_name" content="Brock's Blog on Data|DevOps|Cloud"><meta property="og:title" content="Apache Spark Streaming - templatized"><meta property="og:description" content='Summary This is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a foreachBatch sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like yaml, if so desired.
A Streaming Query template from pyspark.sql import DataFrame from pyspark.sql import functions as f from pyspark.sql.streaming import DataStreamWriter, StreamingQuery NUM = 1 SOURCE = "rate" # delta, kafka, rate TARGET = "noop" # delta, kafka, console, noop QUERY_NAME = f"{SOURCE}-{TARGET}-{NUM}" CHECKPOINT_LOCATION = f"file:/tmp/checkpoint/{QUERY_NAME}" readstream_options = { "rowsPerSecond": "1", # rate # "skipChangeCommits": "true" # delta # "kafka.bootstrap.servers": "localhost:9092", # kafka # "subscribe": "topic1", # kafka # "startingOffsets": "latest", # kafka } trigger_options = { "processingTime": "1 seconds", # "availableNow": "true", } writestream_options = { "checkpointLocation": CHECKPOINT_LOCATION, } def apply_transformations(df: DataFrame) -> DataFrame: return df.withColumn("current_timestamp", f.current_timestamp()) readstream_df: DataFrame = ( spark .readStream .format(SOURCE) .options(**readstream_options) .load() ) transformed_df: DataFrame = apply_transformations(df=readstream_df) datastream_writer: DataStreamWriter = ( transformed_df .writeStream .trigger(**trigger_options) .format(TARGET) .options(**writestream_options) .queryName(QUERY_NAME) ) streaming_query: StreamingQuery = datastream_writer.start() A summary of the above template Dictionaries to manage: stream source options stream sink options trigger options A function to manage all transformations using DataFrame.transform prior to writing to the sink, that can be expanded on A snippet for the DataStreamWriter And finally, a call to .start() to return a StreamingQuery Note that this would need to change to toTable() if writing to a table sink such as delta or iceberg. Further templatizing this code You may wnat to codify this further into a markup language like yaml and then make the configuration strongly-typed using pydantic. Here’s an example of what that might look like:'><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-12T20:21:51-05:00"><meta property="article:modified_time" content="2025-10-12T20:21:51-05:00"><meta property="article:tag" content="Spark"><meta property="article:tag" content="Streaming"><meta property="article:tag" content="Config"><meta property="article:tag" content="Pydantic"><meta name=twitter:card content="summary"><meta name=twitter:title content="Apache Spark Streaming - templatized"><meta name=twitter:description content='Summary
This is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a foreachBatch sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like yaml, if so desired.
A Streaming Query template
from pyspark.sql import DataFrame
from pyspark.sql import functions as f
from pyspark.sql.streaming import DataStreamWriter, StreamingQuery

NUM = 1
SOURCE = "rate" # delta, kafka, rate
TARGET = "noop" # delta, kafka, console, noop
QUERY_NAME = f"{SOURCE}-{TARGET}-{NUM}"
CHECKPOINT_LOCATION = f"file:/tmp/checkpoint/{QUERY_NAME}"

readstream_options = {
    "rowsPerSecond": "1", # rate
    # "skipChangeCommits": "true" # delta
    # "kafka.bootstrap.servers": "localhost:9092", # kafka
    # "subscribe": "topic1", # kafka
    # "startingOffsets": "latest", # kafka
}

trigger_options = {
    "processingTime": "1 seconds",
    # "availableNow": "true",
}

writestream_options = {
    "checkpointLocation": CHECKPOINT_LOCATION,
}


def apply_transformations(df: DataFrame) -> DataFrame:
    return df.withColumn("current_timestamp", f.current_timestamp())


readstream_df: DataFrame = (
    spark
    .readStream
    .format(SOURCE)
    .options(**readstream_options)
    .load()
)

transformed_df: DataFrame = apply_transformations(df=readstream_df)

datastream_writer: DataStreamWriter = (
    transformed_df
    .writeStream
    .trigger(**trigger_options)
    .format(TARGET)
    .options(**writestream_options)
    .queryName(QUERY_NAME)
)

streaming_query: StreamingQuery = datastream_writer.start()
A summary of the above template

Dictionaries to manage:

stream source options
stream sink options
trigger options


A function to manage all transformations using DataFrame.transform prior to writing to the sink, that can be expanded on
A snippet for the DataStreamWriter
And finally, a call to .start() to return a StreamingQuery

Note that this would need to change to toTable() if writing to a table sink such as delta or iceberg.



Further templatizing this code
You may wnat to codify this further into a markup language like yaml and then make the configuration strongly-typed using pydantic. Here&rsquo;s an example of what that might look like:'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://be-rock.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Apache Spark Streaming - templatized","item":"https://be-rock.github.io/blog/posts/apache-spark-streaming-templatized/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Spark Streaming - templatized","name":"Apache Spark Streaming - templatized","description":"Summary This is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a foreachBatch sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like yaml, if so desired.\nA Streaming Query template from pyspark.sql import DataFrame from pyspark.sql import functions as f from pyspark.sql.streaming import DataStreamWriter, StreamingQuery NUM = 1 SOURCE = \u0026#34;rate\u0026#34; # delta, kafka, rate TARGET = \u0026#34;noop\u0026#34; # delta, kafka, console, noop QUERY_NAME = f\u0026#34;{SOURCE}-{TARGET}-{NUM}\u0026#34; CHECKPOINT_LOCATION = f\u0026#34;file:/tmp/checkpoint/{QUERY_NAME}\u0026#34; readstream_options = { \u0026#34;rowsPerSecond\u0026#34;: \u0026#34;1\u0026#34;, # rate # \u0026#34;skipChangeCommits\u0026#34;: \u0026#34;true\u0026#34; # delta # \u0026#34;kafka.bootstrap.servers\u0026#34;: \u0026#34;localhost:9092\u0026#34;, # kafka # \u0026#34;subscribe\u0026#34;: \u0026#34;topic1\u0026#34;, # kafka # \u0026#34;startingOffsets\u0026#34;: \u0026#34;latest\u0026#34;, # kafka } trigger_options = { \u0026#34;processingTime\u0026#34;: \u0026#34;1 seconds\u0026#34;, # \u0026#34;availableNow\u0026#34;: \u0026#34;true\u0026#34;, } writestream_options = { \u0026#34;checkpointLocation\u0026#34;: CHECKPOINT_LOCATION, } def apply_transformations(df: DataFrame) -\u0026gt; DataFrame: return df.withColumn(\u0026#34;current_timestamp\u0026#34;, f.current_timestamp()) readstream_df: DataFrame = ( spark .readStream .format(SOURCE) .options(**readstream_options) .load() ) transformed_df: DataFrame = apply_transformations(df=readstream_df) datastream_writer: DataStreamWriter = ( transformed_df .writeStream .trigger(**trigger_options) .format(TARGET) .options(**writestream_options) .queryName(QUERY_NAME) ) streaming_query: StreamingQuery = datastream_writer.start() A summary of the above template Dictionaries to manage: stream source options stream sink options trigger options A function to manage all transformations using DataFrame.transform prior to writing to the sink, that can be expanded on A snippet for the DataStreamWriter And finally, a call to .start() to return a StreamingQuery Note that this would need to change to toTable() if writing to a table sink such as delta or iceberg. Further templatizing this code You may wnat to codify this further into a markup language like yaml and then make the configuration strongly-typed using pydantic. Here\u0026rsquo;s an example of what that might look like:\n","keywords":["spark","streaming","config","pydantic"],"articleBody":"Summary This is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a foreachBatch sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like yaml, if so desired.\nA Streaming Query template from pyspark.sql import DataFrame from pyspark.sql import functions as f from pyspark.sql.streaming import DataStreamWriter, StreamingQuery NUM = 1 SOURCE = \"rate\" # delta, kafka, rate TARGET = \"noop\" # delta, kafka, console, noop QUERY_NAME = f\"{SOURCE}-{TARGET}-{NUM}\" CHECKPOINT_LOCATION = f\"file:/tmp/checkpoint/{QUERY_NAME}\" readstream_options = { \"rowsPerSecond\": \"1\", # rate # \"skipChangeCommits\": \"true\" # delta # \"kafka.bootstrap.servers\": \"localhost:9092\", # kafka # \"subscribe\": \"topic1\", # kafka # \"startingOffsets\": \"latest\", # kafka } trigger_options = { \"processingTime\": \"1 seconds\", # \"availableNow\": \"true\", } writestream_options = { \"checkpointLocation\": CHECKPOINT_LOCATION, } def apply_transformations(df: DataFrame) -\u003e DataFrame: return df.withColumn(\"current_timestamp\", f.current_timestamp()) readstream_df: DataFrame = ( spark .readStream .format(SOURCE) .options(**readstream_options) .load() ) transformed_df: DataFrame = apply_transformations(df=readstream_df) datastream_writer: DataStreamWriter = ( transformed_df .writeStream .trigger(**trigger_options) .format(TARGET) .options(**writestream_options) .queryName(QUERY_NAME) ) streaming_query: StreamingQuery = datastream_writer.start() A summary of the above template Dictionaries to manage: stream source options stream sink options trigger options A function to manage all transformations using DataFrame.transform prior to writing to the sink, that can be expanded on A snippet for the DataStreamWriter And finally, a call to .start() to return a StreamingQuery Note that this would need to change to toTable() if writing to a table sink such as delta or iceberg. Further templatizing this code You may wnat to codify this further into a markup language like yaml and then make the configuration strongly-typed using pydantic. Here’s an example of what that might look like:\n# app-conf.yaml source: format: rate rowsPerSecond: 1 trigger: processingTime: \"1 seconds\" target: checkpointLocation: \"/tmp/checkpoint\" format: noop query_name: my-streaming-query from pydantic_settings import BaseSettings, YamlConfigSettingsSource class AppConfig(BaseSettings): source: dict trigger: dict target: dict query_name: str config: YamlConfigSettingsSource = YamlConfigSettingsSource( settings_cls=AppConfig, yaml_file=\"app-conf.yaml\", ) config.yaml_data {'source': {'format': 'rate', 'rowsPerSecond': 1}, 'trigger': {'processingTime': '1 seconds'}, 'target': {'checkpointLocation': '/tmp/checkpoint', 'format': 'noop'}, 'query_name': 'my-streaming-query'} app_config: AppConfig = AppConfig(**config.yaml_data) assert app_config.query_name == \"my-streaming-query\" ","wordCount":"363","inLanguage":"en","datePublished":"2025-10-12T20:21:51-05:00","dateModified":"2025-10-12T20:21:51-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://be-rock.github.io/blog/posts/apache-spark-streaming-templatized/"},"publisher":{"@type":"Organization","name":"Brock's Blog on Data|DevOps|Cloud","logo":{"@type":"ImageObject","url":"https://be-rock.github.io/blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://be-rock.github.io/blog/ accesskey=h title="Brock's Blog on Data|DevOps|Cloud (Alt + H)">Brock's Blog on Data|DevOps|Cloud</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://be-rock.github.io/blog/ title=Home><span>Home</span></a></li><li><a href=https://be-rock.github.io/blog/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://be-rock.github.io/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://be-rock.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://be-rock.github.io/blog/conversations/ title=Conversations><span>Conversations</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Apache Spark Streaming - templatized</h1><div class=post-meta><span title='2025-10-12 20:21:51 -0500 -0500'>October 12, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;363 words</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#a-streaming-query-template aria-label="A Streaming Query template">A Streaming Query template</a><ul><li><a href=#a-summary-of-the-above-template aria-label="A summary of the above template">A summary of the above template</a></li><li><a href=#further-templatizing-this-code aria-label="Further templatizing this code">Further templatizing this code</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>This is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a <code>foreachBatch</code> sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like <code>yaml</code>, if so desired.</p><h2 id=a-streaming-query-template>A Streaming Query template<a hidden class=anchor aria-hidden=true href=#a-streaming-query-template>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> DataFrame
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> functions <span style=color:#66d9ef>as</span> f
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.streaming <span style=color:#f92672>import</span> DataStreamWriter, StreamingQuery
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NUM <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>SOURCE <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;rate&#34;</span> <span style=color:#75715e># delta, kafka, rate</span>
</span></span><span style=display:flex><span>TARGET <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;noop&#34;</span> <span style=color:#75715e># delta, kafka, console, noop</span>
</span></span><span style=display:flex><span>QUERY_NAME <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>SOURCE<span style=color:#e6db74>}</span><span style=color:#e6db74>-</span><span style=color:#e6db74>{</span>TARGET<span style=color:#e6db74>}</span><span style=color:#e6db74>-</span><span style=color:#e6db74>{</span>NUM<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>CHECKPOINT_LOCATION <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;file:/tmp/checkpoint/</span><span style=color:#e6db74>{</span>QUERY_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>readstream_options <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;rowsPerSecond&#34;</span>: <span style=color:#e6db74>&#34;1&#34;</span>, <span style=color:#75715e># rate</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># &#34;skipChangeCommits&#34;: &#34;true&#34; # delta</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># &#34;kafka.bootstrap.servers&#34;: &#34;localhost:9092&#34;, # kafka</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># &#34;subscribe&#34;: &#34;topic1&#34;, # kafka</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># &#34;startingOffsets&#34;: &#34;latest&#34;, # kafka</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trigger_options <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;processingTime&#34;</span>: <span style=color:#e6db74>&#34;1 seconds&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#75715e># &#34;availableNow&#34;: &#34;true&#34;,</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>writestream_options <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;checkpointLocation&#34;</span>: CHECKPOINT_LOCATION,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>apply_transformations</span>(df: DataFrame) <span style=color:#f92672>-&gt;</span> DataFrame:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> df<span style=color:#f92672>.</span>withColumn(<span style=color:#e6db74>&#34;current_timestamp&#34;</span>, f<span style=color:#f92672>.</span>current_timestamp())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>readstream_df: DataFrame <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    spark
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>readStream
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>format(SOURCE)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>options(<span style=color:#f92672>**</span>readstream_options)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>transformed_df: DataFrame <span style=color:#f92672>=</span> apply_transformations(df<span style=color:#f92672>=</span>readstream_df)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>datastream_writer: DataStreamWriter <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    transformed_df
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>writeStream
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>trigger(<span style=color:#f92672>**</span>trigger_options)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>format(TARGET)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>options(<span style=color:#f92672>**</span>writestream_options)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>queryName(QUERY_NAME)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>streaming_query: StreamingQuery <span style=color:#f92672>=</span> datastream_writer<span style=color:#f92672>.</span>start()
</span></span></code></pre></div><h3 id=a-summary-of-the-above-template>A summary of the above template<a hidden class=anchor aria-hidden=true href=#a-summary-of-the-above-template>#</a></h3><ul><li>Dictionaries to manage:<ul><li>stream source options</li><li>stream sink options</li><li>trigger options</li></ul></li><li>A function to manage all transformations using <a href=https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.transform.html>DataFrame.transform</a> prior to writing to the sink, that can be expanded on</li><li>A snippet for the <a href=https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>DataStreamWriter</a></li><li>And finally, a call to <code>.start()</code> to return a <a href=https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.html>StreamingQuery</a><ul><li>Note that this would need to change to <code>toTable()</code> if writing to a table sink such as <code>delta</code> or <code>iceberg</code>.</li></ul></li></ul><h3 id=further-templatizing-this-code>Further templatizing this code<a hidden class=anchor aria-hidden=true href=#further-templatizing-this-code>#</a></h3><p>You may wnat to codify this further into a markup language like <code>yaml</code> and then make the configuration strongly-typed using <code>pydantic</code>. Here&rsquo;s an example of what that might look like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># app-conf.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>source</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>format</span>: <span style=color:#ae81ff>rate</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>rowsPerSecond</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>trigger</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>processingTime</span>: <span style=color:#e6db74>&#34;1 seconds&#34;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>target</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>checkpointLocation</span>: <span style=color:#e6db74>&#34;/tmp/checkpoint&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>format</span>: <span style=color:#ae81ff>noop</span>
</span></span><span style=display:flex><span><span style=color:#f92672>query_name</span>: <span style=color:#ae81ff>my-streaming-query</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pydantic_settings <span style=color:#f92672>import</span> BaseSettings, YamlConfigSettingsSource
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AppConfig</span>(BaseSettings):
</span></span><span style=display:flex><span>    source: dict
</span></span><span style=display:flex><span>    trigger: dict
</span></span><span style=display:flex><span>    target: dict
</span></span><span style=display:flex><span>    query_name: str
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>config: YamlConfigSettingsSource <span style=color:#f92672>=</span> YamlConfigSettingsSource(
</span></span><span style=display:flex><span>    settings_cls<span style=color:#f92672>=</span>AppConfig,
</span></span><span style=display:flex><span>    yaml_file<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;app-conf.yaml&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>config<span style=color:#f92672>.</span>yaml_data
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;source&#39;</span>: {<span style=color:#e6db74>&#39;format&#39;</span>: <span style=color:#e6db74>&#39;rate&#39;</span>, <span style=color:#e6db74>&#39;rowsPerSecond&#39;</span>: <span style=color:#ae81ff>1</span>},
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;trigger&#39;</span>: {<span style=color:#e6db74>&#39;processingTime&#39;</span>: <span style=color:#e6db74>&#39;1 seconds&#39;</span>},
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;target&#39;</span>: {<span style=color:#e6db74>&#39;checkpointLocation&#39;</span>: <span style=color:#e6db74>&#39;/tmp/checkpoint&#39;</span>, <span style=color:#e6db74>&#39;format&#39;</span>: <span style=color:#e6db74>&#39;noop&#39;</span>},
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;query_name&#39;</span>: <span style=color:#e6db74>&#39;my-streaming-query&#39;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>app_config: AppConfig <span style=color:#f92672>=</span> AppConfig(<span style=color:#f92672>**</span>config<span style=color:#f92672>.</span>yaml_data)
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> app_config<span style=color:#f92672>.</span>query_name <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;my-streaming-query&#34;</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://be-rock.github.io/blog/tags/spark/>Spark</a></li><li><a href=https://be-rock.github.io/blog/tags/streaming/>Streaming</a></li><li><a href=https://be-rock.github.io/blog/tags/config/>Config</a></li><li><a href=https://be-rock.github.io/blog/tags/pydantic/>Pydantic</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://be-rock.github.io/blog/>Brock's Blog on Data|DevOps|Cloud</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>