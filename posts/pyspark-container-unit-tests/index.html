<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using a Container to run PySpark Unit Tests | Brock B's Blog</title><meta name=keywords content="containers,spark,testing"><meta name=description content="Summary

Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code
This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding

Why, just why?
Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals."><meta name=author content><link rel=canonical href=https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://be-rock.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://be-rock.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://be-rock.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://be-rock.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://be-rock.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/"><meta property="og:site_name" content="Brock B's Blog"><meta property="og:title" content="Using a Container to run PySpark Unit Tests"><meta property="og:description" content="Summary Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding Why, just why? Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-12T12:58:15-05:00"><meta property="article:modified_time" content="2025-07-12T12:58:15-05:00"><meta property="article:tag" content="Containers"><meta property="article:tag" content="Spark"><meta property="article:tag" content="Testing"><meta name=twitter:card content="summary"><meta name=twitter:title content="Using a Container to run PySpark Unit Tests"><meta name=twitter:description content="Summary

Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code
This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding

Why, just why?
Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://be-rock.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Using a Container to run PySpark Unit Tests","item":"https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using a Container to run PySpark Unit Tests","name":"Using a Container to run PySpark Unit Tests","description":"Summary Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding Why, just why? Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.\n","keywords":["containers","spark","testing"],"articleBody":"Summary Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding Why, just why? Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.\nRather than running these tests on-demand, or add these examples in a notebook, I thought that it would be useful to build up a little library of these commands that can be run using pytest where performance can be evaluated and could also be used as a reference/cheatsheet.\nDoes this library serve any real practical purpose other than learning and understanding? No, not really. But maybe this can be a spring-board for something else\nSetup a basic project structure I find it helpful to use a Makefile to produce a consistent and a well-documented and repeatable build process. The Makefile is in References/Makefile. The basic project structure when completed will be:\n❯ tree . ├── Containerfile ├── Makefile ├── pyproject.toml └── tests └── test_pyspark.py 1 directory, 4 files The container image specification Following is the Containerfile (a Dockerfile would also suffice). The specifications of the container image are:\n# Containerfile FROM ghcr.io/astral-sh/uv:debian # Java is required for PySpark RUN apt-get update \u0026\u0026 \\ apt-get install -y openjdk-17-jdk \u0026\u0026 \\ apt-get clean \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* WORKDIR /app COPY pyproject.toml /app/pyproject.toml RUN uv sync ENV PYSPARK_PYTHON=/usr/bin/python3 COPY tests/ /app/tests/ CMD [\"uv\", \"run\", \".venv/bin/pytest\", \"--durations=0\", \"-v\", \"/app/tests/\"] The Python project setup The Python aspect of the app will contain 2 major components, a pyproject.toml and a unit test to get started.\n# pyproject.toml [project] name = \"spark-test\" version = \"0.1.0\" description = \"Spark unit tests\" dependencies = [ \"pytest\", \"pyspark==3.5.2\", ] And the unit tests:\n# tests/tests_pyspark.py import pytest from pyspark.sql import SparkSession from pyspark.sql import functions as sf from pyspark.sql import types as st @pytest.fixture(scope=\"session\") def spark(): spark_session = ( SparkSession.builder .master(\"local[*]\") .appName(\"pytest-pyspark-testing\") .getOrCreate() ) yield spark_session spark_session.stop() @pytest.fixture def sample_df(spark): data = [(\"A\", 10), (\"A\", 20), (\"B\", 5)] return spark.createDataFrame(data, [\"group\", \"value\"]) def test_spark_range_count(spark): assert spark.range(2).count() == 2 def test_group_and_sum(sample_df): result = ( sample_df .groupBy(\"group\") .agg(sf.sum(\"value\").alias(\"total\")) .where(\"group = 'A'\") .first() ) assert isinstance(result, st.Row) assert result.total == 30 Note that if iterating quickly and making many test changes, it may be beneficial to directly mount the tests/ directory as a volume so the image does not need to be rebuilt.\nBuild the container image The associated Makefile assumes that you are using podman instead of Docker, but can easily be swapped out if so desired by simply changing the CMD := podman to CMD := docker. Then run:\nmake build-image\nRun the test suite The tests can now be run with make test. The pytest command is invoked with verbose options that include runtime durations.\n❯ make test podman run --rm spark-test warning: No `requires-python` value found in the workspace. Defaulting to `\u003e=3.11`. ============================= test session starts ============================== platform linux -- Python 3.11.2, pytest-8.4.1, pluggy-1.6.0 -- /app/.venv/bin/python cachedir: .pytest_cache rootdir: /app configfile: pyproject.toml collecting ... collected 2 items tests/test_pyspark.py::test_spark_range_count PASSED [ 50%] tests/test_pyspark.py::test_group_and_sum PASSED [100%] ============================== slowest durations =============================== 6.17s call tests/test_pyspark.py::test_spark_range_count 4.89s setup tests/test_pyspark.py::test_spark_range_count 2.37s call tests/test_pyspark.py::test_group_and_sum 0.97s teardown tests/test_pyspark.py::test_group_and_sum 0.16s setup tests/test_pyspark.py::test_group_and_sum (1 durations \u003c 0.005s hidden. Use -vv to show these durations.) ============================== 2 passed in 15.02s ============================== Tips and Considerations Generally it’s good to keep test data small and in-memory for fast execution but with Spark there likely will be a need to test file-based sources Use pytest fixtures for re-use If the amount of fixtures become unwieldly, consider putting them in a conftest.py Try to keep tests self-contained as much as possible Provide descriptive names for each of the test functions References Makefile # Makefile .DEFAULT_GOAL := help SHELL := /bin/bash CMD := podman IMAGE_NAME := spark-test help: ## Show this help message @echo -e 'Usage: make [target] ...\\n' @echo 'targets:' @egrep '^(.+)\\:\\ ##\\ (.+)' ${MAKEFILE_LIST} | column -t -c 2 -s ':#' .PHONY: build-image build-image: ## build the container image $(CMD) build -t $(IMAGE_NAME) . .PHONY: setup setup: ## setup the basic project structure mkdir -p tests/ \u0026\u0026 touch pyproject.toml Containerfile tests/test_pyspark.py .PHONY: test test: ## run the unit tests $(CMD) run --rm $(IMAGE_NAME) ","wordCount":"757","inLanguage":"en","datePublished":"2025-07-12T12:58:15-05:00","dateModified":"2025-07-12T12:58:15-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/"},"publisher":{"@type":"Organization","name":"Brock B's Blog","logo":{"@type":"ImageObject","url":"https://be-rock.github.io/blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://be-rock.github.io/blog/ accesskey=h title="Brock B's Blog (Alt + H)">Brock B's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://be-rock.github.io/blog/ title=Home><span>Home</span></a></li><li><a href=https://be-rock.github.io/blog/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://be-rock.github.io/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://be-rock.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Using a Container to run PySpark Unit Tests</h1><div class=post-meta><span title='2025-07-12 12:58:15 -0500 -0500'>July 12, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;757 words</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#why-just-why aria-label="Why, just why?">Why, just why?</a></li><li><a href=#setup-a-basic-project-structure aria-label="Setup a basic project structure">Setup a basic project structure</a><ul><li><a href=#the-container-image-specification aria-label="The container image specification">The container image specification</a></li><li><a href=#the-python-project-setup aria-label="The Python project setup">The Python project setup</a></li><li><a href=#build-the-container-image aria-label="Build the container image">Build the container image</a></li></ul></li><li><a href=#run-the-test-suite aria-label="Run the test suite">Run the test suite</a></li><li><a href=#tips-and-considerations aria-label="Tips and Considerations">Tips and Considerations</a></li><li><a href=#references aria-label=References>References</a><ul><li><a href=#makefile aria-label=Makefile>Makefile</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><ul><li>Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code</li><li>This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding</li></ul><h2 id=why-just-why>Why, just why?<a hidden class=anchor aria-hidden=true href=#why-just-why>#</a></h2><p>Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.</p><p>Rather than running these tests on-demand, or add these examples in a notebook, I thought that it would be useful to build up a little library of these commands that can be run using <code>pytest</code> where performance can be evaluated and could also be used as a reference/cheatsheet.</p><p>Does this library serve any <em>real</em> practical purpose other than learning and understanding? No, not really. But maybe this can be a spring-board for something else</p><h2 id=setup-a-basic-project-structure>Setup a basic project structure<a hidden class=anchor aria-hidden=true href=#setup-a-basic-project-structure>#</a></h2><p>I find it helpful to use a <code>Makefile</code> to produce a consistent and a well-documented and repeatable build process. The <code>Makefile</code> is in <a href=#makefile>References/Makefile</a>. The basic project structure when completed will be:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>❯ tree
</span></span><span style=display:flex><span>.
</span></span><span style=display:flex><span>├── Containerfile
</span></span><span style=display:flex><span>├── Makefile
</span></span><span style=display:flex><span>├── pyproject.toml
</span></span><span style=display:flex><span>└── tests
</span></span><span style=display:flex><span>    └── test_pyspark.py
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span> directory, <span style=color:#ae81ff>4</span> files
</span></span></code></pre></div><h3 id=the-container-image-specification>The container image specification<a hidden class=anchor aria-hidden=true href=#the-container-image-specification>#</a></h3><p>Following is the <code>Containerfile</code> (a <code>Dockerfile</code> would also suffice). The specifications of the container image are:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#75715e># Containerfile</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> ghcr.io/astral-sh/uv:debian</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># Java is required for PySpark</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> apt-get update <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    apt-get install -y openjdk-17-jdk <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    apt-get clean <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rm -rf /var/lib/apt/lists/*<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>WORKDIR</span><span style=color:#e6db74> /app</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> pyproject.toml /app/pyproject.toml<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> uv sync<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>ENV</span> PYSPARK_PYTHON<span style=color:#f92672>=</span>/usr/bin/python3<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> tests/ /app/tests/<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>CMD</span> [<span style=color:#e6db74>&#34;uv&#34;</span>, <span style=color:#e6db74>&#34;run&#34;</span>, <span style=color:#e6db74>&#34;.venv/bin/pytest&#34;</span>, <span style=color:#e6db74>&#34;--durations=0&#34;</span>, <span style=color:#e6db74>&#34;-v&#34;</span>, <span style=color:#e6db74>&#34;/app/tests/&#34;</span>]<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><h3 id=the-python-project-setup>The Python project setup<a hidden class=anchor aria-hidden=true href=#the-python-project-setup>#</a></h3><p>The Python aspect of the app will contain 2 major components, a <code>pyproject.toml</code> and a unit test to get started.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span><span style=color:#75715e># pyproject.toml</span>
</span></span><span style=display:flex><span>[<span style=color:#a6e22e>project</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>name</span> = <span style=color:#e6db74>&#34;spark-test&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>version</span> = <span style=color:#e6db74>&#34;0.1.0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>description</span> = <span style=color:#e6db74>&#34;Spark unit tests&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>dependencies</span> = [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;pytest&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;pyspark==3.5.2&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>And the unit tests:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># tests/tests_pyspark.py</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pytest
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> SparkSession
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> functions <span style=color:#66d9ef>as</span> sf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> types <span style=color:#66d9ef>as</span> st
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@pytest.fixture</span>(scope<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;session&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>spark</span>():
</span></span><span style=display:flex><span>    spark_session <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        SparkSession<span style=color:#f92672>.</span>builder
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>master(<span style=color:#e6db74>&#34;local[*]&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>appName(<span style=color:#e6db74>&#34;pytest-pyspark-testing&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>getOrCreate()
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> spark_session
</span></span><span style=display:flex><span>    spark_session<span style=color:#f92672>.</span>stop()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@pytest.fixture</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sample_df</span>(spark):
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> [(<span style=color:#e6db74>&#34;A&#34;</span>, <span style=color:#ae81ff>10</span>), (<span style=color:#e6db74>&#34;A&#34;</span>, <span style=color:#ae81ff>20</span>), (<span style=color:#e6db74>&#34;B&#34;</span>, <span style=color:#ae81ff>5</span>)]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> spark<span style=color:#f92672>.</span>createDataFrame(data, [<span style=color:#e6db74>&#34;group&#34;</span>, <span style=color:#e6db74>&#34;value&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_spark_range_count</span>(spark):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> spark<span style=color:#f92672>.</span>range(<span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>count() <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_group_and_sum</span>(sample_df):
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        sample_df
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#34;group&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>agg(sf<span style=color:#f92672>.</span>sum(<span style=color:#e6db74>&#34;value&#34;</span>)<span style=color:#f92672>.</span>alias(<span style=color:#e6db74>&#34;total&#34;</span>))
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>where(<span style=color:#e6db74>&#34;group = &#39;A&#39;&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>first()
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> isinstance(result, st<span style=color:#f92672>.</span>Row)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> result<span style=color:#f92672>.</span>total <span style=color:#f92672>==</span> <span style=color:#ae81ff>30</span>
</span></span></code></pre></div><p>Note that if iterating quickly and making many test changes, it may be beneficial to directly mount the <code>tests/</code> directory as a volume so the image does not need to be rebuilt.</p><h3 id=build-the-container-image>Build the container image<a hidden class=anchor aria-hidden=true href=#build-the-container-image>#</a></h3><p>The associated <code>Makefile</code> assumes that you are using <a href=https://podman.io/>podman</a> instead of Docker, but can easily be swapped out if so desired by simply changing the <code>CMD := podman</code> to <code>CMD := docker</code>. Then run:</p><p><code>make build-image</code></p><h2 id=run-the-test-suite>Run the test suite<a hidden class=anchor aria-hidden=true href=#run-the-test-suite>#</a></h2><p>The tests can now be run with <code>make test</code>. The <code>pytest</code> command is invoked with verbose options that include runtime durations.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>❯ make test
</span></span><span style=display:flex><span>podman run --rm spark-test
</span></span><span style=display:flex><span>warning: No <span style=color:#e6db74>`</span>requires-python<span style=color:#e6db74>`</span> value found in the workspace. Defaulting to <span style=color:#e6db74>`</span>&gt;<span style=color:#f92672>=</span>3.11<span style=color:#e6db74>`</span>.
</span></span><span style=display:flex><span><span style=color:#f92672>=============================</span> test session starts <span style=color:#f92672>==============================</span>
</span></span><span style=display:flex><span>platform linux -- Python 3.11.2, pytest-8.4.1, pluggy-1.6.0 -- /app/.venv/bin/python
</span></span><span style=display:flex><span>cachedir: .pytest_cache
</span></span><span style=display:flex><span>rootdir: /app
</span></span><span style=display:flex><span>configfile: pyproject.toml
</span></span><span style=display:flex><span>collecting ... collected <span style=color:#ae81ff>2</span> items
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tests/test_pyspark.py::test_spark_range_count PASSED                     <span style=color:#f92672>[</span> 50%<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>tests/test_pyspark.py::test_group_and_sum PASSED                         <span style=color:#f92672>[</span>100%<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>==============================</span> slowest durations <span style=color:#f92672>===============================</span>
</span></span><span style=display:flex><span>6.17s call     tests/test_pyspark.py::test_spark_range_count
</span></span><span style=display:flex><span>4.89s setup    tests/test_pyspark.py::test_spark_range_count
</span></span><span style=display:flex><span>2.37s call     tests/test_pyspark.py::test_group_and_sum
</span></span><span style=display:flex><span>0.97s teardown tests/test_pyspark.py::test_group_and_sum
</span></span><span style=display:flex><span>0.16s setup    tests/test_pyspark.py::test_group_and_sum
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span><span style=color:#ae81ff>1</span> durations &lt; 0.005s hidden.  Use -vv to show these durations.<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>==============================</span> <span style=color:#ae81ff>2</span> passed in 15.02s <span style=color:#f92672>==============================</span>
</span></span></code></pre></div><h2 id=tips-and-considerations>Tips and Considerations<a hidden class=anchor aria-hidden=true href=#tips-and-considerations>#</a></h2><ul><li>Generally it&rsquo;s good to keep test data small and in-memory for fast execution but with Spark there likely will be a need to test file-based sources</li><li>Use <code>pytest</code> fixtures for re-use<ul><li>If the amount of fixtures become unwieldly, consider putting them in a <code>conftest.py</code></li></ul></li><li>Try to keep tests self-contained as much as possible</li><li>Provide descriptive names for each of the test functions</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><h3 id=makefile>Makefile<a hidden class=anchor aria-hidden=true href=#makefile>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Makefile data-lang=Makefile><span style=display:flex><span><span style=color:#75715e># Makefile
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>.DEFAULT_GOAL <span style=color:#f92672>:=</span> help
</span></span><span style=display:flex><span>SHELL <span style=color:#f92672>:=</span> /bin/bash
</span></span><span style=display:flex><span>CMD <span style=color:#f92672>:=</span> podman
</span></span><span style=display:flex><span>IMAGE_NAME <span style=color:#f92672>:=</span> spark-test
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>help</span><span style=color:#f92672>:</span> <span style=color:#75715e>## Show this help message
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	@echo -e <span style=color:#e6db74>&#39;Usage: make [target] ...\n&#39;</span>
</span></span><span style=display:flex><span>	@echo <span style=color:#e6db74>&#39;targets:&#39;</span>
</span></span><span style=display:flex><span>	@egrep <span style=color:#e6db74>&#39;^(.+)\:\ ##\ (.+)&#39;</span> <span style=color:#e6db74>${</span>MAKEFILE_LIST<span style=color:#e6db74>}</span> | column -t -c <span style=color:#ae81ff>2</span> -s <span style=color:#e6db74>&#39;:#&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>.PHONY</span><span style=color:#f92672>:</span> build-image
</span></span><span style=display:flex><span><span style=color:#a6e22e>build-image</span><span style=color:#f92672>:</span> <span style=color:#75715e>## build the container image
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#66d9ef>$(</span>CMD<span style=color:#66d9ef>)</span> build -t <span style=color:#66d9ef>$(</span>IMAGE_NAME<span style=color:#66d9ef>)</span> .
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>.PHONY</span><span style=color:#f92672>:</span> setup
</span></span><span style=display:flex><span><span style=color:#a6e22e>setup</span><span style=color:#f92672>:</span> <span style=color:#75715e>## setup the basic project structure
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	mkdir -p tests/ <span style=color:#f92672>&amp;&amp;</span> touch pyproject.toml Containerfile tests/test_pyspark.py
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>.PHONY</span><span style=color:#f92672>:</span> test
</span></span><span style=display:flex><span><span style=color:#a6e22e>test</span><span style=color:#f92672>:</span> <span style=color:#75715e>## run the unit tests
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#66d9ef>$(</span>CMD<span style=color:#66d9ef>)</span> run --rm <span style=color:#66d9ef>$(</span>IMAGE_NAME<span style=color:#66d9ef>)</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://be-rock.github.io/blog/tags/containers/>Containers</a></li><li><a href=https://be-rock.github.io/blog/tags/spark/>Spark</a></li><li><a href=https://be-rock.github.io/blog/tags/testing/>Testing</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://be-rock.github.io/blog/>Brock B's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>