<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Change Data Capture via Spark Streaming and Delta Lake CDF | Brock's Blog on Data|DevOps|Cloud</title><meta name=keywords content="cdc (change data capture),databricks,delta lake,spark,streaming"><meta name=description content="Summary
Delta Lake provides support for CDC (Change Data Capture) through its internal Change Data Feed (CDF).
The docs describe this feature as:

Change Data Feed (CDF) feature allows Delta tables to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records “change events” for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated."><meta name=author content><link rel=canonical href=https://be-rock.github.io/blog/posts/spark-streaming-delta-lake-cdf/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://be-rock.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://be-rock.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://be-rock.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://be-rock.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://be-rock.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://be-rock.github.io/blog/posts/spark-streaming-delta-lake-cdf/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://be-rock.github.io/blog/posts/spark-streaming-delta-lake-cdf/"><meta property="og:site_name" content="Brock's Blog on Data|DevOps|Cloud"><meta property="og:title" content="Change Data Capture via Spark Streaming and Delta Lake CDF"><meta property="og:description" content="Summary Delta Lake provides support for CDC (Change Data Capture) through its internal Change Data Feed (CDF).
The docs describe this feature as:
Change Data Feed (CDF) feature allows Delta tables to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records “change events” for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-27T10:55:10-05:00"><meta property="article:modified_time" content="2025-10-27T10:55:10-05:00"><meta property="article:tag" content="Cdc (Change Data Capture)"><meta property="article:tag" content="Databricks"><meta property="article:tag" content="Delta Lake"><meta property="article:tag" content="Spark"><meta property="article:tag" content="Streaming"><meta name=twitter:card content="summary"><meta name=twitter:title content="Change Data Capture via Spark Streaming and Delta Lake CDF"><meta name=twitter:description content="Summary
Delta Lake provides support for CDC (Change Data Capture) through its internal Change Data Feed (CDF).
The docs describe this feature as:

Change Data Feed (CDF) feature allows Delta tables to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records “change events” for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://be-rock.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Change Data Capture via Spark Streaming and Delta Lake CDF","item":"https://be-rock.github.io/blog/posts/spark-streaming-delta-lake-cdf/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Change Data Capture via Spark Streaming and Delta Lake CDF","name":"Change Data Capture via Spark Streaming and Delta Lake CDF","description":"Summary Delta Lake provides support for CDC (Change Data Capture) through its internal Change Data Feed (CDF).\nThe docs describe this feature as:\nChange Data Feed (CDF) feature allows Delta tables to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records “change events” for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\n","keywords":["cdc (change data capture)","databricks","delta lake","spark","streaming"],"articleBody":"Summary Delta Lake provides support for CDC (Change Data Capture) through its internal Change Data Feed (CDF).\nThe docs describe this feature as:\nChange Data Feed (CDF) feature allows Delta tables to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records “change events” for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\nDelta Lake provides both a batch and streaming interface to the CDF. Below we review the usage of Streaming CDF with a foreachBatch stream sink where the provided function takes the given CDF insert, update, delete record and performs a merge into the target table.\nThe Delta Lake Protocol describes the CDF like this with respect to how readers are writers should handle change data files:\nChange data files are stored in a directory at the root of the table named _change_data, and represent the changes for the table version they are in. For data with partition values, it is recommended that the change data files are stored within the _change_data directory in their respective partitions (i.e. _change_data/part1=value1/...). Writers can optionally produce these change data files as a consequence of operations that change underlying data, like UPDATE, DELETE, and MERGE operations to a Delta Lake table. If an operation only adds new data or removes existing data without updating any existing rows, a writer can write only data files and commit them in add or remove actions without duplicating the data into change data files.\nA Streaming CDF Example - Setup Create Source and Target Delta tables BASE_SCHEMA = \"\"\" c1 long, c2 string\"\"\" CDF_SCHEMA = \"\"\" _record_commit_version long, _record_commit_timestamp timestamp, _record_change_type string\"\"\" # the CDF is only needed on the source table from which we stream from spark.sql(f\"\"\" CREATE OR REPLACE TABLE src_table ( {BASE_SCHEMA} ) USING DELTA TBLPROPERTIES ( delta.enableChangeDataFeed = true ) \"\"\") spark.sql(f\"\"\" CREATE OR REPLACE TABLE tgt_table ( {BASE_SCHEMA}, {CDF_SCHEMA} ) USING DELTA \"\"\") Setup the stream from pyspark.sql import functions as f from pyspark.sql import SparkSession from pyspark.sql.dataframe import DataFrame from pyspark.sql.streaming.readwriter import DataStreamReader, DataStreamWriter from pyspark.sql.streaming.query import StreamingQuery NUM = 1 VOLUME_PATH = \"\" CHECKPOINT_LOCATION = f\"{VOLUME_PATH}/checkpoint/{NUM}\" QUERY_NAME = f\"my-query-{NUM}\" readstream_options = { \"readChangeFeed\": \"true\", } writestream_options = { \"checkpointLocation\": CHECKPOINT_LOCATION, \"mergeSchema\": \"true\", } trigger_options = { \"availableNow\": True } readstream_df: DataFrame = ( spark.readStream .format(\"delta\") .options(**readstream_options) .table(\"src_table\") .filter(f.col(\"_change_type\").isin([\"update_postimage\", \"insert\", \"delete\"])) .withColumn(\"_record_change_type\", f.expr(\"upper(substr(_change_type, 1, 1))\")) .withColumnRenamed(\"_commit_version\", \"_record_commit_version\") .withColumnRenamed(\"_commit_timestamp\", \"_record_commit_timestamp\") ) def process_batch(df, batch_id): \"\"\"process data in each microbatch\"\"\" df.createOrReplaceTempView(\"updates\") df.sparkSession.sql(\"\"\" MERGE INTO tgt_table AS target USING updates AS source ON source.id = target.id WHEN MATCHED AND source._record_change_type = 'U' THEN UPDATE SET * WHEN MATCHED AND source._record_change_type = 'D' THEN DELETE WHEN NOT MATCHED THEN INSERT * \"\"\" ) datastream_writer: DataStreamWriter = ( readstream_df .writeStream .queryName(QUERY_NAME) .foreachBatch(process_batch) .option(\"checkpointLocation\", CHECKPOINT_LOCATION) .trigger(**trigger_options) ) Load test data and start the stream Insert 5 rows:\ninsert into src_table select id, 'test' from range(5) …and start the streaming query:\nstreaming_query: StreamingQuery = datastream_writer.start() Reviewing the results shows, as expected, 5 records inserted into the target.\nUpdates and Deletes Update one row:\nupdate tgt_table set c2 = 'test2' where `id` \u003e 3 …and start the streaming query again:\nstreaming_query: StreamingQuery = datastream_writer.start() …and the query results show the updated value of test2:\nDeleting one row from the source table:\ndelete from src_table where c2 = 'test2' …and start the streaming query again:\nstreaming_query: StreamingQuery = datastream_writer.start() …and we can see that the row with id = 4 is now deleted.\nThis behavior is pretty intuitive based on the merge logic on our foreachBatch function.\nWhat about SCD Type 2 Dimensions? Now, a next reasonable thought is to take this approach and use it to maintain an SCD Type 2 table in a star schema. This is possible, and can be done, but it can get a bit messy and there are edge cases such as with late-arriving data.\nIf using Databricks, it would likely be worth taking a closer look at Auto CDC API with Declarative Pipelines which can help simplify things by addressing the edge-cases described above without needing to add complexity to your code.\nHow does Delta CDF work? Reads happen via the CDCReader which looks for the change data in a CDC_LOCATION path which defaults to _change_data as also described in the Protocol docs referenced above.\nWhen a user invokes a CDF read such as with the table_changes() table-valued SQL function, or with Spark’s readChangeFeed option, the logic scans the transaction logs in _delta_log for the specified versions, interprets the add and remove actions and reconstructs the rows that were inserted, updated, deleted as of the specified transaction number\nWrites happen based on the type of delta Command submitted. The CDF can produce 4 change events including update_preimage, update_postimage, insert, delete so there are 3 basic ’types’ of CDF events: insert, update, delete which correspond to:\nWriteIntoDelta UpdateCommand DeleteCommand Summary Delta Lake’s CDF is powerful and can be great when wanting to just propogate changes from an upstream Delta source to a dependent table(s). For advanced use cases such as SCD type 2 maintenance, Auto CDC can save some headaches.\nOne potential gotcha that users of the CDF data need to be aware of is that the _change_data is maintained in the same way as the table data. This means that vacuum operations will remove the CDF data at the same time as the table data.\n","wordCount":"913","inLanguage":"en","datePublished":"2025-10-27T10:55:10-05:00","dateModified":"2025-10-27T10:55:10-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://be-rock.github.io/blog/posts/spark-streaming-delta-lake-cdf/"},"publisher":{"@type":"Organization","name":"Brock's Blog on Data|DevOps|Cloud","logo":{"@type":"ImageObject","url":"https://be-rock.github.io/blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://be-rock.github.io/blog/ accesskey=h title="Brock's Blog on Data|DevOps|Cloud (Alt + H)">Brock's Blog on Data|DevOps|Cloud</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://be-rock.github.io/blog/ title=Home><span>Home</span></a></li><li><a href=https://be-rock.github.io/blog/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://be-rock.github.io/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://be-rock.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://be-rock.github.io/blog/conversations/ title=Conversations><span>Conversations</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Change Data Capture via Spark Streaming and Delta Lake CDF</h1><div class=post-meta><span title='2025-10-27 10:55:10 -0500 -0500'>October 27, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;913 words</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#a-streaming-cdf-example---setup aria-label="A Streaming CDF Example - Setup">A Streaming CDF Example - Setup</a><ul><li><a href=#create-source-and-target-delta-tables aria-label="Create Source and Target Delta tables">Create Source and Target Delta tables</a></li><li><a href=#setup-the-stream aria-label="Setup the stream">Setup the stream</a></li><li><a href=#load-test-data-and-start-the-stream aria-label="Load test data and start the stream">Load test data and start the stream</a></li><li><a href=#updates-and-deletes aria-label="Updates and Deletes">Updates and Deletes</a></li></ul></li><li><a href=#what-about-scd-type-2-dimensions aria-label="What about SCD Type 2 Dimensions?">What about SCD Type 2 Dimensions?</a></li><li><a href=#how-does-delta-cdf-work aria-label="How does Delta CDF work?">How does Delta CDF work?</a></li><li><a href=#summary-1 aria-label=Summary>Summary</a></li></ul></div></details></div><div class=post-content><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p><a href=https://delta.io/>Delta Lake</a> provides support for CDC (Change Data Capture) through its internal <a href=https://docs.delta.io/delta-change-data-feed/>Change Data Feed</a> (CDF).</p><p>The docs describe this feature as:</p><blockquote><p>Change Data Feed (CDF) feature allows Delta tables to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records “change events” for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.</p></blockquote><p>Delta Lake provides both a batch and streaming interface to the CDF. Below we review the usage of Streaming CDF with a <a href=https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.foreachBatch.html>foreachBatch</a> stream sink where the provided function takes the given CDF <code>insert</code>, <code>update</code>, <code>delete</code> record and performs a <code>merge</code> into the target table.</p><p><a href=https://github.com/delta-io/delta/blob/master/PROTOCOL.md#change-data-files>The Delta Lake Protocol</a> describes the CDF like this with respect to how readers are writers should handle change data files:</p><blockquote><p>Change data files are stored in a directory at the root of the table named <code>_change_data</code>, and represent the changes for the table version they are in. For data with partition values, it is recommended that the change data files are stored within the <code>_change_data</code> directory in their respective partitions (i.e. <code>_change_data/part1=value1/...</code>). Writers can optionally produce these change data files as a consequence of operations that change underlying data, like <code>UPDATE</code>, <code>DELETE</code>, and <code>MERGE</code> operations to a Delta Lake table. If an operation only adds new data or removes existing data without updating any existing rows, a writer can write only data files and commit them in add or remove actions without duplicating the data into change data files.</p></blockquote><h2 id=a-streaming-cdf-example---setup>A Streaming CDF Example - Setup<a hidden class=anchor aria-hidden=true href=#a-streaming-cdf-example---setup>#</a></h2><h3 id=create-source-and-target-delta-tables>Create Source and Target Delta tables<a hidden class=anchor aria-hidden=true href=#create-source-and-target-delta-tables>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>BASE_SCHEMA <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>c1 long,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>c2 string&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>CDF_SCHEMA <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>_record_commit_version long,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>_record_commit_timestamp timestamp,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>_record_change_type string&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># the CDF is only needed on the source table from which we stream from</span>
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>CREATE OR REPLACE TABLE src_table (
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    </span><span style=color:#e6db74>{</span>BASE_SCHEMA<span style=color:#e6db74>}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>USING DELTA
</span></span></span><span style=display:flex><span><span style=color:#e6db74>TBLPROPERTIES (
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    delta.enableChangeDataFeed = true
</span></span></span><span style=display:flex><span><span style=color:#e6db74>)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>CREATE OR REPLACE TABLE tgt_table (
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    </span><span style=color:#e6db74>{</span>BASE_SCHEMA<span style=color:#e6db74>}</span><span style=color:#e6db74>,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    </span><span style=color:#e6db74>{</span>CDF_SCHEMA<span style=color:#e6db74>}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>USING DELTA
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>)
</span></span></code></pre></div><h3 id=setup-the-stream>Setup the stream<a hidden class=anchor aria-hidden=true href=#setup-the-stream>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> functions <span style=color:#66d9ef>as</span> f
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> SparkSession
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.dataframe <span style=color:#f92672>import</span> DataFrame
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.streaming.readwriter <span style=color:#f92672>import</span> DataStreamReader, DataStreamWriter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.streaming.query <span style=color:#f92672>import</span> StreamingQuery
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NUM <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>VOLUME_PATH <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&lt;your S3 or UC volume location&gt;&#34;</span>
</span></span><span style=display:flex><span>CHECKPOINT_LOCATION <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>VOLUME_PATH<span style=color:#e6db74>}</span><span style=color:#e6db74>/checkpoint/</span><span style=color:#e6db74>{</span>NUM<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>QUERY_NAME <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;my-query-</span><span style=color:#e6db74>{</span>NUM<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>readstream_options <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;readChangeFeed&#34;</span>: <span style=color:#e6db74>&#34;true&#34;</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>writestream_options <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;checkpointLocation&#34;</span>: CHECKPOINT_LOCATION,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;mergeSchema&#34;</span>: <span style=color:#e6db74>&#34;true&#34;</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trigger_options <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;availableNow&#34;</span>: <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>readstream_df: DataFrame <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    spark<span style=color:#f92672>.</span>readStream
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;delta&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>options(<span style=color:#f92672>**</span>readstream_options)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>table(<span style=color:#e6db74>&#34;src_table&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>filter(f<span style=color:#f92672>.</span>col(<span style=color:#e6db74>&#34;_change_type&#34;</span>)<span style=color:#f92672>.</span>isin([<span style=color:#e6db74>&#34;update_postimage&#34;</span>, <span style=color:#e6db74>&#34;insert&#34;</span>, <span style=color:#e6db74>&#34;delete&#34;</span>]))
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>withColumn(<span style=color:#e6db74>&#34;_record_change_type&#34;</span>, f<span style=color:#f92672>.</span>expr(<span style=color:#e6db74>&#34;upper(substr(_change_type, 1, 1))&#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>withColumnRenamed(<span style=color:#e6db74>&#34;_commit_version&#34;</span>, <span style=color:#e6db74>&#34;_record_commit_version&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>withColumnRenamed(<span style=color:#e6db74>&#34;_commit_timestamp&#34;</span>, <span style=color:#e6db74>&#34;_record_commit_timestamp&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process_batch</span>(df, batch_id):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;process data in each microbatch&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    df<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;updates&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    df<span style=color:#f92672>.</span>sparkSession<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    MERGE INTO tgt_table AS target
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    USING updates AS source
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ON source.id = target.id
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    WHEN MATCHED AND source._record_change_type = &#39;U&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      THEN UPDATE SET *
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    WHEN MATCHED AND source._record_change_type = &#39;D&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      THEN DELETE
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    WHEN NOT MATCHED THEN INSERT *
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>datastream_writer: DataStreamWriter <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    readstream_df
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>writeStream
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>queryName(QUERY_NAME)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>foreachBatch(process_batch)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;checkpointLocation&#34;</span>, CHECKPOINT_LOCATION)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>trigger(<span style=color:#f92672>**</span>trigger_options)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=load-test-data-and-start-the-stream>Load test data and start the stream<a hidden class=anchor aria-hidden=true href=#load-test-data-and-start-the-stream>#</a></h3><p>Insert 5 rows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>insert</span> <span style=color:#66d9ef>into</span> src_table
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>select</span>
</span></span><span style=display:flex><span>    id,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;test&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>from</span>
</span></span><span style=display:flex><span>    range(<span style=color:#ae81ff>5</span>)
</span></span></code></pre></div><p>&mldr;and start the streaming query:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>streaming_query: StreamingQuery <span style=color:#f92672>=</span> datastream_writer<span style=color:#f92672>.</span>start()
</span></span></code></pre></div><p>Reviewing the results shows, as expected, 5 records inserted into the target.</p><p><img alt="Query results" loading=lazy src=../spark-streaming-delta-lake-cdf/query_results1.png></p><h3 id=updates-and-deletes>Updates and Deletes<a hidden class=anchor aria-hidden=true href=#updates-and-deletes>#</a></h3><p>Update one row:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>update</span>
</span></span><span style=display:flex><span>  tgt_table
</span></span><span style=display:flex><span><span style=color:#66d9ef>set</span>
</span></span><span style=display:flex><span>  c2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;test2&#39;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>where</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>`</span>id<span style=color:#f92672>`</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>3</span>
</span></span></code></pre></div><p>&mldr;and start the streaming query again:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>streaming_query: StreamingQuery <span style=color:#f92672>=</span> datastream_writer<span style=color:#f92672>.</span>start()
</span></span></code></pre></div><p>&mldr;and the query results show the updated value of <code>test2</code>:</p><p><img alt="Query results" loading=lazy src=../spark-streaming-delta-lake-cdf/query_results2.png></p><p>Deleting one row from the source table:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>delete</span> <span style=color:#66d9ef>from</span>
</span></span><span style=display:flex><span>  src_table
</span></span><span style=display:flex><span><span style=color:#66d9ef>where</span>
</span></span><span style=display:flex><span>  c2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;test2&#39;</span>
</span></span></code></pre></div><p>&mldr;and start the streaming query again:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>streaming_query: StreamingQuery <span style=color:#f92672>=</span> datastream_writer<span style=color:#f92672>.</span>start()
</span></span></code></pre></div><p>&mldr;and we can see that the row with <code>id = 4</code> is now deleted.</p><p><img alt="Query results" loading=lazy src=../spark-streaming-delta-lake-cdf/query_results3.png></p><p>This behavior is pretty intuitive based on the <code>merge</code> logic on our <code>foreachBatch</code> function.</p><h2 id=what-about-scd-type-2-dimensions>What about SCD Type 2 Dimensions?<a hidden class=anchor aria-hidden=true href=#what-about-scd-type-2-dimensions>#</a></h2><p>Now, a next reasonable thought is to take this approach and use it to maintain an SCD Type 2 table in a star schema. This is possible, and can be done, but it can get a bit messy and there are edge cases such as with late-arriving data.</p><p>If using Databricks, it would likely be worth taking a closer look at <a href=https://docs.databricks.com/aws/en/ldp/cdc>Auto CDC API</a> with Declarative Pipelines which can help simplify things by addressing the edge-cases described above without needing to add complexity to your code.</p><h2 id=how-does-delta-cdf-work>How does Delta CDF work?<a hidden class=anchor aria-hidden=true href=#how-does-delta-cdf-work>#</a></h2><p>Reads happen via the <a href=https://github.com/delta-io/delta/blob/b388f280d083d4cf92c6434e4f7a549fc26cd1fa/spark/src/main/scala/org/apache/spark/sql/delta/commands/cdc/CDCReader.scala><code>CDCReader</code></a> which looks for the change data in a <a href=https://github.com/delta-io/delta/blob/b388f280d083d4cf92c6434e4f7a549fc26cd1fa/spark/src/main/scala/org/apache/spark/sql/delta/commands/cdc/CDCReader.scala#L99><code>CDC_LOCATION</code> path</a> which defaults to <code>_change_data</code> as also described in the Protocol docs referenced above.</p><p>When a user invokes a CDF read such as with the <code>table_changes()</code> table-valued SQL function, or with Spark&rsquo;s <code>readChangeFeed</code> option, the logic scans the transaction logs in <code>_delta_log</code> for the specified versions, interprets the <code>add</code> and <code>remove</code> actions and reconstructs the rows that were inserted, updated, deleted as of the specified transaction number</p><p>Writes happen based on the type of delta Command submitted. The CDF can produce 4 change events including <code>update_preimage</code>, <code>update_postimage</code>, <code>insert</code>, <code>delete</code> so there are 3 basic &rsquo;types&rsquo; of CDF events: insert, update, delete which correspond to:</p><ul><li><a href=https://github.com/delta-io/delta/blob/b388f280d083d4cf92c6434e4f7a549fc26cd1fa/spark/src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala#L80><code>WriteIntoDelta</code></a></li><li><a href=https://github.com/delta-io/delta/blob/b388f280d083d4cf92c6434e4f7a549fc26cd1fa/spark/src/main/scala/org/apache/spark/sql/delta/commands/UpdateCommand.scala#L58><code>UpdateCommand</code></a></li><li><a href=https://github.com/delta-io/delta/blob/b388f280d083d4cf92c6434e4f7a549fc26cd1fa/spark/src/main/scala/org/apache/spark/sql/delta/commands/DeleteCommand.scala#L110><code>DeleteCommand</code></a></li></ul><h2 id=summary-1>Summary<a hidden class=anchor aria-hidden=true href=#summary-1>#</a></h2><p>Delta Lake&rsquo;s CDF is powerful and can be great when wanting to just propogate changes from an upstream Delta source to a dependent table(s). For advanced use cases such as SCD type 2 maintenance, Auto CDC can save some headaches.</p><p>One potential gotcha that users of the CDF data need to be aware of is that the <code>_change_data</code> is maintained in the same way as the table data. This means that <code>vacuum</code> operations will remove the CDF data at the same time as the table data.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://be-rock.github.io/blog/tags/cdc-change-data-capture/>Cdc (Change Data Capture)</a></li><li><a href=https://be-rock.github.io/blog/tags/databricks/>Databricks</a></li><li><a href=https://be-rock.github.io/blog/tags/delta-lake/>Delta Lake</a></li><li><a href=https://be-rock.github.io/blog/tags/spark/>Spark</a></li><li><a href=https://be-rock.github.io/blog/tags/streaming/>Streaming</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://be-rock.github.io/blog/>Brock's Blog on Data|DevOps|Cloud</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>