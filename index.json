[{"content":"","date":"2025-11-23","externalUrl":null,"permalink":"/blog/tags/databricks/","section":"Tags","summary":"","title":"Databricks","type":"tags"},{"content":"","date":"2025-11-23","externalUrl":null,"permalink":"/blog/tags/databricks-asset-bundles-dab/","section":"Tags","summary":"","title":"Databricks Asset Bundles (Dab)","type":"tags"},{"content":"This blog will provide an overview of how to use a DAB (Databricks Asset Bundle) template to provision a Declarative Pipeline.\nSetup # A DAB project can be established from a template to ensure consistencies among related projects. DAB templates follow a Go package template syntax, as described in the DAB Template Tutorial docs.\nIn this example, we\u0026rsquo;ll create a Pipeline project template with this structure\ntemplate_example ‚îú‚îÄ‚îÄ databricks_template_schema.json ‚îú‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ template ‚îú‚îÄ‚îÄ databricks.yml.tmpl ‚îú‚îÄ‚îÄ resources ‚îÇ ‚îî‚îÄ‚îÄ pipeline.yml.tmpl ‚îî‚îÄ‚îÄ src ‚îî‚îÄ‚îÄ pipeline.py.tmpl The databricks_template_schema.json file defines template values that are later interpolated into the other project files. For example, you could have a variable like catalog_name that is then interpolated into each template that references it such as: TARGET_CATALOG = \u0026quot;{{.catalog_name}}\u0026quot;.\n{ \u0026#34;properties\u0026#34;: { \u0026#34;project_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;dab_2025-11-23\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The name of the project folder and bundle\u0026#34;, \u0026#34;order\u0026#34;: 1 }, \u0026#34;pipeline_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;my_pipeline_2025-11-23\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The unique name of the DLT pipeline\u0026#34;, \u0026#34;order\u0026#34;: 2 }, \u0026#34;catalog_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;workspace\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Target Unity Catalog for data\u0026#34;, \u0026#34;order\u0026#34;: 3 } } } The template/ directory then contains 3 primary things:\nThe project-level databricks.yml One or more resource files in resources/ One or more source code files in src/ # databricks.yml.tmpl bundle: # User input: project_name name: {{.project_name}} include: - resources/*.yml - resources/*/*.yml # Variable declarations. These variables are assigned in the dev/prod targets below. variables: catalog: description: The catalog to use default: workspace schema: description: The schema to use default: ${workspace.current_user.short_name} targets: dev: mode: development default: true prod: mode: production variables: schema: prod # pipeline.yml.tmpl resources: pipelines: {{.pipeline_name}}: catalog: ${var.catalog} schema: ${var.schema} serverless: true root_path: \u0026#34;../src\u0026#34; libraries: - glob: include: ../src/** channel: PREVIEW # pipeline.py.tmpl from pyspark import pipelines as dp # We can inject the catalog name directly into the code TARGET_CATALOG = \u0026#34;{{.catalog_name}}\u0026#34; @dp.table( comment=\u0026#34;Raw data landing in \u0026#34; + TARGET_CATALOG ) def raw_users(): return spark.range(10) Using the template # Now if we want to use this defined template somewhere, we can reference the template when initializing a new DAB project using databricks bundle init /path/to/template/directory. For example:\n‚ùØ db bundle init /path/to/template/directory The name of the project folder and bundle [dab_2025-11-23]: The unique name of the DLT pipeline [my_pipeline_2025-11-23]: Target Unity Catalog for data [workspace]: ‚ú® Successfully initialized template We could then confirm that our bundle is valid via:\n‚ùØ db bundle validate Name: dab_2025-11-23 Target: dev Workspace: User: myusername@gmail.com Path: /Workspace/Users/myusername@gmail.com/.bundle/dab_2025-11-23/dev Validation OK! Deploying the bundle # And then to deploy the bundle to the Workspace:\n‚ùØ db bundle deploy --target dev Uploading bundle files to /Workspace/Users/myusername@gmail.com/.bundle/dab_2025-11-23/dev/files... Deploying resources... Updating deployment state... Deployment complete! And finally, to run the bundle:\n‚ùØ db bundle run Update URL: https://myworkspaceurl.cloud.databricks.com/#joblist/pipelines/d9d9c4c8-0949-43e1-8180-329c0c8e22c1/updates/68d9c643-f91a-4ca7-8c43-791516dfe78a 2025-11-25T03:48:31.238Z update_progress INFO \u0026#34;Update 68d9c6 is INITIALIZING.\u0026#34; 2025-11-25T03:48:34.072Z update_progress INFO \u0026#34;Update 68d9c6 is SETTING_UP_TABLES.\u0026#34; 2025-11-25T03:48:38.518Z update_progress INFO \u0026#34;Update 68d9c6 is RUNNING.\u0026#34; 2025-11-25T03:48:38.529Z flow_progress INFO \u0026#34;Flow \u0026#39;workspace.myusername.raw_users\u0026#39; is QUEUED.\u0026#34; 2025-11-25T03:48:38.566Z flow_progress INFO \u0026#34;Flow \u0026#39;workspace.myusername.raw_users\u0026#39; is PLANNING.\u0026#34; 2025-11-25T03:48:39.103Z flow_progress INFO \u0026#34;Flow \u0026#39;workspace.myusername.raw_users\u0026#39; is STARTING.\u0026#34; 2025-11-25T03:48:39.161Z flow_progress INFO \u0026#34;Flow \u0026#39;workspace.myusername.raw_users\u0026#39; is RUNNING.\u0026#34; 2025-11-25T03:48:43.764Z flow_progress INFO \u0026#34;Flow \u0026#39;workspace.myusername.raw_users\u0026#39; has COMPLETED.\u0026#34; 2025-11-25T03:48:43.885Z update_progress INFO \u0026#34;Update 68d9c6 is COMPLETED.\u0026#34; Update ID: 68d9c643-f91a-4ca7-8c43-791516dfe78a Summary # So with this approach we can quickly and easily make new pipelines that are structurally known and well-understood. I have found this really helpful for quickly testing and prototyping Pipelines.\nThe code for this example can be found here.\n","date":"2025-11-23","externalUrl":null,"permalink":"/blog/posts/databricks-declarative-pipelines-via-dab/","section":"Posts","summary":"\u003cp\u003eThis blog will provide an overview of how to use a DAB (Databricks Asset Bundle) template to provision a Declarative Pipeline.\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eSetup\n    \u003cdiv id=\"setup\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#setup\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eA DAB project can be established from a template to ensure consistencies among related projects. DAB templates follow a Go package template syntax, as described in the \u003ca\n  href=\"https://docs.databricks.com/aws/en/dev-tools/bundles/template-tutorial\"\n    target=\"_blank\"\n  \u003eDAB Template Tutorial docs\u003c/a\u003e.\u003c/p\u003e","title":"Databricks Declarative Pipelines via DAB","type":"posts"},{"content":"","date":"2025-11-23","externalUrl":null,"permalink":"/blog/tags/declarative-pipelines/","section":"Tags","summary":"","title":"Declarative Pipelines","type":"tags"},{"content":"","date":"2025-11-23","externalUrl":null,"permalink":"/blog/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"2025-11-23","externalUrl":null,"permalink":"/blog/tags/spark/","section":"Tags","summary":"","title":"Spark","type":"tags"},{"content":"","date":"2025-11-23","externalUrl":null,"permalink":"/blog/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2025-11-15","externalUrl":null,"permalink":"/blog/tags/useful/","section":"Tags","summary":"","title":"Useful","type":"tags"},{"content":"This page intends to be a catch-all of useful stuff that I gather over time.\n2025 # 2025-11 # Provide a custom message for each commit to a Delta table (#deltalake) import datetime import json # custom metadata json_data: str = json.dumps({ \u0026#34;name\u0026#34;: \u0026#34;my-app\u0026#34;, \u0026#34;event_timestamp\u0026#34;: datetime.datetime.utcnow().isoformat() }) # delta write ( spark.range(1) write.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;userMetadata\u0026#34;, json_data) .mode(\u0026#34;append\u0026#34;) .saveAsTable(\u0026#34;t1\u0026#34;) ) # read the userMetadata from the delta history with hist as ( desc history t1 ) select userMetadata from hist # {\u0026#34;name\u0026#34;: \u0026#34;my-app\u0026#34;, \u0026#34;event_timestamp\u0026#34;: \u0026#34;2025-11-09...\u0026#34;} Attach a sqlite database to the duckdb CLI. It\u0026rsquo;s a quick way to do analysis on a sqlite database using the duckdb engine for snappier query executions (#duckdb #sqlite) duckdb D INSTALL sqlite; D LOAD sqlite; D ATTACH \u0026#39;mydb.db\u0026#39; AS db (TYPE sqlite); D.databases D USE db; D.tables D select * from mytable; Google released Code Wiki which is described as: A new perspective on development for the agentic era. Gemini-generated documentation, always up-to-date. Ref: https://codewiki.google/\nHere are a few interesting repos to assess:\nhttps://codewiki.google/github.com/apache/spark https://codewiki.google/github.com/duckdb/duckdb https://codewiki.google/github.com/jlowin/fastmcp https://codewiki.google/github.com/mlflow/mlflow https://codewiki.google/github.com/run-llama/llama_index https://codewiki.google/github.com/neondatabase/neon ","date":"2025-11-15","externalUrl":null,"permalink":"/blog/posts/useful-stuff/","section":"Posts","summary":"Miscellaneous useful stuff that I pickup over time","title":"Useful Stuff","type":"posts"},{"content":"","date":"2025-11-01","externalUrl":null,"permalink":"/blog/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2025-11-01","externalUrl":null,"permalink":"/blog/tags/markdown/","section":"Tags","summary":"","title":"Markdown","type":"tags"},{"content":"","date":"2025-11-01","externalUrl":null,"permalink":"/blog/tags/prompt-engineering/","section":"Tags","summary":"","title":"Prompt Engineering","type":"tags"},{"content":" Summary # Markdown files have long been the standards for documenting projects, explaining intent, and usage of an app.\nWhen building apps with AI assistance like Cursor, Claude Code, OpenAI Codex, etc, Markdown and plaintext files can serve a different purpose.\nExamples of these include:\nllms.txt Cursor rules AGENTS.md (and CLAUDE.md) llms.txt # llms.txt has a well-documented specification at llmstxt.org/. It most often sits at the root of the site such as https://\u0026lt;site\u0026gt;/llms.txt and serves as an index for LLMs to reference (primarily) during inference.\nIt is further described in the specification as (italics are added by me for emphasis):\nrobots.txt and llms.txt have different purposes - robots.txt is generally used to let automated tools know what access to a site is considered acceptable, such as for search indexing bots. On the other hand, llms.txt information will often be used on demand when a user explicitly requests information about a topic, such as when including a coding library‚Äôs documentation in a project, or when asking a chat bot with search functionality for information. Our expectation is that llms.txt will mainly be useful for inference, i.e. at the time a user is seeking assistance, as opposed to for training. However, perhaps if llms.txt usage becomes widespread, future training runs could take advantage of the information in llms.txt files too. Source: https://llmstxt.org/#existing-standards\nThe llms.txt file is a basic markdown file and according to the spec, should like like this:\n# Title \u0026gt; Optional description goes here Optional details go here ## Section name - [Link title](https://link_url): Optional link details ## Optional - [Link title](https://link_url) Source: https://llmstxt.org/#example\nCursor rules # This convention is specific to the Cursor IDE and can live in varying paths in your project. Here are some examples, assuming that . (current directory) is your base project directory:\n./.cursor/rules/project-structure.mdc ./backend/api/.cursor/rules/endpoint-details.mdc You can specify when you want the rule to be applied in the front-matter of the .mdc file. Here\u0026rsquo;s an example of a file that would make sense to use at the base of a Python project:\n--- title: \u0026#34;Python style and typing\u0026#34; description: \u0026#34;Enforce typing, docstrings, and formatting for Python files.\u0026#34; globs: [\u0026#34;**/*.py\u0026#34;] alwaysApply: true --- - Use Python typing annotations for all public functions and methods (both return types and parameter annotations). - Add one-line docstrings for all public functions and modules; prefer Google style. - Use snake_case for functions and variables, PascalCase for classes. - Prefer pure functions and minimal side-effects. - Use f-strings for formatting. - Keep line length less than or equal to 88 characters and run `black --line-length 88`. - Do not add or remove external dependencies without updating pyproject.toml/requirements.txt. AGENTS.md # AGENTS.md is also supported by Cursor and can be nested through a project directory structure in a similar way as the .cursor/rules convention described above. Claude Code accepts this file but according to Claude Code: Best practices for agentic coding, the preferred naming convention is CLAUDE.md.\nThe contents of this file becomes part of the prompt and can further clarify intent. There are some great recommendations on how to use CLAUDE.md effectively in How I Use Every Claude Code Feature such as:\nYour CLAUDE.md should start small, documenting based on what Claude is getting wrong\nDon‚Äôt @-File Docs. If you have extensive documentation elsewhere, it‚Äôs tempting to @-mention those files in your CLAUDE.md. This bloats the context window by embedding the entire file on every run\nTakeaway # Plaintext and Markdown files are useful in AI tooling (like Cursor IDE) for documenting project structure, coding standards, and best practices. Conventions such as .cursor/rules and AGENTS.md (or CLAUDE.md) can be used to enforce each of these and improve communication for AI coding agents. Use small, focused documentation files and reference best practices to make the most of these features.\n","date":"2025-11-01","externalUrl":null,"permalink":"/blog/posts/2025-10_the-role-of-plaintext-and-markdown-files-in-ai/","section":"Posts","summary":"Using plaintext and Markdown Files in AI-driven development","title":"The Role of plaintext and Markdown Files in AI-driven development","type":"posts"},{"content":"","date":"2025-10-27","externalUrl":null,"permalink":"/blog/tags/cdc-change-data-capture/","section":"Tags","summary":"","title":"Cdc (Change Data Capture)","type":"tags"},{"content":" Summary # Delta Lake provides support for CDC (Change Data Capture) through its internal Change Data Feed (CDF).\nThe docs describe this feature as:\nChange Data Feed (CDF) feature allows Delta tables to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records ‚Äúchange events‚Äù for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\nDelta Lake provides both a batch and streaming interface to the CDF. Below we review the usage of Streaming CDF with a foreachBatch stream sink where the provided function takes the given CDF insert, update, delete record and performs a merge into the target table.\nThe Delta Lake Protocol describes the CDF like this with respect to how readers are writers should handle change data files:\nChange data files are stored in a directory at the root of the table named _change_data, and represent the changes for the table version they are in. For data with partition values, it is recommended that the change data files are stored within the _change_data directory in their respective partitions (i.e. _change_data/part1=value1/...). Writers can optionally produce these change data files as a consequence of operations that change underlying data, like UPDATE, DELETE, and MERGE operations to a Delta Lake table. If an operation only adds new data or removes existing data without updating any existing rows, a writer can write only data files and commit them in add or remove actions without duplicating the data into change data files.\nA Streaming CDF Example - Setup # Create Source and Target Delta tables # BASE_SCHEMA = \u0026#34;\u0026#34;\u0026#34; c1 long, c2 string\u0026#34;\u0026#34;\u0026#34; CDF_SCHEMA = \u0026#34;\u0026#34;\u0026#34; _record_commit_version long, _record_commit_timestamp timestamp, _record_change_type string\u0026#34;\u0026#34;\u0026#34; # the CDF is only needed on the source table from which we stream from spark.sql(f\u0026#34;\u0026#34;\u0026#34; CREATE OR REPLACE TABLE src_table ( {BASE_SCHEMA} ) USING DELTA TBLPROPERTIES ( delta.enableChangeDataFeed = true ) \u0026#34;\u0026#34;\u0026#34;) spark.sql(f\u0026#34;\u0026#34;\u0026#34; CREATE OR REPLACE TABLE tgt_table ( {BASE_SCHEMA}, {CDF_SCHEMA} ) USING DELTA \u0026#34;\u0026#34;\u0026#34;) Setup the stream # from pyspark.sql import functions as f from pyspark.sql import SparkSession from pyspark.sql.dataframe import DataFrame from pyspark.sql.streaming.readwriter import DataStreamReader, DataStreamWriter from pyspark.sql.streaming.query import StreamingQuery NUM = 1 VOLUME_PATH = \u0026#34;\u0026lt;your S3 or UC volume location\u0026gt;\u0026#34; CHECKPOINT_LOCATION = f\u0026#34;{VOLUME_PATH}/checkpoint/{NUM}\u0026#34; QUERY_NAME = f\u0026#34;my-query-{NUM}\u0026#34; readstream_options = { \u0026#34;readChangeFeed\u0026#34;: \u0026#34;true\u0026#34;, } writestream_options = { \u0026#34;checkpointLocation\u0026#34;: CHECKPOINT_LOCATION, \u0026#34;mergeSchema\u0026#34;: \u0026#34;true\u0026#34;, } trigger_options = { \u0026#34;availableNow\u0026#34;: True } readstream_df: DataFrame = ( spark.readStream .format(\u0026#34;delta\u0026#34;) .options(**readstream_options) .table(\u0026#34;src_table\u0026#34;) .filter(f.col(\u0026#34;_change_type\u0026#34;).isin([\u0026#34;update_postimage\u0026#34;, \u0026#34;insert\u0026#34;, \u0026#34;delete\u0026#34;])) .withColumn(\u0026#34;_record_change_type\u0026#34;, f.expr(\u0026#34;upper(substr(_change_type, 1, 1))\u0026#34;)) .withColumnRenamed(\u0026#34;_commit_version\u0026#34;, \u0026#34;_record_commit_version\u0026#34;) .withColumnRenamed(\u0026#34;_commit_timestamp\u0026#34;, \u0026#34;_record_commit_timestamp\u0026#34;) ) def process_batch(df, batch_id): \u0026#34;\u0026#34;\u0026#34;process data in each microbatch\u0026#34;\u0026#34;\u0026#34; df.createOrReplaceTempView(\u0026#34;updates\u0026#34;) df.sparkSession.sql(\u0026#34;\u0026#34;\u0026#34; MERGE INTO tgt_table AS target USING updates AS source ON source.id = target.id WHEN MATCHED AND source._record_change_type = \u0026#39;U\u0026#39; THEN UPDATE SET * WHEN MATCHED AND source._record_change_type = \u0026#39;D\u0026#39; THEN DELETE WHEN NOT MATCHED THEN INSERT * \u0026#34;\u0026#34;\u0026#34; ) datastream_writer: DataStreamWriter = ( readstream_df .writeStream .queryName(QUERY_NAME) .foreachBatch(process_batch) .option(\u0026#34;checkpointLocation\u0026#34;, CHECKPOINT_LOCATION) .trigger(**trigger_options) ) Load test data and start the stream # Insert 5 rows:\ninsert into src_table select id, \u0026#39;test\u0026#39; from range(5) \u0026hellip;and start the streaming query:\nstreaming_query: StreamingQuery = datastream_writer.start() Reviewing the results shows, as expected, 5 records inserted into the target.\nUpdates and Deletes # Update one row:\nupdate tgt_table set c2 = \u0026#39;test2\u0026#39; where `id` \u0026gt; 3 \u0026hellip;and start the streaming query again:\nstreaming_query: StreamingQuery = datastream_writer.start() \u0026hellip;and the query results show the updated value of test2:\nDeleting one row from the source table:\ndelete from src_table where c2 = \u0026#39;test2\u0026#39; \u0026hellip;and start the streaming query again:\nstreaming_query: StreamingQuery = datastream_writer.start() \u0026hellip;and we can see that the row with id = 4 is now deleted.\nThis behavior is pretty intuitive based on the merge logic on our foreachBatch function.\nWhat about SCD Type 2 Dimensions? # Now, a next reasonable thought is to take this approach and use it to maintain an SCD Type 2 table in a star schema. This is possible, and can be done, but it can get a bit messy and there are edge cases such as with late-arriving data.\nIf using Databricks, it would likely be worth taking a closer look at Auto CDC API with Declarative Pipelines which can help simplify things by addressing the edge-cases described above without needing to add complexity to your code.\nHow does Delta CDF work? # Reads happen via the CDCReader which looks for the change data in a CDC_LOCATION path which defaults to _change_data as also described in the Protocol docs referenced above.\nWhen a user invokes a CDF read such as with the table_changes() table-valued SQL function, or with Spark\u0026rsquo;s readChangeFeed option, the logic scans the transaction logs in _delta_log for the specified versions, interprets the add and remove actions and reconstructs the rows that were inserted, updated, deleted as of the specified transaction number\nWrites happen based on the type of delta Command submitted. The CDF can produce 4 change events including update_preimage, update_postimage, insert, delete so there are 3 basic \u0026rsquo;types\u0026rsquo; of CDF events: insert, update, delete which correspond to:\nWriteIntoDelta UpdateCommand DeleteCommand Summary # Delta Lake\u0026rsquo;s CDF is powerful and can be great when wanting to just propogate changes from an upstream Delta source to a dependent table(s). For advanced use cases such as SCD type 2 maintenance, Auto CDC can save some headaches.\nOne potential gotcha that users of the CDF data need to be aware of is that the _change_data is maintained in the same way as the table data. This means that vacuum operations will remove the CDF data at the same time as the table data.\n","date":"2025-10-27","externalUrl":null,"permalink":"/blog/posts/spark-streaming-delta-lake-cdf/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eSummary\n    \u003cdiv id=\"summary\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#summary\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003e\u003ca\n  href=\"https://delta.io/\"\n    target=\"_blank\"\n  \u003eDelta Lake\u003c/a\u003e provides support for CDC (Change Data Capture) through its internal \u003ca\n  href=\"https://docs.delta.io/delta-change-data-feed/\"\n    target=\"_blank\"\n  \u003eChange Data Feed\u003c/a\u003e (CDF).\u003c/p\u003e","title":"Change Data Capture via Spark Streaming and Delta Lake CDF","type":"posts"},{"content":"","date":"2025-10-27","externalUrl":null,"permalink":"/blog/tags/delta-lake/","section":"Tags","summary":"","title":"Delta Lake","type":"tags"},{"content":"","date":"2025-10-27","externalUrl":null,"permalink":"/blog/tags/streaming/","section":"Tags","summary":"","title":"Streaming","type":"tags"},{"content":"","date":"2025-10-13","externalUrl":null,"permalink":"/blog/tags/streaming-query-listener/","section":"Tags","summary":"","title":"Streaming Query Listener","type":"tags"},{"content":" Summary # This is an example of how to setup a Spark StreamingQueryListener that writes to some sample targets such as json and Postgres.\nSetup # Streaming Query # from pyspark.sql import DataFrame from pyspark.sql import functions as f from pyspark.sql.streaming import DataStreamWriter, StreamingQuery NUM = 1 SOURCE = \u0026#34;rate\u0026#34; TARGET = \u0026#34;noop\u0026#34; QUERY_NAME = f\u0026#34;{SOURCE}-{TARGET}-{NUM}\u0026#34; CHECKPOINT_LOCATION = f\u0026#34;file:/tmp/checkpoint/{QUERY_NAME}\u0026#34; readstream_options = { \u0026#34;rowsPerSecond\u0026#34;: \u0026#34;1\u0026#34;, } trigger_options = { \u0026#34;processingTime\u0026#34;: \u0026#34;10 seconds\u0026#34;, } writestream_options = { \u0026#34;checkpointLocation\u0026#34;: CHECKPOINT_LOCATION, } readstream_df: DataFrame = ( spark .readStream .format(SOURCE) .options(**readstream_options) .load() ) datastream_writer: DataStreamWriter = ( readstream_df .writeStream .trigger(**trigger_options) .format(TARGET) .options(**writestream_options) .queryName(QUERY_NAME) ) streaming_query: StreamingQuery = datastream_writer.start() Streaming Query Listener - json to stdout # import datetime import json from pyspark.sql.streaming.listener import ( QueryIdleEvent, QueryProgressEvent, QueryStartedEvent, QueryTerminatedEvent, StreamingQueryListener, ) class StreamingQueryListenerJson(StreamingQueryListener): def onQueryStarted(self, event: QueryStartedEvent) -\u0026gt; None: payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryStarted\u0026#34;, \u0026#34;id\u0026#34;: event.id, \u0026#34;name\u0026#34;: event.name, \u0026#34;runId\u0026#34;: event.runId, \u0026#34;message\u0026#34;: \u0026#34;query started\u0026#34; } print(payload) def onQueryProgress(self, event: QueryProgressEvent) -\u0026gt; None: json_data = json.loads(event.progress.json) payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryProgress\u0026#34;, \u0026#34;id\u0026#34;: json_data.get(\u0026#34;id\u0026#34;), \u0026#34;name\u0026#34;: json_data.get(\u0026#34;name\u0026#34;), \u0026#34;runId\u0026#34;: json_data.get(\u0026#34;runId\u0026#34;), \u0026#34;message\u0026#34;: json.dumps(json_data) } print(payload) def onQueryIdle(self, event: QueryIdleEvent) -\u0026gt; None: payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryIdle\u0026#34;, \u0026#34;id\u0026#34;: event.id, \u0026#34;name\u0026#34;: event.name, \u0026#34;runId\u0026#34;: event.runId, \u0026#34;message\u0026#34;: \u0026#34;query is idle\u0026#34; } print(payload) def onQueryTerminated(self, event: QueryTerminatedEvent) -\u0026gt; None: payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryIdle\u0026#34;, \u0026#34;id\u0026#34;: event.id, \u0026#34;name\u0026#34;: event.name, \u0026#34;runId\u0026#34;: event.runId, \u0026#34;message\u0026#34;: \u0026#34;query is terminated\u0026#34; } print(payload) spark.streams.addListener(StreamingQueryListenerJson()) Streaming Query Listener - json to a file # import datetime from pyspark.sql.streaming.listener import ( QueryIdleEvent, QueryProgressEvent, QueryStartedEvent, QueryTerminatedEvent, StreamingQueryListener, ) class StreamingQueryListenerJson(StreamingQueryListener): def __init__(self) -\u0026gt; None: self.f = open(\u0026#34;/tmp/f.json\u0026#34;, \u0026#34;w\u0026#34;) def onQueryStarted(self, event: QueryStartedEvent) -\u0026gt; None: payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryStarted\u0026#34;, \u0026#34;id\u0026#34;: event.id, \u0026#34;name\u0026#34;: event.name, \u0026#34;runId\u0026#34;: event.runId, \u0026#34;message\u0026#34;: \u0026#34;query started\u0026#34; } self.f.write(json.dumps(payload)) def onQueryProgress(self, event: QueryProgressEvent) -\u0026gt; None: json_data = json.loads(event.progress.json) payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryProgress\u0026#34;, \u0026#34;id\u0026#34;: json_data.get(\u0026#34;id\u0026#34;), \u0026#34;name\u0026#34;: json_data.get(\u0026#34;name\u0026#34;), \u0026#34;runId\u0026#34;: json_data.get(\u0026#34;runId\u0026#34;), \u0026#34;message\u0026#34;: json.dumps(json_data) } self.f.write(json.dumps(payload)) def onQueryIdle(self, event: QueryIdleEvent) -\u0026gt; None: payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryIdle\u0026#34;, \u0026#34;id\u0026#34;: event.id, \u0026#34;name\u0026#34;: event.name, \u0026#34;runId\u0026#34;: event.runId, \u0026#34;message\u0026#34;: \u0026#34;query is idle\u0026#34; } self.f.write(json.dumps(payload)) def onQueryTerminated(self, event: QueryTerminatedEvent) -\u0026gt; None: payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryIdle\u0026#34;, \u0026#34;id\u0026#34;: event.id, \u0026#34;name\u0026#34;: event.name, \u0026#34;runId\u0026#34;: event.runId, \u0026#34;message\u0026#34;: \u0026#34;query is terminated\u0026#34; } self.f.write(json.dumps(payload)) self.f.close() spark.streams.addListener(StreamingQueryListenerJson()) Streaming Query Listener - Sqlite # Sqlite is not multi-threaded so we can experience failures with concurrent writes. One way to prevent this issue from happening, is to append the StreamingQueryListener events to append to a Python queue.Queue() instead of directly to the database, so that all writes can be managed by a single thread.\nfrom concurrent.futures import ThreadPoolExecutor import datetime import json import queue import sqlite3 import time from pyspark.sql.streaming.listener import ( QueryIdleEvent, QueryProgressEvent, QueryStartedEvent, QueryTerminatedEvent, StreamingQueryListener, ) DB_PATH = \u0026#34;file:/tmp/f.db\u0026#34; class StreamingQueryListenerSqlite(StreamingQueryListener): def __init__(self, db_path: str = DB_PATH) -\u0026gt; None: self.queue = queue.Queue() self.db_path = db_path self._executor = ThreadPoolExecutor(max_workers=1) self._database_setup() self._insert_statement = \u0026#34;\u0026#34;\u0026#34; INSERT INTO data (time, event, id, name, runId, message) values (?, ?, ?, ?, ?, ?) \u0026#34;\u0026#34;\u0026#34; self._executor.submit(self._queue_worker) def _create_connection(self) -\u0026gt; sqlite3.Connection: return sqlite3.connect(self.db_path) def _create_cursor(self, connection: sqlite3.Connection) -\u0026gt; sqlite3.Cursor: return connection.cursor() def _database_setup(self) -\u0026gt; None: conn = self._create_connection() cur = self._create_cursor(connection=conn) cur.execute(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE IF NOT EXISTS data ( time TIMESTAMP NOT NULL, event TEXT NOT NULL, id TEXT NOT NULL, name TEXT NOT NULL, runId TEXT NOT NULL, message TEXT NOT NULL ) \u0026#34;\u0026#34;\u0026#34;) conn.commit() conn.close() def onQueryStarted(self, event: QueryStartedEvent) -\u0026gt; None: payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryStarted\u0026#34;, \u0026#34;id\u0026#34;: event.id, \u0026#34;name\u0026#34;: event.name, \u0026#34;runId\u0026#34;: event.runId, \u0026#34;message\u0026#34;: \u0026#34;query started\u0026#34;, } self.queue.put(json.dumps(payload)) def onQueryProgress(self, event: QueryProgressEvent) -\u0026gt; None: json_data = json.loads(event.progress.json) payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryProgress\u0026#34;, \u0026#34;id\u0026#34;: json_data.get(\u0026#34;id\u0026#34;), \u0026#34;name\u0026#34;: json_data.get(\u0026#34;name\u0026#34;), \u0026#34;runId\u0026#34;: json_data.get(\u0026#34;runId\u0026#34;), \u0026#34;message\u0026#34;: json.dumps(json_data), } self.queue.put(json.dumps(payload)) def onQueryIdle(self, event: QueryIdleEvent) -\u0026gt; None: payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryIdle\u0026#34;, \u0026#34;id\u0026#34;: event.id, \u0026#34;name\u0026#34;: event.name, \u0026#34;runId\u0026#34;: event.runId, \u0026#34;message\u0026#34;: \u0026#34;query is idle\u0026#34;, } self.queue.put(json.dumps(payload)) def onQueryTerminated(self, event: QueryTerminatedEvent) -\u0026gt; None: payload = { \u0026#34;time\u0026#34;: datetime.datetime.now().isoformat(), \u0026#34;event\u0026#34;: \u0026#34;onQueryIdle\u0026#34;, \u0026#34;id\u0026#34;: event.id, \u0026#34;name\u0026#34;: event.name, \u0026#34;runId\u0026#34;: event.runId, \u0026#34;message\u0026#34;: \u0026#34;query is terminated\u0026#34;, } self.queue.put(json.dumps(payload)) self._conn.close() def _queue_worker(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Background worker that processes items from the queue and inserts them into the database. Runs continuously in a separate thread, polling the queue every second for new items. When an item is found, calls _insert_item_to_db to persist it. The worker will continue running until the thread is terminated or the program exits. If the queue is empty, it will wait for 1 second before checking again. \u0026#34;\u0026#34;\u0026#34; self._conn = self._create_connection() self._cursor = self._create_cursor(connection=self._conn) while True: try: item: str = self.queue.get(timeout=1) self._insert_item_to_db( item=item, connection=self._conn, cursor=self._cursor ) except queue.Empty: continue def _insert_item_to_db( self, item: str, connection: sqlite3.Connection, cursor: sqlite3.Cursor ) -\u0026gt; None: print(f\u0026#34;inserting item {item} to db ...\u0026#34;) data = json.loads(item) cursor.execute( self._insert_statement, ( data.get(\u0026#34;time\u0026#34;), data.get(\u0026#34;event\u0026#34;), data.get(\u0026#34;id\u0026#34;), data.get(\u0026#34;name\u0026#34;), data.get(\u0026#34;runId\u0026#34;), data.get(\u0026#34;message\u0026#34;), ), ) connection.commit() spark.streams.addListener(StreamingQueryListenerSqlite()) Streaming Query Listener - other options # Other possible targets for the StreamingQueryListener events could include:\nan OLTP database like Postgres DuckDB (although similar concurrency issues as experienced with Sqlite may occur) OpenTelemetry AWS CloudWatch Datadog / Splunk / Prometheus ","date":"2025-10-13","externalUrl":null,"permalink":"/blog/posts/spark-streaming-streamingquerylistener/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eSummary\n    \u003cdiv id=\"summary\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#summary\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eThis is an example of how to setup a Spark  \u003ca\n  href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQueryListener.html\"\n    target=\"_blank\"\n  \u003e\u003ccode\u003eStreamingQueryListener\u003c/code\u003e\u003c/a\u003e that writes to some sample targets such as \u003ccode\u003ejson\u003c/code\u003e and \u003ccode\u003ePostgres\u003c/code\u003e.\u003c/p\u003e","title":"Using the StreamingQueryListener with Spark Streaming","type":"posts"},{"content":" Summary # This is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a foreachBatch sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like yaml, if so desired.\nA Streaming Query template # from pyspark.sql import DataFrame from pyspark.sql import functions as f from pyspark.sql.streaming import DataStreamWriter, StreamingQuery NUM = 1 SOURCE = \u0026#34;rate\u0026#34; # delta, kafka, rate TARGET = \u0026#34;noop\u0026#34; # delta, kafka, console, noop QUERY_NAME = f\u0026#34;{SOURCE}-{TARGET}-{NUM}\u0026#34; CHECKPOINT_LOCATION = f\u0026#34;file:/tmp/checkpoint/{QUERY_NAME}\u0026#34; readstream_options = { \u0026#34;rowsPerSecond\u0026#34;: \u0026#34;1\u0026#34;, # rate # \u0026#34;skipChangeCommits\u0026#34;: \u0026#34;true\u0026#34; # delta # \u0026#34;kafka.bootstrap.servers\u0026#34;: \u0026#34;localhost:9092\u0026#34;, # kafka # \u0026#34;subscribe\u0026#34;: \u0026#34;topic1\u0026#34;, # kafka # \u0026#34;startingOffsets\u0026#34;: \u0026#34;latest\u0026#34;, # kafka } trigger_options = { \u0026#34;processingTime\u0026#34;: \u0026#34;1 seconds\u0026#34;, # \u0026#34;availableNow\u0026#34;: \u0026#34;true\u0026#34;, } writestream_options = { \u0026#34;checkpointLocation\u0026#34;: CHECKPOINT_LOCATION, } def apply_transformations(df: DataFrame) -\u0026gt; DataFrame: return df.withColumn(\u0026#34;current_timestamp\u0026#34;, f.current_timestamp()) readstream_df: DataFrame = ( spark .readStream .format(SOURCE) .options(**readstream_options) .load() ) transformed_df: DataFrame = apply_transformations(df=readstream_df) datastream_writer: DataStreamWriter = ( transformed_df .writeStream .trigger(**trigger_options) .format(TARGET) .options(**writestream_options) .queryName(QUERY_NAME) ) streaming_query: StreamingQuery = datastream_writer.start() A summary of the above template # Dictionaries to manage: stream source options stream sink options trigger options A function to manage all transformations using DataFrame.transform prior to writing to the sink, that can be expanded on A snippet for the DataStreamWriter And finally, a call to .start() to return a StreamingQuery Note that this would need to change to toTable() if writing to a table sink such as delta or iceberg. Further templatizing this code # You may wnat to codify this further into a markup language like yaml and then make the configuration strongly-typed using pydantic. Here\u0026rsquo;s an example of what that might look like:\n# app-conf.yaml source: format: rate rowsPerSecond: 1 trigger: processingTime: \u0026#34;1 seconds\u0026#34; target: checkpointLocation: \u0026#34;/tmp/checkpoint\u0026#34; format: noop query_name: my-streaming-query from pydantic_settings import BaseSettings, YamlConfigSettingsSource class AppConfig(BaseSettings): source: dict trigger: dict target: dict query_name: str config: YamlConfigSettingsSource = YamlConfigSettingsSource( settings_cls=AppConfig, yaml_file=\u0026#34;app-conf.yaml\u0026#34;, ) config.yaml_data {\u0026#39;source\u0026#39;: {\u0026#39;format\u0026#39;: \u0026#39;rate\u0026#39;, \u0026#39;rowsPerSecond\u0026#39;: 1}, \u0026#39;trigger\u0026#39;: {\u0026#39;processingTime\u0026#39;: \u0026#39;1 seconds\u0026#39;}, \u0026#39;target\u0026#39;: {\u0026#39;checkpointLocation\u0026#39;: \u0026#39;/tmp/checkpoint\u0026#39;, \u0026#39;format\u0026#39;: \u0026#39;noop\u0026#39;}, \u0026#39;query_name\u0026#39;: \u0026#39;my-streaming-query\u0026#39;} app_config: AppConfig = AppConfig(**config.yaml_data) assert app_config.query_name == \u0026#34;my-streaming-query\u0026#34; ","date":"2025-10-12","externalUrl":null,"permalink":"/blog/posts/spark-streaming-templatized/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eSummary\n    \u003cdiv id=\"summary\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#summary\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eThis is a template / code snippet that can be useful to quickly setup a Spark Streaming Query with a source and sink. The template will not work in all circumstances, such as a \u003ccode\u003eforeachBatch\u003c/code\u003e sink but is enough to get you started in a quick way, with some examples below on how this could be templatized even more using a markup language like \u003ccode\u003eyaml\u003c/code\u003e, if so desired.\u003c/p\u003e","title":"Apache Spark Streaming - templatized","type":"posts"},{"content":"","date":"2025-10-12","externalUrl":null,"permalink":"/blog/tags/config/","section":"Tags","summary":"","title":"Config","type":"tags"},{"content":"","date":"2025-10-12","externalUrl":null,"permalink":"/blog/tags/pydantic/","section":"Tags","summary":"","title":"Pydantic","type":"tags"},{"content":" Introduction # The Python development ecosystem has changed pretty dramatically over the recent years.\nThis post provides a practical introduction to the toolkit that I\u0026rsquo;ve been using, explaining why I chose each tool and how they work together.\nThe Toolkit # Command Runner - make Configuration - pydantic Containerization - podman Continuous Integration - GitHub Actions IDE - cursor Linting \u0026amp; Formatting - ruff Load Testing - locust Logging - logging Pre-commit - prek Python project and environment management - uv Scheduling - cron Testing - pytest Type Checking - ty Command Runner - make # While not initially designed for this purpose, make works very well as a command runner. A Makefile can be created in your project root to represent common ways for your project to be used. It is also self-documenting and a helpful way for newcomers to get started with your project.\nThere are numerous tools that can serve the purpose of make and function as a command runner, such as just and task libraries but, although it may have a slightly more archaic syntax, I feel that make wins here because of its availability. It\u0026rsquo;s present on virtually all systems and LLMs have made it easy to get up and running if you need assistance with the syntax.\nHere\u0026rsquo;s a list of common make targets that I use on my Python projects:\n.PHONY: help setup format lint test run build-image clean help: ## Show this help message @echo \u0026#34;Available commands:\u0026#34; @grep -E \u0026#39;^[a-zA-Z_-]+:.*?## .*$$\u0026#39; $(MAKEFILE_LIST) | sort | awk \u0026#39;BEGIN {FS = \u0026#34;:.*?## \u0026#34;}; {printf \u0026#34;\\033[36m%-20s\\033[0m %s\\n\u0026#34;, $$1, $$2}\u0026#39; setup: ## Bootstrap the project and install dependencies uv init --no-readme uv add --dev pytest ruff ty uv sync format: ## Format code with ruff uvx ruff format src/ tests/ lint: ## Lint code with ruff uvx ruff check src/ tests/ test: ## Run tests with pytest uvx pytest tests/ -v run: ## Run the application uv run python -m src.main build-image: ## Build container image with podman podman build -t my-python-app . clean: ## Clean up generated files find . -type f -name \u0026#34;*.pyc\u0026#34; -delete find . -type d -name \u0026#34;__pycache__\u0026#34; -delete rm -rf .pytest_cache/ Common targets I use:\nmake help - Show available commands (self-documenting!) make setup - Bootstrap the project and install dependencies make format - Format code with ruff make test - Run tests make build-image - Build the container image Configuration - pydantic # pydantic provides type validation and settings management. I use it for configurations to ensure type safety / validation.\nfrom pydantic_settings import BaseSettings, SettingsConfigDict class Settings(BaseSettings): database_url: str = \u0026#34;app.db\u0026#34; api_host: str = \u0026#34;0.0.0.0\u0026#34; api_port: int = 8000 api_debug: bool = False model_config = SettingsConfigDict( env_file=\u0026#34;.env\u0026#34;, env_file_encoding=\u0026#34;utf-8\u0026#34;, case_sensitive=False ) # Usage settings = Settings() print(f\u0026#34;API will run on {settings.api_host}:{settings.api_port}\u0026#34;) The pydantic settings documentation has excellent examples for more complex configurations.\nContainerization - podman # I use podman for containerization since there are no licensing restrictions to be concerned about, unlike docker. For Mac development, the apple/container project might be worth considering.\nContinuous Integration - GitHub Actions # GitHub Actions integrates well into a GitHub repo and starters are widely available for Python projects. Unless your company has an alternate CI solution, GitHub Actions should be the starting-point.\nIt can also be run locally use the awesome OSS https://github.com/nektos/act project.\nIDE (Cursor or VS Code) # Cursor is a great IDE because of its familiar VS Code-like interface and AI integration. Its website puts it this way:\nBuilt to make you extraordinarily productive, Cursor is the best way to code with AI.\ncursor.com Linting \u0026amp; Formatting - ruff # ruff is a super fast Python linter and formatter that can used in place of other tools like black for formatting and isort for import sorting\nExample usage:\n# Format code uvx ruff format src/ # Lint code uvx ruff check src/ # Fix auto-fixable issues uvx ruff check --fix src/ Logging - logging # Although libraries like loguru are easier to use, have more bells-and-whistles, I feel that the standard library logging library has always been \u0026lsquo;good enough\u0026rsquo; for what I need.\nIn this case, we will create a custom Python logger in json format for easier downstream parsing, if required.\nimport datetime import logging import json from typing import Any class JsonFormatter(logging.Formatter): def format(self, record) -\u0026gt; str: log_record: dict[str, Any] = { \u0026#34;time\u0026#34;: datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\u0026#39;milliseconds\u0026#39;), \u0026#34;level\u0026#34;: record.levelname, \u0026#34;msg\u0026#34;: record.getMessage(), \u0026#34;logger\u0026#34;: record.name, } return json.dumps(log_record) def get_logger( logger_name: str = __name__, logger_level: int = logging.INFO, logging_formatter: JsonFormatter = JsonFormatter, ) -\u0026gt; logging.Logger: logger = logging.getLogger(logger_name) logger.setLevel(logger_level) handler = logging.StreamHandler() handler.setFormatter(logging_formatter()) logger.addHandler(handler) return logger logger = get_logger() logger.info(\u0026#34;This is an info message\u0026#34;) {\u0026#34;time\u0026#34;: \u0026#34;2025-10-01T04:30:41.691+00:00\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;This is an info message\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;__main__\u0026#34;} Load Testing - locust # locustio/locust is an excellent project for load testing with a built-in web UI. It shines when load testing APIs - the primary place I\u0026rsquo;ve used it, but it can be easily extensible to non-API use cases.\nPre-commit - prek # The pre-commit Python library has long been king in this area for, as the name suggests, ensuring certain conditions are met prior to a git commit succeeding. The prek project promises to be:\nüöÄ A single binary with no dependencies, does not require Python or any other runtime. ‚ö° About 10x faster than pre-commit and uses only half the disk space. üîÑ Fully compatible with the original pre-commit configurations and hooks.\nAs the name suggests, it is a git hook that is triggered prior to commits. It can be used to ensure that JSON, YAML files are formatted properly, check to ensure files end with a newline, check for merge conflicts, and more.\nHere\u0026rsquo;s a sample .pre-commit-config.yaml:\nrepos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v4.4.0 hooks: - id: trailing-whitespace - id: end-of-file-fixer - id: check-yaml - id: check-added-large-files - id: check-merge-conflict - repo: https://github.com/astral-sh/ruff-pre-commit rev: v0.1.6 hooks: - id: ruff args: [--fix, --exit-non-zero-on-fix] - id: ruff-format Install and setup:\n# Install prek curl -L https://github.com/j178/prek/releases/latest/download/prek_linux_amd64.tar.gz | tar -xz sudo mv prek /usr/local/bin/ # Install the hooks defined in the .pre-commit-config.yaml prek install Python project and environment management - `uv``` # uv makes managing dependencies easier and has other project-bootstrapping capabilities. Written in Rust, it\u0026rsquo;s project dependency resolution, as an alternative to pip install. is significantly faster.\nHere\u0026rsquo;s some common (AI-generated) examples:\n# Initialize a new project uv init my-project cd my-project # Add dependencies uv add fastapi uvicorn uv add --dev pytest ruff # Create and activate virtual environment uv venv source .venv/bin/activate # On Unix/macOS # Install dependencies uv sync # Run commands in the virtual environment uv run python main.py uv run pytest uv run ruff check . # Run tools without installing globally uvx ruff format . uvx ty check src/ Scheduling - cron # cron is the ultimate \u0026lsquo;keep it simple\u0026rsquo; scheduler and is widely available. When something more sophisticated is needed, Airflow may be the way to go, especially for data-centric jobs. But if \u0026lsquo;keep it simple\u0026rsquo; is good enough, here\u0026rsquo;s a crontab template file that I referenced for years.\n# .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * command to be executed # * * * * * command --arg1 --arg2 file1 file2 2\u0026gt;\u0026amp;1 Testing - pytest # pytest shouldn\u0026rsquo;t need much of an introduction but in my opinion it is both easier to read and more customizable than the built-in testing library.\nHere\u0026rsquo;s a simple (AI-Generated) example:\n# tests/test_user.py import pytest from src.models import User class TestUser: def test_user_creation(self): user = User(name=\u0026#34;John Doe\u0026#34;, email=\u0026#34;john@example.com\u0026#34;) assert user.name == \u0026#34;John Doe\u0026#34; assert user.email == \u0026#34;john@example.com\u0026#34; def test_user_email_validation(self): with pytest.raises(ValueError): User(name=\u0026#34;John Doe\u0026#34;, email=\u0026#34;invalid-email\u0026#34;) @pytest.fixture def sample_user(self): return User(name=\u0026#34;Jane Doe\u0026#34;, email=\u0026#34;jane@example.com\u0026#34;) def test_user_with_fixture(self, sample_user): assert sample_user.name == \u0026#34;Jane Doe\u0026#34; Type Checking - ty # ty is a newer project that promises to be faster than alternatives such as mypy and can be run easily when paired with uv.\nBasic usage:\nuvx ty check src/ Takeaways # While I\u0026rsquo;m not an expert in these tools, I have used most extensively, with the exception of new entries such as ty and the supposed backwards-compatible version of pre-commit, prek\n","date":"2025-09-29","externalUrl":null,"permalink":"/blog/posts/my-recent-python-toolkit/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eIntroduction\n    \u003cdiv id=\"introduction\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#introduction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eThe Python development ecosystem has changed pretty dramatically over the recent years.\u003c/p\u003e\n\u003cp\u003eThis post provides a practical introduction to the toolkit that I\u0026rsquo;ve been using, explaining why I chose each tool and how they work together.\u003c/p\u003e","title":"My Recent Python Toolkit","type":"posts"},{"content":"","date":"2025-09-29","externalUrl":null,"permalink":"/blog/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":" Summary # I enjoy learning new languages, and decided it would be fun to do some learning the \u0026lsquo;old-school\u0026rsquo; way, meaning no AI-assisted coding, trial-and-error, and using the docs. Note - LLMs will be used merely as a search-engine equivalent to aid with solutions and resolve issues, but not to build a solution. I have worked with numerous languages in the past, but 2 that have been on my radar are go and V. This post will be about V but I hope to do something similar for go. V promises to be stable (despite not yet having reached 1.0 release), easy to learn (\u0026ldquo;can be learned over the course of a weekend\u0026rdquo;), fast, and is statically typed.\nLearning V # Getting started # Installation # Installation is pretty straight-forward, to clone, build from source, and symlink the executable just took a few seconds:\ncd ~/.local/lib time git clone --depth=1 https://github.com/vlang/v cd v time make ... git clone --depth=1 https://github.com/vlang/v 0.60s user 0.77s system 18% cpu 7.314 total ... V has been successfully built V 0.4.11 603cd90 make 6.82s user 0.98s system 61% cpu 12.739 total sudo ./v symlink Creating a project # Creating things is one of the best ways to learn. I\u0026rsquo;ll go with the idea of a coin flipper app. It is a simple concept and can be used in a variety of ways to explore the language. I have previously done this with Python as a basis of advancing knowledge of TDD, and several Architectural patterns. A coin flipper is one of the suggestions made in this repo:\nCoin Flip Simulation - Write some code that simulates flipping a single coin however many times the user decides. The code should record the outcomes and count the number of tails and heads. Source: https://github.com/karan/Projects\nThe beauty of a simple concept like this is that it can be the basis for so many things such as adding a REST API, a front-end, a CLI, database interactions, \u0026hellip;\nStarting the project # There are numerous types of project templates bundled with the v CLI to get started with. The default is bin. v new --help shows 3 built-in:\n--bin Use the template for an executable application [default]. --lib Use the template for a library project. --web Use the template for a vweb project. I\u0026rsquo;ll start with the default (--lib) because it includes tests.\nv new --lib coin_flipper Input your project description: a coin flipper app Input your project version: (0.0.0) Input your project license: (MIT) Initialising ... Created library project `coin_flipper` tree . ‚îú‚îÄ‚îÄ coin_flipper ‚îÇ¬†‚îú‚îÄ‚îÄ coin_flipper.v ‚îÇ¬†‚îú‚îÄ‚îÄ tests ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ square_test.v ‚îÇ¬†‚îî‚îÄ‚îÄ v.mod ‚îî‚îÄ‚îÄ Makefile 3 directories, 4 files The contents of these files are:\n// coin_flipper/coin_flipper.v module coin_flipper // square calculates the second power of `x` pub fn square(x int) int { return x * x } // coin_flipper/tests/square_test.v import coin_flipper fn test_square() { assert coin_flipper.square(2) == 4 } // v.mod Module { name: \u0026#39;coin_flipper\u0026#39; description: \u0026#39;a coin flipper app\u0026#39; version: \u0026#39;0.0.0\u0026#39; license: \u0026#39;MIT\u0026#39; dependencies: [] } To run the project:\nv run coin_flipper coin_flipper/coin_flipper.v:1:1: error: project must include a `main` module or be a shared library (compile with `v -shared`) Strangely, it does not run. Adding a main.v file seems to resolve this:\n// coin_flipper/main.v import coin_flipper fn main() { println(coin_flipper.square(3)) } v run coin_flipper 9 Ok, we have a successful app that works, although it does not yet flip a coin, it performs some basic useful tasks that we can build upon.\nLearning the language # I have spent some time browsing through the docs and testing in the bundled REPL via v repl (or just v, which also defaults to the repl) but usually find it best just to dig in.\nThe first thing I\u0026rsquo;d like to do is build the coin flipping logic into our tests in a TDD sort of manner.\nIn Python, I would probably start with something like:\n# tests/test_app.py def test_app(): import random def coin_flipper() -\u0026gt; str: return random.choice([\u0026#39;heads\u0026#39;, \u0026#39;tails\u0026#39;]) assert coin_flipper() in [\u0026#39;heads\u0026#39;, \u0026#39;tails\u0026#39;] And then run the tests. Upon success, we would move the coin_flipper() to a new module and then import the module into the test_app.py like:\n# tests/test_app.py import coin_flipper def test_app(): assert coin_flipper() in [\u0026#39;heads\u0026#39;, \u0026#39;tails\u0026#39;] Eventually the coin_flipper() function could later be refactored into a CoinFlipper class with a flip_coin() method if needed.\nTo do the same thing in v, this became:\n// tests/coin_flipper_test.v fn flip_coin() !string { return rand.element([\u0026#39;heads\u0026#39;, \u0026#39;tails\u0026#39;])! } fn test_flip_coin() { result := flip_coin()! assert result in [\u0026#39;heads\u0026#39;, \u0026#39;tails\u0026#39;] } Running the tests with v -stats test tests/ succeeded. Through trial and error, I learned about the ! behavior in V and that simply returning type string from the function was not acceptable because rand.element() actually returns !string which, I understand to mean that it can return null or string.\nThe rand module docs say that rand.element\u0026rsquo;s signature is: fn element[T](array []T) !T. I interpret this to mean that element takes an array of any type T and returns type !T which means it can return type T or an error/none.\nSo our fn flip_coin() returns !string to accomodate this. Another way to address this, without the ! symbol would be to add the or {'none'}:\nfn flip_coin() string { return rand.element([\u0026#39;heads\u0026#39;, \u0026#39;tails\u0026#39;]) or {\u0026#39;none\u0026#39;} } fn test_flip_coin() { result := flip_coin() assert result in [\u0026#39;heads\u0026#39;, \u0026#39;tails\u0026#39;] } Once these tests succeed, we can further refactor the code such that the flip_coin function is moved to coin_flipper/coin_flipper.v and then imported to the tests/coin_flipper_test.v such as:\n// coin_flipper/coin_flipper.v module coin_flipper import rand pub fn flip_coin() string { return rand.element([\u0026#39;heads\u0026#39;, \u0026#39;tails\u0026#39;]) or {\u0026#39;none\u0026#39;} } // tests/coin_flipper_test.v import coin_flipper fn test_flip_coin() { result := coin_flipper.flip_coin() assert result in [\u0026#39;heads\u0026#39;, \u0026#39;tails\u0026#39;] } And then main.v simply:\nimport coin_flipper fn main() { println(coin_flipper.flip_coin()) } Running and Compiling the project # To run the project, you can v run .. To run and compile, change to v crun .. crun. In my case, the compiled library was coin-flipper-v.\nHere\u0026rsquo;s a quick example of runtime differences between v run in a shell for loop and an execution of the built binary which shows that the binary is, unsurprisingly, considerably faster.\n# ./coin-flipper-v /usr/bin/time -p zsh -c \u0026#39;for i in {1..1000}; do ./coin-flipper-v \u0026gt; /dev/null ; done\u0026#39; real 3.09 user 0.01 sys 0.13 # v run /usr/bin/time -p zsh -c \u0026#39;for i in {1..1000}; do v run . \u0026gt; /dev/null; done\u0026#39; real 427.76 user 0.23 sys 0.26 Expanding the app # Once I get a chance, I would like to enhance this app in a few ways such as:\nAdd a web component such as a REST API with veb Provide flexiblity to the app user to give a dynamic number of inputs Write the coin flip results to a SQL database https://modules.vlang.io/db.sqlite.ht This is just a toy app and each of the above components can be expanded upon.\nWrapping up # This was a super quick intro to V. It was a language that I had wanted to dive into and have some surface-level knowledge of. I really enjoyed navigating through the docs, reviewing the examples, prototyping in the REPL.\nI\u0026rsquo;ve gotten a better sense of the basic data structures, the project structure and packaging, error handling, and more.\nI felt that the language, thus far, has been intuitive. If a syntax or paradigm was new or unfamiliar, it made sense in a short amount of time with review.\nReference # The project code can be found in https://github.com/be-rock/coin-flipper-v\n","date":"2025-08-25","externalUrl":null,"permalink":"/blog/posts/evaluating-vlang/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eSummary\n    \u003cdiv id=\"summary\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#summary\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eI enjoy learning new languages, and decided it would be fun to do some learning the \u0026lsquo;old-school\u0026rsquo; way, meaning no AI-assisted coding, trial-and-error, and using the docs. Note - LLMs will be used merely as a search-engine equivalent to aid with solutions and resolve issues, but \u003cem\u003enot\u003c/em\u003e to build a solution.\u003c/li\u003e\n\u003cli\u003eI have worked with numerous languages in the past, but 2 that have been on my radar are \u003ca\n  href=\"https://go.dev/\"\n    target=\"_blank\"\n  \u003ego\u003c/a\u003e and \u003ca\n  href=\"https://vlang.io/\"\n    target=\"_blank\"\n  \u003eV\u003c/a\u003e. This post will be about \u003ccode\u003eV\u003c/code\u003e but I hope to do something similar for \u003ccode\u003ego\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003eV\u003c/code\u003e promises to be stable (despite not yet having reached 1.0 release), easy to learn (\u0026ldquo;can be learned over the course of a weekend\u0026rdquo;), fast, and is statically typed.\u003c/p\u003e","title":"Evaluating V (Language)","type":"posts"},{"content":"","date":"2025-08-25","externalUrl":null,"permalink":"/blog/tags/languages/","section":"Tags","summary":"","title":"Languages","type":"tags"},{"content":"","date":"2025-08-25","externalUrl":null,"permalink":"/blog/tags/v-language/","section":"Tags","summary":"","title":"V Language","type":"tags"},{"content":"","date":"2025-08-09","externalUrl":null,"permalink":"/blog/tags/bufstream/","section":"Tags","summary":"","title":"Bufstream","type":"tags"},{"content":"","date":"2025-08-09","externalUrl":null,"permalink":"/blog/tags/kafka/","section":"Tags","summary":"","title":"Kafka","type":"tags"},{"content":"","date":"2025-08-09","externalUrl":null,"permalink":"/blog/tags/protobuf/","section":"Tags","summary":"","title":"Protobuf","type":"tags"},{"content":" Summary # Bufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary Bufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink What is the point? # I often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing. This blog attempts to test the claims of Buf - the company behind: Bufstream - the \u0026ldquo;drop-in replacement for Kafka\u0026rdquo; BSR - the Buf Schema Registry which implements the Confluent Schema Registry API buf CLI - a simple way to develop and manage Protobuf Note that the buf CLI will be referenced in this blog, but less of a focus.\nGetting Started # On a surface-level, I have found the Buf docs to be nice and well written with 4 applicable quickstart guides that will be the basis for getting started https://buf.build/docs/bufstream/quickstart/ https://buf.build/docs/bsr/quickstart/ https://buf.build/docs/cli/quickstart/ https://buf.build/docs/bufstream/iceberg/quickstart/ Bufstream # Installation # Starting with this Bufstream quickstart guide, we are advised to download the bufstream CLI with curl and then simply ./bufstream serve. Let\u0026rsquo;s see if that\u0026rsquo;s as easy as it sounds.\ncurl -sSL -o bufstream \\ \u0026#34;https://buf.build/dl/bufstream/latest/bufstream-$(uname -s)-$(uname -m)\u0026#34; \u0026amp;\u0026amp; \\ chmod +x bufstream ./bufstream serve ... \u0026lt;snipped\u0026gt; ... time=2025-08-05T22:45:14.648-05:00 level=INFO msg=\u0026#34;kafka server started\u0026#34; host=localhost port=9092 tls=false public=true time=2025-08-05T22:45:14.648-05:00 level=INFO msg=\u0026#34;kafka server started\u0026#34; host=127.0.0.1 port=9092 tls=false time=2025-08-05T22:45:14.648-05:00 level=INFO msg=\u0026#34;kafka server started\u0026#34; host=::1 port=9092 tls=false time=2025-08-05T22:45:14.664-05:00 level=INFO msg=\u0026#34;updating ownership\u0026#34; oldShardNum=0 oldShardCount=0 shardNum=0 shardCount=1 Ok, that really was quite easy.\nSetting up a web-based Kafka Administrative Console (optional) # The quickstart then recommends installing either either AKHQ or Redpanda console for managing Kafka-compatible workloads. The Redpanda installation is a simple 4-liner docker run so I\u0026rsquo;ll give that one a try.\ndocker run -p 8080:8080 \\ -e KAFKA_BROKERS=host.docker.internal:9092 \\ -e KAFKA_CLIENTID=\u0026#34;rpconsole;broker_count=1;host_override=host.docker.internal\u0026#34; \\ docker.redpanda.com/redpandadata/console:v3.1.3 ... \u0026lt;snipped\u0026gt; ... {\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2025-08-06T03:56:19.199Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;failed to test Kafka connection, going to retry in 1s\u0026#34;,\u0026#34;remaining_retries\u0026#34;:5} ... {\u0026#34;level\u0026#34;:\u0026#34;fatal\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2025-08-06T03:56:51.007Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;failed to start console service\u0026#34;,\u0026#34;error\u0026#34;:\u0026#34;failed to test kafka connectivity: failed to test kafka connection: failed to request metadata: unable to dial: dial tcp: lookup host.docker.internal on 10.0.2.3:53: no such host\u0026#34;} Hmmm, that did not work as expected. I am testing on Linux and believe that the quickstart presumes we\u0026rsquo;re running MacOS. I have also seen subtle network differences in the past when working with Docker (ok, Podman) on Linux. This eventually worked:\ndocker run --network=host -p 8080:8080 \\ -e KAFKA_BROKERS=localhost:9092 \\ docker.redpanda.com/redpandadata/console:v3.1.3 ... {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2025-08-06T04:08:00.900Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;started Redpanda Console\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v3.1.3\u0026#34;,\u0026#34;built_at\u0026#34;:\u0026#34;1753360440\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2025-08-06T04:08:00.903Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;connecting to Kafka seed brokers, trying to fetch cluster metadata\u0026#34;,\u0026#34;seed_brokers\u0026#34;:[\u0026#34;localhost:9092\u0026#34;]} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2025-08-06T04:08:00.910Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;successfully connected to kafka cluster\u0026#34;,\u0026#34;advertised_broker_count\u0026#34;:1,\u0026#34;topic_count\u0026#34;:0,\u0026#34;controller_id\u0026#34;:1234570725,\u0026#34;kafka_version\u0026#34;:\u0026#34;between v0.10.2 and v0.11.0\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2025-08-06T04:08:01.111Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Server listening on address\u0026#34;,\u0026#34;address\u0026#34;:\u0026#34;[::]:8080\u0026#34;,\u0026#34;port\u0026#34;:8080} I was then able to view the Redpanda console in my browser at http://localhost:8080/overview.\nWorking with Protobuf and the BSR # Continuing with the quickstart, I cloned the Github repo and moved the previously installed bufstream CLI to the cloned directory.\ngit clone https://github.com/bufbuild/bufstream-demo.git \u0026amp;\u0026amp; \\ mv bufstream ./bufstream-demo \u0026amp;\u0026amp; \\ cd ./bufstream-demo This repo contains pre-created Protobuf files that have integration and support for Confluent-compatabile schema registries (which BSR complies with).\nAn example proto file is in proto/bufstream/demo/v1/demo.proto of the cloned repo and defines an EmailUpdated message.\nThings get a little unclear here but from what I understand, this demo.proto file imports a reference to a confluent module with import \u0026quot;buf/confluent/v1/extensions.proto\u0026quot;; and this is what enables the BSR to be compatible with Confluent schema registry. The mapping between the proto file and the topic is done with name: \u0026quot;email-updated-value\u0026quot;. So the topic name here becomes email-updated.\nProducing and Consuming data # The quickstart references a go run ./cmd/bufstream-demo-produce ... and go run ./cmd/bufstream-demo-consume ... but I\u0026rsquo;ve noticed that the cloned repo comes with a Makefile. That would be simpler to use here but the produce/consume commands seem to be out of sync so we\u0026rsquo;ll stick with what the demo suggests to be safe.\ngo run ./cmd/bufstream-demo-produce \\ --topic email-updated \\ --group email-verifier \\ --csr-url \u0026#34;https://demo.buf.dev/integrations/confluent/bufstream-demo\u0026#34; go run ./cmd/bufstream-demo-produce --topic email-updated --group email-verifier go: downloading github.com/brianvoe/gofakeit/v7 v7.3.0 go: downloading github.com/google/uuid v1.6.0 ... \u0026lt;downloading lots of go libs here\u0026gt; ... time=2025-08-06T22:41:20.299-05:00 level=INFO msg=\u0026#34;produced semantically invalid protobuf message\u0026#34; id=072da6ca-8878-4c11-944b-5876a4fc4370 time=2025-08-06T22:41:20.450-05:00 level=INFO msg=\u0026#34;produced invalid data\u0026#34; id=32bc2119-f1bb-43f4-a9db-28fbb04ff7b5 time=2025-08-06T22:41:21.588-05:00 level=INFO msg=\u0026#34;produced semantically valid protobuf message\u0026#34; id=d6fc0bf3-e13b-421f-a696-212059d7961d ... So a lot of data is being generated, some of which is considered semantically invalid. Looking in the RedPanda console, I can see sample data such as foobar as well as other sample data such as $f4525add-da7d-4b2b-aae9-884e7bab535dgarnettwunsch@dickens.netllama, it\u0026rsquo;s clear which of these 2 do not conform to the proto schema.\nNow I\u0026rsquo;ll try the make target make consume-run and see what happens since this is consistent with the snippet in the blog (unlike the produce).\nmake consume-run go run ./cmd/bufstream-demo-consume --topic email-updated --group email-verifier \\ --csr-url \u0026#34;https://demo.buf.dev/integrations/confluent/bufstream-demo\u0026#34; make consume-run go run ./cmd/bufstream-demo-consume --topic email-updated --group email-verifier \\ --csr-url \u0026#34;https://demo.buf.dev/integrations/confluent/bufstream-demo\u0026#34; time=2025-08-06T22:48:44.981-05:00 level=INFO msg=\u0026#34;starting consume\u0026#34; time=2025-08-06T22:48:44.982-05:00 level=INFO msg=\u0026#34;consumed message with new email sisterglover@labadie.biz and old email orvilledickinson@turcotte.biz\u0026#34; time=2025-08-06T22:48:45.984-05:00 level=INFO msg=\u0026#34;consumed message with new email hound and old email antoniomurphy@bradtke.info\u0026#34; time=2025-08-06T22:48:45.984-05:00 level=INFO msg=\u0026#34;consumed malformed data\u0026#34; error=\u0026#34;registration is missing for encode/decode\u0026#34; length=7 ... The log message consumed malformed data makes it sound like the consumer is consuming the data even if it is malformed and does not conform to the schema. This seems unexpected but I may have a misunderstanding on how this works.\nThe quickstart then suggests to run:\n./bufstream serve --config config/bufstream.yaml \u0026hellip;which now seems to tie it all together by ensuring that the consumer has a connection to the BSR and ensuring that the Consumer only sees records that comply with the registered schema. I verified that schema enforcement does seem to be happening here by reviewing the terminal output for the producer and consumer. The Producer shows invalid records (e.g. does not conform to the schema):\n# sample invalid log message from the producer time=2025-08-09T22:36:50.224-05:00 level=ERROR msg=\u0026#34;error on produce of invalid data\u0026#34; error=\u0026#34;failed to produce: INVALID_RECORD: This record has failed the validation on the broker and hence been rejected.\u0026#34; \u0026hellip;but the Consumer logs, only showed valid messages such as:\n# sample valid log message from the consumer time=2025-08-09T22:36:45.222-05:00 level=INFO msg=\u0026#34;consumed message with new email monkey and old email ernestlegros@parisian.name\u0026#34; Bufstream and Apache Spark # If Bufstream really is a drop-in replacement for Apache Kafka, reading from Bufstream with Spark should also be straight-forward. Let\u0026rsquo;s confirm this.\nfrom pyspark.sql import DataFrame from pyspark.sql import functions as sf from pyspark.sql.streaming.readwriter import DataStreamWriter from pyspark.sql.streaming.query import StreamingQuery KAFKA_BOOTSTRAP_SERVERS = \u0026#34;localhost:9092\u0026#34; TOPIC = \u0026#34;email-updated\u0026#34; df: DataFrame = ( spark .readStream .format(\u0026#34;kafka\u0026#34;) .option(\u0026#34;kafka.bootstrap.servers\u0026#34;, KAFKA_BOOTSTRAP_SERVERS) .option(\u0026#34;subscribe\u0026#34;, TOPIC) .option(\u0026#34;startingOffsets\u0026#34;, \u0026#34;earliest\u0026#34;) .load() .selectExpr(\u0026#34;CAST(key AS STRING)\u0026#34;, \u0026#34;CAST(value AS STRING)\u0026#34;) ) datastream_writer: DataStreamWriter = ( df .select(\u0026#34;value\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .option(\u0026#34;truncate\u0026#34;, \u0026#34;false\u0026#34;) ) streaming_query: StreamingQuery = datastream_writer.start() ------------------------------------------- Batch: 0 ------------------------------------------- +---------------------------------------------------------------------------------------------+ |value | +---------------------------------------------------------------------------------------------+ |\\n$926b3e7f-0311-4b0e-8925-e49b653e3f95jacqueskoss@lehner.infoÔøΩalexisgraham@raynor.com | |\\n$c0ec4ae6-d7c0-49d0-96f3-f45e5df45e78jewelrohan@strosin.orgÔøΩsalmon | |foobar | |\\n$a1d8b79c-87c5-4dde-9183-b702e5b5b334marshallreynolds@armstrong.ioÔøΩericklittle@lang.name| |\\n$ab20b911-7bee-4fe9-8366-6211766412dfgissellejohnston@runolfsson.orgÔøΩ\\vgrasshopper | |foobar | |\\n$f89d1a48-2d9b-4d77-a872-d39be4c61921rickybechtelar@stamm.netÔøΩraoulbeahan@rutherford.io | |\\n$8af0f411-c2ed-494d-ac82-e495b111045austinanitzsche@lebsack.netÔøΩ\\bplatypus | |foobar | |\\n$a58f1996-c6b2-416b-8531-a603921dd828marisafunk@towne.netÔøΩblakerippin@kemmer.biz | |\\n$48fa5f74-5921-4061-ad25-559ce2cb6e7aeleonoreupton@rohan.netÔøΩwasp | |foobar ... So this appeared to work at a very basic level, but this is not how we should be performing reads via Spark given that we want our consumers to know and cross-reference the expected schema defined in the BSR. Given that we confirmed this worked as expected with the above Consumer, we will for now presume that this capability is possible with Spark as well and leave the full Spark \u0026ndash;\u0026gt; BSR integration for later research.\nWhat about Iceberg? # I followed the Iceberg quickstart but it seems like the compose file is referencing a spark/ directory that doesn\u0026rsquo;t exist: OSError: Dockerfile not found in /home/username/tmp/buf-examples/bufstream/iceberg-quickstart/spark .So I couldn\u0026rsquo;t get this working but according to the guide, all you need is:\nBufstream broker object storage an Iceberg catalog implementation The details of your Iceberg catalog are defined in config/bufstream.yaml.\nWhat about Bufstream in Production? # The initial impression I had of Bustream is that deployments are super simple. I suppose this is true, in comparison to a Kafka deployment, but it still requires:\nObject storage such as AWS S3, Google Cloud Storage, or Azure Blob Storage. A metadata storage service such as PostgreSQL. There are numerous different deployment options such as Docker, Helm, and others with a provided Terraform module. It\u0026rsquo;s unclear if BSR requires an additional set of deployments or if that is integrated into Postgres.\nRecap # Buf\u0026rsquo;s vision seems well-thought and clearly-defined. A simplified Kafka-compatible streaming engine A single way to manage schemas with Protobuf Schema enforcement with the BSR Data Lakehouse integration with Apache Iceberg The streaming market has no shortage of vended options but Buf\u0026rsquo;s vision really makes it stand out. Protobuf has the reputation for being less user-friendly than other serialization formats like JSON but this may change if the buf CLI can keep it\u0026rsquo;s promises of simplifying the schema mangement experience. ","date":"2025-08-09","externalUrl":null,"permalink":"/blog/posts/kafka-protobuf-and-bufstream/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eSummary\n    \u003cdiv id=\"summary\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#summary\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eBufstream is said to be a drop-in replacement for Kafka via a simple Go-based binary\u003c/li\u003e\n\u003cli\u003eBufstream also standardizes on Protocol Buffers as the serialization format for messaging and integrates well with Lakehouses by writing directly to an Iceberg sink\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eWhat is the point?\n    \u003cdiv id=\"what-is-the-point\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#what-is-the-point\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eI often work with streaming and Kafka with Apache Spark. Setting up and managing a Kafka cluster can be cumbersome so having a quick, easy, standardized, and leightweight way to run Kafka is appealing.\u003c/li\u003e\n\u003cli\u003eThis blog attempts to test the claims of Buf - the company behind:\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eBufstream\u003c/code\u003e - the \u0026ldquo;drop-in replacement for Kafka\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBSR\u003c/code\u003e - the Buf Schema Registry which implements the Confluent Schema Registry API\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ebuf\u003c/code\u003e CLI - a simple way to develop and manage Protobuf\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that the \u003ccode\u003ebuf\u003c/code\u003e CLI will be referenced in this blog, but less of a focus.\u003c/p\u003e","title":"Streaming with Bufstream, Protobuf, and Spark","type":"posts"},{"content":"","date":"2025-07-12","externalUrl":null,"permalink":"/blog/tags/containers/","section":"Tags","summary":"","title":"Containers","type":"tags"},{"content":"","date":"2025-07-12","externalUrl":null,"permalink":"/blog/tags/testing/","section":"Tags","summary":"","title":"Testing","type":"tags"},{"content":" Summary # Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding Why, just why? # Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.\nRather than running these tests on-demand, or add these examples in a notebook, I thought that it would be useful to build up a little library of these commands that can be run using pytest where performance can be evaluated and could also be used as a reference/cheatsheet.\nDoes this library serve any real practical purpose other than learning and understanding? No, not really. But maybe this can be a spring-board for something else\nSetup a basic project structure # I find it helpful to use a Makefile to produce a consistent and a well-documented and repeatable build process. The Makefile is in References/Makefile. The basic project structure when completed will be:\n‚ùØ tree . ‚îú‚îÄ‚îÄ Containerfile ‚îú‚îÄ‚îÄ Makefile ‚îú‚îÄ‚îÄ pyproject.toml ‚îî‚îÄ‚îÄ tests ‚îî‚îÄ‚îÄ test_pyspark.py 1 directory, 4 files The container image specification # Following is the Containerfile (a Dockerfile would also suffice). The specifications of the container image are:\n# Containerfile FROM ghcr.io/astral-sh/uv:debian # Java is required for PySpark RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y openjdk-17-jdk \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* WORKDIR /app COPY pyproject.toml /app/pyproject.toml RUN uv sync ENV PYSPARK_PYTHON=/usr/bin/python3 COPY tests/ /app/tests/ CMD [\u0026#34;uv\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;.venv/bin/pytest\u0026#34;, \u0026#34;--durations=0\u0026#34;, \u0026#34;-v\u0026#34;, \u0026#34;/app/tests/\u0026#34;] The Python project setup # The Python aspect of the app will contain 2 major components, a pyproject.toml and a unit test to get started.\n# pyproject.toml [project] name = \u0026#34;spark-test\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Spark unit tests\u0026#34; dependencies = [ \u0026#34;pytest\u0026#34;, \u0026#34;pyspark==3.5.2\u0026#34;, ] And the unit tests:\n# tests/tests_pyspark.py import pytest from pyspark.sql import SparkSession from pyspark.sql import functions as sf from pyspark.sql import types as st @pytest.fixture(scope=\u0026#34;session\u0026#34;) def spark(): spark_session = ( SparkSession.builder .master(\u0026#34;local[*]\u0026#34;) .appName(\u0026#34;pytest-pyspark-testing\u0026#34;) .getOrCreate() ) yield spark_session spark_session.stop() @pytest.fixture def sample_df(spark): data = [(\u0026#34;A\u0026#34;, 10), (\u0026#34;A\u0026#34;, 20), (\u0026#34;B\u0026#34;, 5)] return spark.createDataFrame(data, [\u0026#34;group\u0026#34;, \u0026#34;value\u0026#34;]) def test_spark_range_count(spark): assert spark.range(2).count() == 2 def test_group_and_sum(sample_df): result = ( sample_df .groupBy(\u0026#34;group\u0026#34;) .agg(sf.sum(\u0026#34;value\u0026#34;).alias(\u0026#34;total\u0026#34;)) .where(\u0026#34;group = \u0026#39;A\u0026#39;\u0026#34;) .first() ) assert isinstance(result, st.Row) assert result.total == 30 Note that if iterating quickly and making many test changes, it may be beneficial to directly mount the tests/ directory as a volume so the image does not need to be rebuilt.\nBuild the container image # The associated Makefile assumes that you are using podman instead of Docker, but can easily be swapped out if so desired by simply changing the CMD := podman to CMD := docker. Then run:\nmake build-image\nRun the test suite # The tests can now be run with make test. The pytest command is invoked with verbose options that include runtime durations.\n‚ùØ make test podman run --rm spark-test warning: No `requires-python` value found in the workspace. Defaulting to `\u0026gt;=3.11`. ============================= test session starts ============================== platform linux -- Python 3.11.2, pytest-8.4.1, pluggy-1.6.0 -- /app/.venv/bin/python cachedir: .pytest_cache rootdir: /app configfile: pyproject.toml collecting ... collected 2 items tests/test_pyspark.py::test_spark_range_count PASSED [ 50%] tests/test_pyspark.py::test_group_and_sum PASSED [100%] ============================== slowest durations =============================== 6.17s call tests/test_pyspark.py::test_spark_range_count 4.89s setup tests/test_pyspark.py::test_spark_range_count 2.37s call tests/test_pyspark.py::test_group_and_sum 0.97s teardown tests/test_pyspark.py::test_group_and_sum 0.16s setup tests/test_pyspark.py::test_group_and_sum (1 durations \u0026lt; 0.005s hidden. Use -vv to show these durations.) ============================== 2 passed in 15.02s ============================== Tips and Considerations # Generally it\u0026rsquo;s good to keep test data small and in-memory for fast execution but with Spark there likely will be a need to test file-based sources Use pytest fixtures for re-use If the amount of fixtures become unwieldly, consider putting them in a conftest.py Try to keep tests self-contained as much as possible Provide descriptive names for each of the test functions References # The corresponding Github repo\nMakefile # # Makefile .DEFAULT_GOAL := help SHELL := /bin/bash CMD := podman IMAGE_NAME := spark-test help: ## Show this help message @echo -e \u0026#39;Usage: make [target] ...\\n\u0026#39; @echo \u0026#39;targets:\u0026#39; @egrep \u0026#39;^(.+)\\:\\ ##\\ (.+)\u0026#39; ${MAKEFILE_LIST} | column -t -c 2 -s \u0026#39;:#\u0026#39; .PHONY: build-image build-image: ## build the container image $(CMD) build -t $(IMAGE_NAME) . .PHONY: setup setup: ## setup the basic project structure mkdir -p tests/ \u0026amp;\u0026amp; touch pyproject.toml Containerfile tests/test_pyspark.py .PHONY: test test: ## run the unit tests $(CMD) run --rm $(IMAGE_NAME) ","date":"2025-07-12","externalUrl":null,"permalink":"/blog/posts/pyspark-container-unit-tests/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eSummary\n    \u003cdiv id=\"summary\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#summary\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRunning PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code\u003c/li\u003e\n\u003cli\u003eThis simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eWhy, just why?\n    \u003cdiv id=\"why-just-why\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#why-just-why\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eHopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.\u003c/p\u003e","title":"Using a Container to run PySpark Unit Tests","type":"posts"},{"content":" Summary # Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources A custom data source allows you to connect to a source system that that Spark may not currently have support for PySpark Custom Data Sources - an Overview # Starting with Apache Spark 4.0 and Databricks 15.2, PySpark supports custom data sources.\nSo what are PySpark Custom Data Sources?\nCustom data sources allow you to define a source format other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint.\nDuckDB and DuckLake are being discussed a lot lately so I thought it would be fun to see how easy it would be to interact with it from Spark. DuckDB does have an experimental Spark API but that would just be too easy, so let\u0026rsquo;s try to roll our own for educational purposes.\nHow does it work? # To use Custom Data Source, we define a class that inherits from DataSource and then a Reader or Writer class inherits from DataSourceReader or DataSourceWriter respectively. These classes all reside in the pyspark.sql.datasource module.\npyspark.sql.datasource ‚îú‚îÄ‚îÄ DataSource ‚îú‚îÄ‚îÄ DataSourceReader ‚îî‚îÄ‚îÄ DataSourceWriter There is also a DataSourceStreamReader and DataSourceStreamWriter but we wont touch on them in this blog.\nSetup # Setup instructions follow but can also be setup using targets in a sample Makefile shown below in References/Makefile\nInstall the DuckDB CLI\nNote - This is a platform-dependent step so I\u0026rsquo;ll omit this setup instruction here and refer you to the docs instead. Setup the Python environment. Also can be run with make python-setup\nuv venv --python 3.12 source .venv/bin/activate uv pip install duckdb ipython pyspark==4.0.0 Create a quick test table in DuckDb. Can also be run with make duckdb-setup duckdb dev.duckdb create table t1 (c1 int); insert into t1 values (1); Start PySpark use the make target: make start-pyspark source .venv/bin/activate \u0026amp;\u0026amp; \\ PYSPARK_DRIVER_PYTHON_OPTS=\u0026#34;--TerminalInteractiveShell.editing_mode=vi --colors=Linux\u0026#34; PYSPARK_DRIVER_PYTHON=ipython pyspark Define a Data Source # Your custom data source must inherit from DataSource and will then be referenced by the custom reader class that will ultimately inherit from DataSourceReader.\nfrom pyspark.sql.datasource import DataSource, DataSourceReader from pyspark.sql.types import StructType class DuckDBDataSource(DataSource): @classmethod def name(cls): return \u0026#34;duckdb\u0026#34; def schema(self): ... def reader(self, schema: str): return DuckDBDataSourceReader(schema, self.options) Define a Data Source Reader # This is where much of the magic resides in terms of how the Reader will interact with the DataSource. class DuckDBDataSourceReader(DataSourceReader): def __init__(self, schema, options): self.schema = schema self.options = options def read(self, partition): import duckdb db_path = self.options[\u0026#34;db_path\u0026#34;] query = self.options[\u0026#34;query\u0026#34;] with duckdb.connect(db_path) as conn: cursor = conn.execute(query) for row in cursor.fetchall(): yield tuple(row) Read from a DuckDB source # from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() spark.dataSource.register(DuckDBDataSource) ( spark.read .format(\u0026#34;duckdb\u0026#34;) .option(\u0026#34;db_path\u0026#34;, \u0026#34;dev.duckdb\u0026#34;) .option(\u0026#34;query\u0026#34;, \u0026#34;SELECT * FROM t1\u0026#34;) .schema(\u0026#34;c1 int\u0026#34;) .load() ).show() +---+ | c1| +---+ | 1| +---+ Takeaway # This blog just scratches the surface of what\u0026rsquo;s possible with Pyspark Custom Data Sources and would need numerous enhancements (obviously) to be used in any serious manner but hopefully gets across the point of the Data Source\u0026rsquo;s capabilities.\nReferences # Makefile # A Makefile to help simplify the setup # Makefile .DEFAULT_GOAL := help SHELL := /bin/bash help: ## Show this help message @echo -e \u0026#39;Usage: make [target] ...\\n\u0026#39; @echo \u0026#39;targets:\u0026#39; @egrep \u0026#39;^(.+)\\:\\ ##\\ (.+)\u0026#39; ${MAKEFILE_LIST} | column -t -c 2 -s \u0026#39;:#\u0026#39; .PHONY: setup-python setup-python: ## setup python env and dependencies uv venv --python 3.12 .venv source .venv/bin/activate \u0026amp;\u0026amp; uv pip install duckdb ipython pyarrow pyspark==4.0.0 .PHONY: setup-duckdb setup-duckdb: ## setup duckdb database and test table with data duckdb dev.duckdb -c \u0026#34;create table t1 (c1 int); insert into t1 values (1);\u0026#34; .PHONY: setup setup: ## setup python and duckdb setup-python setup-duckdb .PHONY: start-pyspark start-pyspark: ## start-pyspark source .venv/bin/activate \u0026amp;\u0026amp; \\ PYSPARK_DRIVER_PYTHON_OPTS=\u0026#34;--TerminalInteractiveShell.editing_mode=vi --colors=Linux\u0026#34; PYSPARK_DRIVER_PYTHON=ipython pyspark ","date":"2025-06-14","externalUrl":null,"permalink":"/blog/posts/pyspark-custom-datasource-duckdb/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eSummary\n    \u003cdiv id=\"summary\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#summary\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eApache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources\u003c/li\u003e\n\u003cli\u003eA custom data source allows you to connect to a source system that that Spark may not currently have support for\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003ePySpark Custom Data Sources - an Overview\n    \u003cdiv id=\"pyspark-custom-data-sources---an-overview\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#pyspark-custom-data-sources---an-overview\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eStarting with \u003ca\n  href=\"https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html\"\n    target=\"_blank\"\n  \u003eApache Spark 4.0\u003c/a\u003e and \u003ca\n  href=\"https://docs.databricks.com/aws/en/pyspark/datasources\"\n    target=\"_blank\"\n  \u003eDatabricks 15.2\u003c/a\u003e, PySpark supports custom data sources.\u003c/p\u003e","title":"Building a Pyspark Custom Data Sources for DuckDB","type":"posts"},{"content":"","date":"2025-06-14","externalUrl":null,"permalink":"/blog/tags/duckdb/","section":"Tags","summary":"","title":"Duckdb","type":"tags"},{"content":"A tech enthusiast learning through writing with a primary focus on Data, Cloud, DevOps\n","externalUrl":null,"permalink":"/blog/","section":"Brock's Blog","summary":"\u003cp\u003eA tech enthusiast learning through writing with a primary focus on Data, Cloud, DevOps\u003c/p\u003e","title":"Brock's Blog","type":"page"},{"content":"","externalUrl":null,"permalink":"/blog/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/blog/search/","section":"Brock's Blog","summary":"search","title":"Search","type":"page"}]