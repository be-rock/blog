[{"content":"Summary Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding Why, just why? Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.\nRather than running these tests on-demand, or add these examples in a notebook, I thought that it would be useful to build up a little library of these commands that can be run using pytest where performance can be evaluated and could also be used as a reference/cheatsheet.\nDoes this library serve any real practical purpose other than learning and understanding? No, not really. But maybe this can be a spring-board for something else\nSetup a basic project structure I find it helpful to use a Makefile to produce a consistent and a well-documented and repeatable build process. The Makefile is in References/Makefile. The basic project structure when completed will be:\n❯ tree . ├── Containerfile ├── Makefile ├── pyproject.toml └── tests └── test_pyspark.py 1 directory, 4 files The container image specification Following is the Containerfile (a Dockerfile would also suffice). The specifications of the container image are:\n# Containerfile FROM ghcr.io/astral-sh/uv:debian # Java is required for PySpark RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y openjdk-17-jdk \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* WORKDIR /app COPY pyproject.toml /app/pyproject.toml RUN uv sync ENV PYSPARK_PYTHON=/usr/bin/python3 COPY tests/ /app/tests/ CMD [\u0026#34;uv\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;.venv/bin/pytest\u0026#34;, \u0026#34;--durations=0\u0026#34;, \u0026#34;-v\u0026#34;, \u0026#34;/app/tests/\u0026#34;] The Python project setup The Python aspect of the app will contain 2 major components, a pyproject.toml and a unit test to get started.\n# pyproject.toml [project] name = \u0026#34;spark-test\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;Spark unit tests\u0026#34; dependencies = [ \u0026#34;pytest\u0026#34;, \u0026#34;pyspark==3.5.2\u0026#34;, ] And the unit tests:\n# tests/tests_pyspark.py import pytest from pyspark.sql import SparkSession from pyspark.sql import functions as sf from pyspark.sql import types as st @pytest.fixture(scope=\u0026#34;session\u0026#34;) def spark(): spark_session = ( SparkSession.builder .master(\u0026#34;local[*]\u0026#34;) .appName(\u0026#34;pytest-pyspark-testing\u0026#34;) .getOrCreate() ) yield spark_session spark_session.stop() @pytest.fixture def sample_df(spark): data = [(\u0026#34;A\u0026#34;, 10), (\u0026#34;A\u0026#34;, 20), (\u0026#34;B\u0026#34;, 5)] return spark.createDataFrame(data, [\u0026#34;group\u0026#34;, \u0026#34;value\u0026#34;]) def test_spark_range_count(spark): assert spark.range(2).count() == 2 def test_group_and_sum(sample_df): result = ( sample_df .groupBy(\u0026#34;group\u0026#34;) .agg(sf.sum(\u0026#34;value\u0026#34;).alias(\u0026#34;total\u0026#34;)) .where(\u0026#34;group = \u0026#39;A\u0026#39;\u0026#34;) .first() ) assert isinstance(result, st.Row) assert result.total == 30 Note that if iterating quickly and making many test changes, it may be beneficial to directly mount the tests/ directory as a volume so the image does not need to be rebuilt.\nBuild the container image The associated Makefile assumes that you are using podman instead of Docker, but can easily be swapped out if so desired by simply changing the CMD := podman to CMD := docker. Then run:\nmake build-image\nRun the test suite The tests can now be run with make test. The pytest command is invoked with verbose options that include runtime durations.\n❯ make test podman run --rm spark-test warning: No `requires-python` value found in the workspace. Defaulting to `\u0026gt;=3.11`. ============================= test session starts ============================== platform linux -- Python 3.11.2, pytest-8.4.1, pluggy-1.6.0 -- /app/.venv/bin/python cachedir: .pytest_cache rootdir: /app configfile: pyproject.toml collecting ... collected 2 items tests/test_pyspark.py::test_spark_range_count PASSED [ 50%] tests/test_pyspark.py::test_group_and_sum PASSED [100%] ============================== slowest durations =============================== 6.17s call tests/test_pyspark.py::test_spark_range_count 4.89s setup tests/test_pyspark.py::test_spark_range_count 2.37s call tests/test_pyspark.py::test_group_and_sum 0.97s teardown tests/test_pyspark.py::test_group_and_sum 0.16s setup tests/test_pyspark.py::test_group_and_sum (1 durations \u0026lt; 0.005s hidden. Use -vv to show these durations.) ============================== 2 passed in 15.02s ============================== Tips and Considerations Generally it\u0026rsquo;s good to keep test data small and in-memory for fast execution but with Spark there likely will be a need to test file-based sources Use pytest fixtures for re-use If the amount of fixtures become unwieldly, consider putting them in a conftest.py Try to keep tests self-contained as much as possible Provide descriptive names for each of the test functions References Makefile # Makefile .DEFAULT_GOAL := help SHELL := /bin/bash CMD := podman IMAGE_NAME := spark-test help: ## Show this help message @echo -e \u0026#39;Usage: make [target] ...\\n\u0026#39; @echo \u0026#39;targets:\u0026#39; @egrep \u0026#39;^(.+)\\:\\ ##\\ (.+)\u0026#39; ${MAKEFILE_LIST} | column -t -c 2 -s \u0026#39;:#\u0026#39; .PHONY: build-image build-image: ## build the container image $(CMD) build -t $(IMAGE_NAME) . .PHONY: setup setup: ## setup the basic project structure mkdir -p tests/ \u0026amp;\u0026amp; touch pyproject.toml Containerfile tests/test_pyspark.py .PHONY: test test: ## run the unit tests $(CMD) run --rm $(IMAGE_NAME) ","permalink":"https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/","summary":"\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRunning PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code\u003c/li\u003e\n\u003cli\u003eThis simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"why-just-why\"\u003eWhy, just why?\u003c/h2\u003e\n\u003cp\u003eHopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.\u003c/p\u003e","title":"Using a Container to run PySpark Unit Tests"},{"content":"Summary Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources A custom data source allows you to connect to a source system that that Spark may not currently have support for PySpark Custom Data Sources - an Overview Starting with Apache Spark 4.0 and Databricks 15.2, PySpark supports custom data sources.\nSo what are PySpark Custom Data Sources?\nCustom data sources allow you to define a source format other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint.\nDuckDB and DuckLake are being discussed a lot lately so I thought it would be fun to see how easy it would be to interact with it from Spark. DuckDB does have an experimental Spark API but that would just be too easy, so let\u0026rsquo;s try to roll our own for educational purposes.\nHow does it work? To use Custom Data Source, we define a class that inherits from DataSource and then a Reader or Writer class inherits from DataSourceReader or DataSourceWriter respectively. These classes all reside in the pyspark.sql.datasource module.\npyspark.sql.datasource ├── DataSource ├── DataSourceReader └── DataSourceWriter There is also a DataSourceStreamReader and DataSourceStreamWriter but we wont touch on them in this blog.\nSetup Setup instructions follow but can also be setup using targets in a sample Makefile shown below in References/Makefile\nInstall the DuckDB CLI\nNote - This is a platform-dependent step so I\u0026rsquo;ll omit this setup instruction here and refer you to the docs instead. Setup the Python environment. Also can be run with make python-setup\nuv venv --python 3.12 source .venv/bin/activate uv pip install duckdb ipython pyspark==4.0.0 Create a quick test table in DuckDb. Can also be run with make duckdb-setup duckdb dev.duckdb create table t1 (c1 int); insert into t1 values (1); Start PySpark use the make target: make start-pyspark source .venv/bin/activate \u0026amp;\u0026amp; \\ PYSPARK_DRIVER_PYTHON_OPTS=\u0026#34;--TerminalInteractiveShell.editing_mode=vi --colors=Linux\u0026#34; PYSPARK_DRIVER_PYTHON=ipython pyspark Define a Data Source Your custom data source must inherit from DataSource and will then be referenced by the custom reader class that will ultimately inherit from DataSourceReader.\nfrom pyspark.sql.datasource import DataSource, DataSourceReader from pyspark.sql.types import StructType class DuckDBDataSource(DataSource): @classmethod def name(cls): return \u0026#34;duckdb\u0026#34; def schema(self): ... def reader(self, schema: str): return DuckDBDataSourceReader(schema, self.options) Define a Data Source Reader This is where much of the magic resides in terms of how the Reader will interact with the DataSource. class DuckDBDataSourceReader(DataSourceReader): def __init__(self, schema, options): self.schema = schema self.options = options def read(self, partition): import duckdb db_path = self.options[\u0026#34;db_path\u0026#34;] query = self.options[\u0026#34;query\u0026#34;] with duckdb.connect(db_path) as conn: cursor = conn.execute(query) for row in cursor.fetchall(): yield tuple(row) Read from a DuckDB source from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() spark.dataSource.register(DuckDBDataSource) ( spark.read .format(\u0026#34;duckdb\u0026#34;) .option(\u0026#34;db_path\u0026#34;, \u0026#34;dev.duckdb\u0026#34;) .option(\u0026#34;query\u0026#34;, \u0026#34;SELECT * FROM t1\u0026#34;) .schema(\u0026#34;c1 int\u0026#34;) .load() ).show() +---+ | c1| +---+ | 1| +---+ Takeaway This blog just scratches the surface of what\u0026rsquo;s possible with Pyspark Custom Data Sources and would need numerous enhancements (obviously) to be used in any serious manner but hopefully gets across the point of the Data Source\u0026rsquo;s capabilities.\nReferences Makefile A Makefile to help simplify the setup # Makefile .DEFAULT_GOAL := help SHELL := /bin/bash help: ## Show this help message @echo -e \u0026#39;Usage: make [target] ...\\n\u0026#39; @echo \u0026#39;targets:\u0026#39; @egrep \u0026#39;^(.+)\\:\\ ##\\ (.+)\u0026#39; ${MAKEFILE_LIST} | column -t -c 2 -s \u0026#39;:#\u0026#39; .PHONY: setup-python setup-python: ## setup python env and dependencies uv venv --python 3.12 .venv source .venv/bin/activate \u0026amp;\u0026amp; uv pip install duckdb ipython pyarrow pyspark==4.0.0 .PHONY: setup-duckdb setup-duckdb: ## setup duckdb database and test table with data duckdb dev.duckdb -c \u0026#34;create table t1 (c1 int); insert into t1 values (1);\u0026#34; .PHONY: setup setup: ## setup python and duckdb setup-python setup-duckdb .PHONY: start-pyspark start-pyspark: ## start-pyspark source .venv/bin/activate \u0026amp;\u0026amp; \\ PYSPARK_DRIVER_PYTHON_OPTS=\u0026#34;--TerminalInteractiveShell.editing_mode=vi --colors=Linux\u0026#34; PYSPARK_DRIVER_PYTHON=ipython pyspark ","permalink":"https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/","summary":"\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eApache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources\u003c/li\u003e\n\u003cli\u003eA custom data source allows you to connect to a source system that that Spark may not currently have support for\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"pyspark-custom-data-sources---an-overview\"\u003ePySpark Custom Data Sources - an Overview\u003c/h2\u003e\n\u003cp\u003eStarting with \u003ca href=\"https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html\"\u003eApache Spark 4.0\u003c/a\u003e and \u003ca href=\"https://docs.databricks.com/aws/en/pyspark/datasources\"\u003eDatabricks 15.2\u003c/a\u003e, PySpark supports custom data sources.\u003c/p\u003e\n\u003cp\u003eSo what \u003cem\u003eare\u003c/em\u003e PySpark Custom Data Sources?\u003c/p\u003e\n\u003cp\u003eCustom data sources allow you to define a source \u003ccode\u003eformat\u003c/code\u003e other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint.\u003c/p\u003e","title":"Building a Pyspark Custom Data Sources for DuckDB"}]