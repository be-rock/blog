<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Pyspark on Brock B's Blog</title><link>https://be-rock.github.io/blog/tags/pyspark/</link><description>Recent content in Pyspark on Brock B's Blog</description><generator>Hugo -- 0.148.1</generator><language>en-us</language><lastBuildDate>Sat, 12 Jul 2025 12:58:15 -0500</lastBuildDate><atom:link href="https://be-rock.github.io/blog/tags/pyspark/index.xml" rel="self" type="application/rss+xml"/><item><title>Using a Container to run PySpark Unit Tests</title><link>https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/</link><pubDate>Sat, 12 Jul 2025 12:58:15 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/pyspark-container-unit-tests/</guid><description>&lt;h2 id="summary">Summary&lt;/h2>
&lt;ul>
&lt;li>Running PySpark unit tests in a Container can make for a repeatable, portable way to unit test your code&lt;/li>
&lt;li>This simple library can be used as a template to create a repeatable set of Pyspark tests for both reference and understanding&lt;/li>
&lt;/ul>
&lt;h2 id="why-just-why">Why, just why?&lt;/h2>
&lt;p>Hopefully you do not need to be convinced of the value of unit testing; there is really no shortage of content on this topic. If we can agree on that, what I have found in my day-to-day is that I am often in the Spark shell trying to evaluate functions, generate explain plans, and trying to confirm my understanding of internals.&lt;/p></description></item><item><title>Building a Pyspark Custom Data Sources for DuckDB</title><link>https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/</link><pubDate>Sat, 14 Jun 2025 23:51:42 -0500</pubDate><guid>https://be-rock.github.io/blog/posts/pyspark-custom-datasource-duckdb/</guid><description>&lt;h2 id="summary">Summary&lt;/h2>
&lt;ul>
&lt;li>Apache Spark 4.0 and Databricks 15.2 supports custom Pyspark Data Sources&lt;/li>
&lt;li>A custom data source allows you to connect to a source system that that Spark may not currently have support for&lt;/li>
&lt;/ul>
&lt;h2 id="pyspark-custom-data-sources---an-overview">PySpark Custom Data Sources - an Overview&lt;/h2>
&lt;p>Starting with &lt;a href="https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html">Apache Spark 4.0&lt;/a> and &lt;a href="https://docs.databricks.com/aws/en/pyspark/datasources">Databricks 15.2&lt;/a>, PySpark supports custom data sources.&lt;/p>
&lt;p>So what &lt;em>are&lt;/em> PySpark Custom Data Sources?&lt;/p>
&lt;p>Custom data sources allow you to define a source &lt;code>format&lt;/code> other than the default built-in formats such as csv, json, and parquet. There are many other supported formats such as Delta and Iceberg, but if there is no community or commercial support offered for the data source of interest, you can extend and inherit some builtin PySpark classes and roll your own. One common example of this may be calling a REST endpoint.&lt;/p></description></item></channel></rss>